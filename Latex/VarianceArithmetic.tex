\documentclass[twoside]{article}
%Latex for arxiv needs figures in pdf format.  To convert png to pdf, go to https://pngpdf.com/#google_vignette

\pagestyle{myheadings}
\setlength{\oddsidemargin}{44pt}
\setlength{\evensidemargin}{44pt}
\setcounter{page}{1}

\usepackage{url,intmacros,graphicx}
\usepackage{amssymb,amsmath}
\usepackage{capt-of}
\usepackage{float}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\numberwithin{equation}{section}

\newcommand{\email}[1]{\\ \small{\url{#1}} \\}
\newcommand{\institution}[1]{\\ \parbox{3.0in}{\small{#1}}}
\newcommand{\keywords}[1]{\small\textbf{Keywords: }#1}
\newcommand{\AMSsubj}[1]{\noindent\textbf{AMS subject classifications: }#1}
\newcommand\whenaccepted{}

\newcommand{\eqspace}{\;\;\;}
\newcommand{\largespace}{\;\;\;\;\;\;\;\;\;\;\;\;}
\newcommand{\Verbose}{}
%\newcommand{\ManualReference}{}



%-------------------------------------------------------


%  You may add additional packages here.  However, if they
%  are not available with the usual LaTeX distribution,
%  they must be supplied with the final, accepted LaTeX.

% Fill in your title here. (Retain the footnote.)
\title{Statistical Taylor Expansion: A New and Path-Independent Method for Uncertainty Analysis \footnote{\whenaccepted}}

% Delete the "\and" or add more as needed
\author{Chengpu Wang
\institution{40 Grossman Street, Melville, NY 11747, USA}
\email{Chengpu@gmail.com}}

% Put a short running title within the first argument to
% this command.  Do not alter the second argument
\markboth{CP Wang, \textit{Statistical Taylor Expansion}}
         {\textit{https://github.com/Chengpu0707/VarianceArithmetic}}

%\date{2025/05/20}



\begin{document}
\maketitle
\begin{abstract}

As a rigorous statistical approach, statistical Taylor expansion extends the conventional Taylor expansion by replacing precise input variables with random variables of known distributions to compute the means and standard deviations of the results.
Statistical Taylor expansion tracks the propagation of the input uncertainties through intermediate steps, causing the variables in intermediate analytic expressions to become interdependent, while the final analytic result becomes path independent.
This fundamentally distinguishes it from common approaches in applied mathematics that optimize computational path for each calculation.
In essence, statistical Taylor expansion may standardize numerical computations for analytic expressions.
Its statistical nature enables reliable testing of results when the sample size is sufficiently large, as well as includes sample count in result qualification.
This study also introduces an implementation of statistical Taylor expansion termed variance arithmetic and presents corresponding test results across a wide range of mathematical applications.

Another important conclusion of this study is that numerical errors in library functions can significantly affect results.  
For instance, periodic numerical errors in trigonometric library functions may resonate with periodic signals, producing large numerical errors in the outcomes.
\end{abstract}

% Put keywords appropriate to your paper here, as shown
\keywords{computer arithmetic, error analysis, interval arithmetic, uncertainty, numerical algorithms.}

% Put your AMS subject classifications into the argument of
% the following command.
\AMSsubj{G.1.0}

Copyright \copyright{2024}


\ifdefined\Verbose
\clearpage
\tableofcontents
\fi
\clearpage
\section{Introduction}
\label{sec: introduction}

\subsection{Measurement Uncertainty}

Except for the most basic counting, scientific and engineering measurements never yield completely precise results \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}. 
In such measurements, the uncertainty of a quantity $x$ is typically expressed either by the sample deviation $\delta x$ or by the uncertainty range $\Delta x$ \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
\begin{itemize}
\item If $\delta x = 0$ or $\Delta x = 0$, $x$ is a \emph{precise value}.

\item Otherwise, $x$ is an \emph{imprecise value}.
\end{itemize}
 
$P(x) \equiv \delta x / |x|$ is defined as the \emph{statistical precision} (hereafter referred to as precision) of the measurement, where $x$ denotes the value, and $\delta x$ represents the uncertainty deviation.
A larger precision indicates a coarser measurement whereas a smaller precision indicates a finer measurement.
The precision of measured values can range from order-of-magnitude estimates to $10^{-2}$ to $10^{-4}$ in common measurements, and to $10^{-15}$ in state-of-the-art determinations of fundamental physical constants \cite{Basic_Constants_Measurements}. 

This study focuses on determining the result value and uncertainty of a general analytic expression with imprecise input values. 



\subsection{Extension to Existing Statistics}

Rather than focusing on the uncertainty distribution of the result when applying a function to a random variable with a known uncertainty distribution \cite{Probability_Statistics}, statistical Taylor expansion provides the mean and variance of the result for general analytic expressions.
\begin{itemize}
\item 
Previous studies have examined the effect of input uncertainties on output values for specific cases \cite{Lower_Order_Variance_Expansion}.
Statistical Taylor expansion generalizes these effects as uncertainty bias, as shown in Formula \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 2d mean} in this paper.

\item
The traditional variance-covariance framework accounts only for linear interactions between random variables through an analytic function \cite{Probability_Statistics}\cite{Lower_Order_Variance_Expansion}, whereas statistical Taylor expansion extends this framework to include higher-order interactions as expressed in Formula \eqref{eqn: Taylor 2d variance} in this paper.
As shown in Formula \eqref{eqn: determinant variance}, \eqref{eqn: inverse matrix 2 variance}, and \eqref{eqn: inverse matrix 2 bias}, such extension is important even for linear algebra.
 
\end{itemize}


\subsection{Problem of the Related Numerical Arithmetics}

Variance arithmetic implements statistical Taylor expansion and surpasses all existing related numerical arithmetic methods.


\subsubsection{Conventional Floating-Point Arithmetic}

Conventional floating-point arithmetic \cite{Computer_Architecture}\cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard} treats every bit of its significand to be valid at all times, and continuously either generates artificial information or erases useful information to maintain this assumption during the normalization process \cite{Arithmetic_Digital_Computers}.
For example, the following calculation is performed exactly in integer format:
\begin{multline}
\label{eqn: int num calc}
64919224 \times 205117922 - 159018721 \times 83739041=\\
13316075197586562 - 13316075197586561 = 1;
\end{multline}
If Formula \eqref{eqn: int num calc} is carried out using conventional 64-bit floating-point arithmetic: 
\begin{multline}
\label{eqn: float num calc}
64919224 \times 205117922 - 159018721 \times 83739041 =\\ 
64919224.000000000 \times 205117922.000000000 - 159018721.000000000 \times 83739041.000000000 =\\
13316075197586562. - 13316075197586560. = 2. = 2.0000000000000000;
\end{multline}
\begin{enumerate}
\item  The normalization of input values may artificially pad zero bits in the least significant positions until every bit in the 53-bit significand is used, for example, converting $64919224$ to $64919224.000000000$.

\item  The results of addition or multiplication may exceed the bit range of the 53-bit significand, requiring rounding off from the least significant bits to fit within the 53-bit significand. 
This process can generate rounding errors, for example, converting $13316075197586561$ to $13316075197586560$, which produces a rounding error of $1$ in the least significant bit.

\item The results of subtraction or division can cancel values from higher significant bits, exposing rounding errors in the lower significant bits. For example, $13316075197586562. - 13316075197586560. = 2$.

\item  The normalization of subtraction or division results can amplify rounding errors toward the most significant bit by padding zeros, such as converting $2.$ to $2.0000000000000000$.  

\end{enumerate}
Formula \eqref{eqn: float num calc} demonstrates a classic case of catastrophic cancellation \cite{Rounding_Error}\cite{Algorithms_Accuracy}.

Because rounding errors from lower digits can propagate to higher digits, the $10^{-7}$ significance of the 32-bit IEEE floating-point format \cite{Computer_Architecture}\cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard} is usually insufficient for calculations involving input data with a precision of $10^{-2}$ to $10^{-4}$.
For more complex calculations, even the $10^{-16}$ significance of the 64-bit IEEE floating-point format \cite{Computer_Architecture}\cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard} may be sufficient for inputs with $10^{-2}$ to $10^{-4}$ precision.
This represents a fundamental controversy of conventional floating-point arithmetic.

Because rounding error is path dependent, a major objective of conventional numerical methods is to identify optimal computation strategies that minimize rounding errors, such as Gaussian elimination \cite{Algorithms_Accuracy}\cite{Numerical_Recipes}\cite{Precise_Numerical_Methods}, even though all alternative paths of root finding methods in linear algebra are mathematically equivalent \cite{Linear_Algebra}.

Self-censoring rules have been developed to limit such rounding error propagation \cite{Numerical_Recipes}\cite{Precise_Numerical_Methods}, for example, by avoiding the subtraction of results from large multiplications, as shown in Formula \eqref{eqn: float num calc}.  
However, these rules are neither enforceable nor easily adoptable, and they are even more difficult to quantify.
To date, research on rounding error propagation has focused primarily on linear calculations \cite{Rounding_Error}\cite{Algorithms_Accuracy}\cite{prob_interval}, or on special cases \cite{Numerical_Recipes}\cite{Error_Analysi_Digital_Filters}\cite{Floating-point_Digital_Filters}, whereas in practice, rounding errors often manifest as pervasive and mysterious numerical instabilities \cite{Chaotic_Dynamics}.

The forward rounding error study \cite{Algorithms_Accuracy} compares (1) the result containing rounding error and (2) the ideal result without rounding error, for example, by comparing a result obtained using 32-bit IEEE floating-point arithmetic with the corresponding result computed using 64-bit IEEE floating-point arithmetic \cite{rouding_higher_precision}.
The most recent study of this type presents an extremely optimistic view of numerical library errors, reporting them as fractions of the least significant bit of the floating-point significand \cite{rouding_higher_precision}.
However, such optimism contradicts the statistical tests on numerical library functions presented in this paper. 

The backward rounding error study \cite{Rounding_Error}\cite{Algorithms_Accuracy}\cite{prob_interval} estimates only the result uncertainty caused by rounding errors, thereby overlooking the bias that rounding errors introduce into the result value.
This analysis is typically limited to very small uncertainties because it relies on perturbation theory and it is tailored to each specific algorithm \cite{Rounding_Error}\cite{Algorithms_Accuracy}\cite{prob_interval}.

In contrast, variance arithmetic traces rounding error directly as part of uncertainty.
Statistical Taylor expansion applies generally to any analytic function, providing both result mean and deviation, and accommodates input uncertainties of any magnitude.
By demonstrating that the analytic result should be path independent, statistical Taylor expansion fundamentally challenges the conventional methodology of seeking an optimal execution strategy for a given analytic expression.




\subsubsection{Interval Arithmetic}

Interval arithmetic \cite{Precise_Numerical_Methods}\cite{Interval_Analysis}\cite{Worst_Case_Error_Bounds}\cite{Interval_Analysis_Theory_Applications}\cite{Interval_Arithmetic}\cite{Interval_Analysis_Notations} is currently a standard method for tracking computational uncertainty.  
Its objective is to ensure that a value remains strictly bounded within its bounding range throughout the computation.

However, the bounding range in interval arithmetic is not compatible with the approach commonly used in scientific and engineering measurements, which instead characterizes uncertainty in terms of the statistical mean and deviation \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}  
\footnote{
There is one attempt to connect intervals in interval arithmetic to confidence interval or the equivalent so-called p-box in statistics \cite{Statistics_For_Interval_Arithmetic}. 
Because this attempt seems to rely heavily on 1) specific properties of the uncertainty distribution within the interval and/or 2) specific properties of the functions upon which the interval arithmetic is used, this attempt does not seem to be generic. 
If probability model is introduced to interval arithmetic to allow tiny bounding leakage, the bounding range is much less than the corresponding pure bounding range \cite{prob_interval}.
Anyway, these attempts seem to be outside the main course of interval arithmetic.
}. 

Interval arithmetic represents only the worst-case scenario of uncertainty propagation.  
For example, in addition, it assumes that the two input variables are perfectly positively correlated \cite{Affine_Arithmetic}, thereby producing the widest possible bounding range.  
In contrast, if the variables were perfectly negatively correlated, the bounding range after addition would decrease \cite{Random_Interval_Arithmetic}  
\footnote{
Such case is called the best case in random interval arithmetic.
The vast overestimation of bounding ranges in these two worst cases prompts the development of affine arithmetic \cite{Affine_Arithmetic}\cite{Affine_Arithmetic_book}, which traces error sources using a first-order model.  
Being expensive in execution and depending on approximate modeling even for such basic operations as multiplication and division, affine arithmetic has not been widely used.  
In another approach, random interval arithmetic \cite{Random_Interval_Arithmetic} randomly chooses between the best-case and the worst-case intervals, so that it can no longer guarantee bounding without leakage.  
Anyway, these attempts seem to be outside the main course of interval arithmetic.
}.
This worst-case assumption can lead to order-of-magnitude over-estimations \cite{Prev_Precision_Arithmetic}.

The results of interval arithmetic can depend strongly on the specific algebraic form of an analytic function $f(x)$, a phenomenon known as
the \emph{dependency problem}. This issue is amplified in interval arithmetic \cite{Interval_Arithmetic} but also exists in conventional floating-point arithmetic \cite{Numerical_Recipes}.  

Furthermore, interval arithmetic lacks a mechanism to reject invalid calculations, even though every mathematical operation has a valid input range.
For example, it produces branched results for $1/(x \pm \Delta x)$ or $\sqrt{x \pm \Delta x}$ when $0 \in [x - \Delta x, x + \Delta x]$, whereas a context-sensitive uncertainty bearing arithmetic should reject such calculation naturally.

In contrast, variance arithmetic specifies each value by its mean and deviation.
It does not suffer from the dependency problem.
Its statistical framework naturally rejects certain input intervals on mathematical grounds, such as inversion and square root operations when the statistical bounding range includes zero.


\subsubsection{Statistical Propagation of Uncertainty}

Statistical propagation of uncertainty treats each imprecise value as a random variable and seeks correlations among such random variables to calculate resulting mean and deviation \cite{Statistical_Arithmetic}.

In contrast, statistical Taylor expansion interprets the uncertainty of an imprecise value as a limitation in obtaining its precise value. 
This assumption aligns with most error analyses in the literature \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
Its statistical foundation is the uncorrelated uncertainty assumption \cite{Prev_Precision_Arithmetic}: any two imprecise values do not correlate in their uncertainties.

Statistical propagation of uncertainty appears to have applied an incorrect statistical context to uncertainty.
For example, a time series is a random variable whereas each imprecise value within the time series is merely an imprecise measurement.
The uncorrelated uncertainty assumption can hold for each imprecise value in a time series, even though the time series can highly correlate to another time series at their signal level, or even correlated to itself as a periodic time series.
If the uncertainties of some imprecise values are correlated, these imprecise values contain systematic errors among them \cite{Statistical_Methods}.
Systematic errors should be treated as unwanted signals but not as normal uncertainties \cite{Statistical_Methods}.
In short, uncertainty should not be treated as signal.



\subsubsection{Significance Arithmetic}

Significance arithmetic \cite{Significance_Arithmetic} seeks to track the number of reliable bits in an imprecise value throughout a calculation.  
In its early implementations \cite{Digital_Significance_Arithmetic}\cite{Unnormalized_Arithmetic}, significance arithmetic was based on simple operational rules applied to reliable bit counts rather than on formal statistical methods. 
In these approaches, the reliable bit count is treated as an integer, even though in practice it can take a fractional value \cite{Mathematica_Significance_Arithmetic}.
This constraint can produce an artificial step-wise reduction in significance.  
The implementation of significance arithmetic in Mathematica \cite{Mathematica_Significance_Arithmetic} employs a linear error model consistent with the first-order approximation of interval arithmetic \cite{Precise_Numerical_Methods}\cite{Interval_Analysis_Theory_Applications}\cite{Interval_Arithmetic}. 

One limitation of significance arithmetic is its inability to specify uncertainty accurately \cite{Prev_Precision_Arithmetic}.
For example, if the least significant bit of the significand is used to represent uncertainty,  the result precision can be very coarse, as in $1 \pm 10^{-3} = 1024 \times 2^{-10}$  \cite{Prev_Precision_Arithmetic}.
Introducing a limited number of bits to represent uncertainty does not fully resolve this issue.
Consequently, various attempts to develop  floating-point arithmetic without normalization \cite{Unnormalized_Arithmetic} have not been widely adopted, and conventional floating-point arithmetic continues to dominate the numerical world.
For this reason, variance arithmetic has abandoned the significance arithmetic principles of its predecessor \cite{Prev_Precision_Arithmetic}.



\subsubsection{Stochastic Arithmetic}

Stochastic arithmetic \cite{Stochastic_Arithmetic}\cite{CADNA_library} randomizes the least significant bits of each input floating-point value, repeats the same calculation multiple times, and then applies statistical analysis to identify the invariant digits among the results as significant digits.  
However, this approach can be computationally expensive as the number of repetitions required for each input depends on the algorithm and can increase substantially when the algorithm contains branching operations.  

In contrast, statistical Taylor expansion provides a direct characterization of the result's mean and deviation without sampling.






\subsection{ An Overview of This Paper}

This paper presents the theory of statistical Taylor expansion, its implementation as variance arithmetic, and the corresponding validation tests.
Section \ref{sec: introduction} compares statistical Taylor expansion and variance arithmetic with other established uncertainty bearing arithmetic.
Section \ref{sec: statistical Taylor expansion} develops the theoretical foundation of statistical Taylor expansion.
Section \ref{sec: variance arithmetic} describes variance arithmetic as an implementation of statistical Taylor expansion.
Section \ref{sec: validation} outlines the standards and methodologies used to validate variance arithmetic.
Section \ref{sec: polynomial} illustrates variance arithmetic in computing polynomial.
Section \ref{sec: Math Library} evaluates variance arithmetic on common mathematical library functions.
Section \ref{sec: matrix} applies variance arithmetic to adjugate matrix and matrix inversion.
Section \ref{sec: Moving-Window Linear Regression} demonstrates its application to time-series data. 
Section \ref{sec: FFT} examines the impact of numerical library errors and shows that these errors can be significant.
Section \ref{sec: recursion} applies variance arithmetic to regression analysis.
Section \ref{sec: conclusion and discussion} concludes with a summary and discussion.






\ifdefined\Verbose
\clearpage
\fi
\section{Statistical Taylor Expansion}
\label{sec: statistical Taylor expansion}

\subsection{The Uncorrelated Uncertainty Assumption}

\iffalse

\begin{align*}
& \frac{1}{\gamma_{P}} - 1 = \left(\frac{1}{\gamma} -1\right) \frac{1}{P^2}; \\
& P^2 \left( \frac{1}{\gamma_{P}} - 1 \right) + 1 = \frac{1}{\gamma};
\end{align*}

\fi

\ifdefined\Verbose
\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Signal_and_Noise.pdf}
\captionof{figure}{
Effect of noise on bit values of a measured value.  The triangular wave signal and the added white noise are shown at top using the thin black line and the grey area, respectively.  The values are measured by a theoretical 4-bit Digital-to-Analog Converter in ideal condition, assuming LSB is the 0th bit.  The measured 3rd and 2nd bits without the added noise are shown using thin black lines, while the mean values of the measured 3rd and 2nd bits with the added noise are shown using thin grey lines.
This figure is a reproduction of Figure 1 in \cite{Prev_Precision_Arithmetic}
}
\label{fig: Signal_and_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.25in]{Independent_Uncertainty_Assumption.pdf} 
\captionof{figure}{
$\gamma$ vs $P$ for different $\gamma_P$ in Formula \eqref{eqn: precision correlation}.
If $\gamma_P$ is the maximal allowed correlation on the uncertainty level for uncorrelated uncertainty assumption to hold, then $\gamma$ is the maximal allowed correlation for two input to hold.
For a given $\gamma_P$, $\gamma$ increases with finer $P$.
This figure is a reproduction of Figure 2 in \cite{Prev_Precision_Arithmetic}.
}
\label{fig: Independent_Uncertainty_Assumption}
\end{figure}
\fi

The \emph{uncorrelated uncertainty assumption} \cite{Prev_Precision_Arithmetic} states that the uncertainties of any two input imprecise values are uncorrelated.  
This assumption is satisfied when there is no systematic errors in the inputs \cite{Statistical_Methods} and  is consistent with standard methods for processing experimental data \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
The uncorrelated uncertainty assumption permits the signals themselves to be highly correlated in the following statistical test \cite{Prev_Precision_Arithmetic}.

\ifdefined\Verbose
\begin{quotation}

Let $X$, $Y$, and $Z$ denote three mutually independent random variables \cite{Probability_Statistics} with variance $V(X)$, $V(Y)$ and $V(Z)$, respectively.  
Let $\alpha$ denote a constant.  
Let $Cov()$ denote the covariance function.  
Let $\gamma$ denote the correlation between $(X + Y)$ and $(\alpha X + Z)$. And let:
\begin{align}
\label{eqn: uncertainty ratio}
& \eta _{1} ^{2} \equiv \frac{V(Y)}{V(X)}; \eqspace
 \eta _{2} ^{2} \equiv \frac{V(Z)}{V(\alpha X)} =\frac{V(Z)}{\alpha ^{2} V(X)};  \\
\label{eqn: uncertainty correlation}
& \gamma =\frac{Cov(X+Y,\alpha X+Z)}{\sqrt{V(X+Y)} \sqrt{V(\alpha X+Z)}}  =\frac{\alpha /|\alpha |}{\sqrt{1+\eta _{1} ^{2} } \sqrt{1+\eta _{2} ^{2}}} \equiv \frac{\alpha /|\alpha |}{1+\eta ^{2}};
\end{align}
Formula \eqref{eqn: uncertainty correlation} gives the correlation $\gamma$ between two random variables, each of which contains a completely uncorrelated part and a completely correlated part $X$, with $\eta$ being the average ratio between these two parts.  
Formula \eqref{eqn: uncertainty correlation} can also be interpreted reversely: if two random variables are correlated by $\gamma$, each of them can be viewed hypothetically as containing a completely uncorrelated part and a completely correlated part, with $\eta$ being the average ratio between these two parts \footnote{The widely used common factor analysis is also based on this principle.}.
The correlated parts common to different measurements are regarded as signals, which can either be desired or unwanted.

One special application of Formula \eqref{eqn: uncertainty correlation} is the correlation between a measured signal and its true signal, in which noise is the uncorrelated part between the two.  
Figure \ref{fig: Signal_and_Noise} shows the effect of noise on the most significant two bits of a 4-bit measured signal when $\eta=1/4$.  
Its top chart shows a triangular waveform between 0 and 16 as a black line, and a white noise between -2 and +2, using the grey area.  
The measured signal is the sum of the triangle waveform and the noise.  
The middle chart of Figure \ref{fig: Signal_and_Noise} shows the values of the 3rd digit of the true signal as a black line, and the mean values of the 3rd bit of the measurement as a grey line.  
The 3rd bit is affected by the noise during its transition between 0 and 1.  
For example, when the signal is slightly below 8, only a small positive noise can turn the 3rd digit from 0 to 1.  
The bottom chart of Figure \ref{fig: Signal_and_Noise} shows the values of the 2nd digit of the signal and the measurement as a black line and a grey line, respectively.  
Figure \ref{fig: Signal_and_Noise} clearly shows that the correlation between the measurement and the true signal is less at the 2nd digit than at the 3rd digit.  
Quantitatively:
\begin{itemize}
\item  The overall measurement is 99.2\% correlated to the signal with $\eta=1/8$;
\item  The 3rd digit of the measurement is 97.0\% correlated to the signal with $\eta=1/4$;
\item  The 2nd digit of the measurement is 89.4\% correlated to the signal with $\eta=1/2$;
\item  The 1st digit of the measurement is 70.7\% correlated to the signal with $\eta=1$;
\item  The 0th digit of the measurement is 44.7\% correlated to the signal with $\eta=2$.
\end{itemize}
The above conclusion agrees with the common experiences that, below the noise level of measured signals, noises rather than true signals dominate each digit.  

Similarly, while the correlated portion between two values has the same value at each bit of the two values, the ratio of the uncorrelated portion to the correlated portion increases by 2-fold for each bit down from MSB of the two values. 
Quantitatively, let $P$ denote the precision of an imprecise value, and let $\eta_{P}$ denote the ratio of the uncorrelated portion to the correlated portion at level of uncertainty; then $\eta_{P}$ increases with decreased $P$ according to Formula \eqref{eqn: uncertainty level}. 
According to Formula \eqref{eqn: uncertainty correlation}, if two significant values are overall correlated with $\gamma$, at the level of uncertainty the correlation between the two values decreases to $\gamma_P$ according to Formula \eqref{eqn: precision correlation}.
\begin{align}
\label{eqn: uncertainty level}
& \eta_{P} = \frac{\eta}{P}, \eqspace P < 1; \\
\label{eqn: precision correlation}
& \frac{1}{\gamma_{P}} - 1 = \left(\frac{1}{\gamma} -1\right) \frac{1}{P^2}, \eqspace P < 1;
\end{align}

Figure \ref{fig: Independent_Uncertainty_Assumption} plots the relation of $\gamma$ vs. $P$ for each given $\gamma_{P}$ in Formula \eqref{eqn: precision correlation}.  
When $\gamma_{P}$ is less than a predefined maximal threshold (e.g., 2\%, 5\% or 10\%), the two values can be deemed uncorrelated to each other at the level of uncertainty.  
For each independence standard $\gamma_{P}$, there is a maximal allowed correlation between two values below which the uncorrelated uncertainty assumption of statistical Taylor expansion holds.  
The maximal allowed correlation is a function of the larger precision of the two values according to Formula \eqref{eqn: precision correlation}.  
Figure \ref{fig: Independent_Uncertainty_Assumption} shows that for two precisely measured values, their correlation $\gamma$ is allowed to be quite high.  

\end{quotation}

\else	% Verbose

Suppose two signals have a correlation coefficient $\gamma$, and measured precisions $P_1$ and $P_2$, respectively.
Let $P$ be the coarser of $P_1$ and $P_2$.
At the level of $P$, the correlation is reduced to $\gamma_P$ according to Formula \eqref{eqn: precision correlation} \cite{Prev_Precision_Arithmetic}.
The  value of $\gamma_P$ decreases rapidly as $P$ becomes finer, so that when $P$ is sufficiently fine, the correlation $\gamma_P$ between the uncertainties of the two signals is effectively zero \cite{Prev_Precision_Arithmetic}.
\begin{equation}
\label{eqn: precision correlation}
\frac{1}{\gamma_{P}} - 1 = \left(\frac{1}{\gamma} -1\right) \frac{1}{P^2};
\end{equation}

\fi


\subsection{Distributional Zero and Distributional Pole}

\iffalse

To solve for mode:
\begin{align*}
& \rho(\tilde{y}, y, \delta y) = \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z});
\eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x}; \\
& 0 = \frac{d \rho(\tilde{y}, y, \delta y)}{d \tilde{y}} = \frac{d^2 \tilde{z}}{d \tilde{y}^2} N(\tilde{z}) - \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z}) \tilde{z}; \\
& \frac{d^2 \tilde{z}}{d \tilde{y}^2} = \frac{d \tilde{z}}{d \tilde{y}} \tilde{z}; \eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x};
\end{align*}

The exponential function:
\begin{align*}
f(x) = e^x:& \eqspace 
\tilde{z} = \frac{\log(\tilde{y}) - x}{\delta x}, \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\tilde{y} \delta x} = \frac{1}{\frac{d}{d \tilde{z}} e^{x + \tilde{z} \delta x}}; \eqspace \\
\frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z})
&= e^{-\log(\tilde{y})} \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2}{2 \delta^2 x}}
 = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2 + 2 \log(\tilde{y}) \delta^2 x }{2 \delta^2 x}} \\
& = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - (x - \delta^2 x))^2 + 2 x \delta^2 x - (\delta^2 x)^2 }{2 \delta^2 x}}
 = N(\frac{\log(\tilde{y}) - (x - \delta^2 x)}{\delta x}) e^{-x + \frac{\delta^2 x}{2}}; \\
& 0 = \tilde{z} e^{x + \tilde{z} \delta x} + (\delta x) e^{x + \tilde{z} \delta x}; \eqspace
 \tilde{z} = -(\delta x) = \frac{1}{\delta x}(x - (\delta x)^2 - x);
\end{align*}

The log function $f(x) = \log(x)$:
\begin{align*}
& \tilde{z} = \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} e^{\tilde{y}} = \frac{1}{\delta x} (x + \tilde{z} \delta x)
 = \frac{1}{\frac{d}{d \tilde{z}} \log(x + \tilde{z} \delta x)}; \\
& \frac{1}{\delta x} e^{\tilde{y}} = (\frac{1}{\delta x} e^{\tilde{y}})^2 \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
(e^{\tilde{y}})^2 - x e^{\tilde{y}} - \delta^2 x = 0; \\
e^{\tilde{y}_m} &= \frac{x + \sqrt{x^2 + 4 \delta^2 x}}{2}; \\
& \eqspace 0 = \frac{\tilde{z}}{x + \tilde{z} \delta x} - (\delta x) \frac{1}{(x + \tilde{z} \delta x)^2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} - (\delta x); \\
& \tilde{z} = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 + 4 (\delta x)^2}}{2} - x);
\end{align*}

The power mode $f(x) = x^{\frac{1}{p}}$:
\begin{align*}
& \tilde{z} = \frac{\tilde{y}^p - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} p \tilde{y}^{p-1}; \\
& \frac{1}{\delta x} p (p - 1) \tilde{y}^{p-2} = (\frac{1}{\delta x} p \tilde{y}^{p-1})^2 \frac{\tilde{y}^p - x}{\delta x}; \\
& \tilde{y}^{2p} - x \tilde{y}^{p} - \frac{p - 1}{p} \delta^2 x = 0; \\
\tilde{y}_m^p &= \frac{1}{2} (x + \sqrt{x^2 + 4 \frac{p - 1}{p} \delta^2 x}); \\
& 0 = c (x + \tilde{z} \delta x)^{c-1} \tilde{z} + c (c-1) \delta x (x + \tilde{z} \delta x)^{c-2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} + (c - 1) (\delta x); \\
& \tilde{z}_m = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2} - x); \eqspace 
f^{-1}(\tilde{y}_m) = \frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2}; \\
p = 2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 2 \tilde{y}; \eqspace 
  \tilde{y}_m^{\frac{1}{2}} = \frac{1}{2} \left( x + \sqrt{x^2 + 2 \delta^2 x} \right); \\
p = 3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 3 \tilde{y}^2; \eqspace  
  \tilde{y}_m^{\frac{1}{3}} = \frac{1}{2} \left( x \pm \sqrt{x^2 + \frac{8}{3} \delta^2 x} \right); \\
p = 1/2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{2} \tilde{y}^{-\frac{1}{2}}; \eqspace 
  \tilde{y}_m^2 = \frac{1}{2} \left( x + \sqrt{x^2 - 4 \delta^2 x} \right); \\
p = 1/3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{3} \tilde{y}^{-\frac{2}{3}}; \eqspace
  \tilde{y}_m^2 = \frac{1}{2} \left( x \pm \sqrt{x^2 - 8 \delta^2 x} \right); \\
p = -1:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = - \frac{1}{\delta x} \tilde{y}^{-2}; \eqspace 
  \tilde{y}_m^{-1} = \frac{1}{2} \left( x + \sqrt{x^2 + 8 \delta^2 x} \right); 
\end{align*}


The distribution difference:
\begin{align*}
&\nu = \frac{x - f^{-1}(\tilde{y}_m)}{\delta x}: \eqspace 
 N(\frac{f^{-1}(\tilde{y}) - f^{-1}(\tilde{y}_m)}{\delta x} )
 = N(\tilde{z} + \nu) = N(\tilde{z}) e^{-\tilde{z} \nu} e^{-\frac{1}{2} \nu^2}; \\
\eta &\equiv \int |\varrho(\tilde{y}, y, \delta y) - \rho(\tilde{y}, y, \delta y)| d \tilde{y}
 =  \int |\rho(f^{-1}(\tilde{y}), f^{-1}(\tilde{y_m}), \delta x) - \rho(f^{-1}(\tilde{y}), x, \delta x)| d f^{-1}(\tilde{y}) \\
&= \int |N(\tilde{z} + \nu) - N(\tilde{z})| d \tilde{z} 
 = \int |e^{-\frac{1}{2} \nu^2} e^{-\tilde{z} \nu} - 1| N(\tilde{z}) d \tilde{z}; \\
&= | \int |\sum_{m=0}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=0}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n - 1| 
   N(\tilde{z}) d \tilde{z} | \\
&= | \int \left(\sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n
  - \frac{1}{2} \nu^2 - \nu \tilde{z} \right) N(\tilde{z}) d \tilde{z} | \\
&= \frac{1}{2} \nu^2 - \sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{\nu^{2n}}{2^n n!}
 = \frac{1}{2} \nu^2 - \sum_{m=2}^{\infty} \sum_{n=1}^{m-1} \frac{(-1)^m}{2^m (m - n)! n!} \nu^{2m} \\
&\simeq \frac{1}{2} \nu^2 - \frac{1}{4} \nu^4 + \frac{1}{8} \nu^6 - \frac{7}{192} \nu^8;
\end{align*}
When $f(x)=x^c$:
\begin{align*}
\nu &= \frac{x - \frac{1}{2} (x + \sqrt{x^2 + (1 - c) 4 \delta^2 x})}{\delta x} = \frac{1 - \sqrt{1 + (1 - c) 4 P(x)^2}}{2 P(x)} \\
&= - \sum_{m=1} \frac{(1 - c)^m (2P(x))^{2m - 1}}{m!} \prod_{n=1}^{m} \frac{\frac{3}{2} -n}{n} \\
&\simeq -(1 - c) P(x) + \frac{1}{4} (1 - c)^2 P(x)^3 - \frac{1}{4} (1 - c)^3 P(x)^5; \\
\eta &= \frac{1}{2} (1-c)^2 P(x)^2 - \frac{1}{4} (1-c)^3 (2-c) P(x)^4 + 1/8 (1-c)^4 (c^2-4c+7) P(x)^6
\end{align*}

\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Distribution.pdf} 
\captionof{figure}{
Probability density function of $\tilde{y} = \tilde{x}^2$, for various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\sqrt{\tilde{y}}$.
}
\label{fig: Square_Distribution}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Root_Distribution.pdf} 
\captionof{figure}{
Probability density function for $\tilde{y} = \sqrt{\tilde{x}}$, various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with the distributional mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\tilde{y}^2$.
}
\label{fig: Square_Root_Distribution}
\end{figure}

Let $\tilde{y} = f(\tilde{x})$ be a strictly monotonic function, so that its inverse function $\tilde{x} = f^{-1}(\tilde{y})$ exists.
Formula \eqref{eqn: function distribution} shows the probability density function of $\tilde{y}$ \cite{Statistical_Methods}\cite{Probability_Statistics}.
In Formula \eqref{eqn: function distribution}, the same distribution can be expressed in terms of either $\tilde{x}$ or $\tilde{y}$, which are simply different representations of the same underlying random variable.
Using Formula \eqref{eqn: function distribution}, Formula \eqref{eqn: power distribution} specifies the $\rho(\tilde{y}, \mu_y, \sigma_y)$ for $x^c$ when $\rho(\tilde{x}, \mu, \sigma)$ is Gaussian.
\begin{align}
\label{eqn: function distribution}
\rho(\tilde{x}, \mu, \sigma) d\tilde{x} &= \rho(f^{-1}(\tilde{y}), \mu, \sigma) \frac{d\tilde{x}}{d\tilde{y}} d\tilde{y} 
= \rho(\tilde{y}, \mu_y, \sigma_y) d\tilde{y}; \\
\label{eqn: power distribution}
&y = x^c: \eqspace \rho(\tilde{y}, \mu_y, \sigma_y) = c \tilde{y}^{\frac{1}{c}-1} \frac{1}{\sigma} N(\frac{\tilde{y}^\frac{1}{c} - \mu}{\sigma}); 
\end{align}

Viewed in the $f^{-1}(\tilde{y})$ coordinate, $\rho(\tilde{y}, \mu_y, \sigma_y)$ is given by $\rho(\tilde{x}, \mu, \sigma)$ modulated by $\frac{d\tilde{x}}{d\tilde{y}} = 1/f^{(1)}_x$, in which $f^{(1)}_x$ is the first derivative of $f(x)$ with respect to $x$.
A \emph{distributional zero} of the uncertainty distribution occurs when $f^{(1)}_x=\infty \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = 0$, while a \emph{distributional pole} occurs when $f^{(1)}_x=0 \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = \infty$.
Zeros and poles provide the strongest local modulation to $\rho(\tilde{x}, \mu, \sigma)$:
\begin{itemize}
\item
If $\tilde{y} = \alpha + \beta \tilde{x}$, the resulting distribution is identical to the original distribution, since $\rho(\tilde{y}, \mu_y, \sigma_y) = \rho(\tilde{y}, \alpha + \beta \mu, \beta \sigma)$ \cite{Probability_Statistics}.
A linear transformation generates neither a distributional zero nor a distributional pole, according to Formula \eqref{eqn: function distribution}.

\item 
Figure \ref{fig: Square_Distribution} shows the probability density function for $(x \pm 1)^2$ according to Formula \eqref{eqn: power distribution}, which exhibits a distributional pole at $x=0$.
The distribution $(0 \pm 1)^2$ corresponds to the $\chi^2$ distribution \cite{Statistical_Methods}.
At the distributional pole, the probability density function resembles a Delta distribution.

\item 
Figure \ref{fig: Square_Root_Distribution} illustrates the probability density function for $\sqrt{x \pm 1}$ according to Formula \eqref{eqn: power distribution}, which has a distributional zero at $x=0$.
At the distributional zero, the probability density function is zero.

\end{itemize}
In both Figure \ref{fig: Square_Distribution} and \ref{fig: Square_Root_Distribution}, $\rho(\tilde{y}, \mu_y, \sigma_y)$ closely representation resembles $\rho(\tilde{x}, \mu, \sigma)$ when the mode of $\rho(\tilde{x}, \mu, \sigma)$ lies sufficiently far away from either a distributional pole or a distributional zero, thereby allowing for a generic characterization of the output. 



\subsection{Statistical Taylor Expansion}

Formula \eqref{eqn: function distribution} provides the uncertainty distribution of an analytic function.
However, in most scientific and engineering calculations, the primary interest lies not in the full result distribution but in few summary statistics of the result, such as the mean and deviation \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
These simplified statistics can be obtained through a statistical Taylor expansion.

Let $\rho(\tilde{x}, \mu, \sigma)$ denote the probability density function of a random variable $\tilde{x}$ with the distribution mean $\mu$ and distribution deviation $\sigma$.
Define $\tilde{z} \equiv (\tilde{x} - \mu)/\sigma$ and let $\rho(\tilde{z})$ be the normalized form of $\rho(\tilde{x}, \mu, \sigma)$ such that $\tilde{z}$ has distribution mean $0$ and distribution deviation $1$.
For example, Normal distribution $N(\tilde{z})$ is the normalized form of the Gaussian distribution.

\begin{align}
\label{eqn: bound moment}
\zeta(n) \equiv& \int_{\mu - \varrho \sigma}^{\mu + \kappa \sigma} \tilde{x}^n \rho(\tilde{x}, \mu, \sigma) d\tilde{x} 
	=  \int_{-\varrho}^{+\kappa} \tilde{z}^n \rho(\tilde{z}) d \tilde{z};\\
\label{eqn: Taylor 1d} 
f(x + \tilde{x}) =& f(x + \tilde{z} \sigma) = \sum_{n=0}^{\infty} \frac{f^{(n)}_x}{n!} \tilde{z}^n \sigma^n; \\
\label{eqn: Taylor 1d mean}
\overline{f(x)} =& \int_{-\varrho}^{+\kappa} f(x + \tilde{x}) \rho(\tilde{x}, \mu, \sigma) d \tilde{x}
  = \sum_{n=0}^{\infty}\sigma^n \frac{f^{(n)}_x}{n!} \zeta(n); \\
\label{eqn: Taylor 1d variance}
\delta^2 f(x) =& \overline{(f(x) - \overline{f(x)})^2} = \overline{f(x)^2} - \overline{f(x)}^2 \nonumber \\
	=& \sum_{n=1}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{f^{(j)}_x}{j!} \frac{f^{(n-j)}_x}{(n-j)!} \big(\zeta(n) - \zeta(j) \zeta(n -j) \big);  
\end{align}

An confidence interval \cite{Probability_Statistics} $\tilde{x} \in [\mu - \varrho \sigma, \mu + \kappa \sigma]$ can describe a sampling from an underlying distribution, where $0 < \varrho, \kappa$ specify the \emph{bounding ranges}. 
Formula \eqref{eqn: bound moment} defines the \emph{bound moment} $\zeta(n)$.
As discussed later, $\kappa$ determines $\varrho$, while $\kappa$ itself is determined by both the sample size $N$ and the underlying distribution of the input.
When $N \rightarrow \infty$, $\varrho, \kappa \rightarrow \infty$, so that by definition $\zeta(0) = 1$, $\zeta(1) = 0$, and $\zeta(2) = 1$.
When $N$ is bounded, $\varrho$ and $\kappa$ are also bounded, so that $\zeta(0) < 1$, and $\zeta(2) < 1$.
The probability of $\tilde{x} \not \in [\mu - \varrho \sigma, \mu + \kappa \sigma]$ is defined as the \emph{bounding leakage} $\epsilon(\kappa) \equiv 1 - \zeta(0, \kappa)$.

An analytic function $f(x)$ can be accurately evaluated over in a range using the Taylor series as shown in Formula \eqref{eqn: Taylor 1d}.
Using Formula \eqref{eqn: bound moment}, Formula \eqref{eqn: Taylor 1d mean} and Formula \eqref{eqn: Taylor 1d variance} yield the mean $\overline{f(x)}$ and the variance $\delta^2 f(x)$ of $f(x)$, respectively.
The difference $\overline{f(x)} - f(x)$ is defined as the \emph{uncertainty bias}, representing the effect of input uncertainty on the resulting value.

\begin{align}
\label{eqn: Taylor 2d}
&f(x + \tilde{x}, y + \tilde{y}) = \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{f^{(m,n)}_{(x,y)}}{m! n!} \tilde{x}^m \tilde{y}^n; \\
\label{eqn: Taylor 2d mean}
\overline{f(x,y)} =& \int \int f(x + \tilde{x}, y + \tilde{y}) \rho(\tilde{x}, \mu_x, \sigma_x) \rho(\tilde{y}, \mu_y, \sigma_y)\; d \tilde{x} d \tilde{y} \\
=& \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} (\sigma_x)^m (\sigma_y)^n \frac{f^{(m,n)}_{(x,y)}}{m!\;n!} \zeta_x(m) \zeta_y(n); \nonumber \\
\label{eqn: Taylor 2d variance}
\delta^2 f(x, y) =& \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} (\sigma_x)^m (\sigma_y)^n \sum_{i=0}^{m} \sum_{j=0}^{n} 
		\frac{f^{(i,j)}_{(x,y)}}{i!\;j!}\frac{f^{(m-i, n-j)}_{(x,y)}}{(m-i)!\;(n-j)!} \nonumber \\
	&\largespace\largespace \big( \zeta_x(m) \zeta_y(n) - \zeta_x(i)\zeta_x(m-i)\; \zeta_y(j)\zeta_y(n-j) \big);
\end{align}
Under the uncorrelated uncertainty assumption, Formula \eqref{eqn: Taylor 2d mean} and \eqref{eqn: Taylor 2d variance} compute the mean and variance of the Taylor expansion given in Formula \eqref{eqn: Taylor 2d}, where $\zeta_x(m)$ and $\zeta_y(n)$ denote the variance moments for $x$ and $y$, respectively.
Although Formula \eqref{eqn: Taylor 2d variance} is only for 2-dimensional, it can be extended easily to any dimension.


\subsection{Bounding Symmetry}

A bounding range $[\mu - \varrho \sigma, \mu + \kappa \sigma]$ is \emph{mean-preserving} if $\zeta(1) = 0$, meaning that it has the same mean as the unbounded distribution.
To achieve mean preserving, $\kappa$ determines $\varrho$, so that $\zeta(n)$ becomes $\zeta(n, \kappa)$.
Under a mean preserving bounding, Formula \eqref{eqn: addition mean} and \eqref{eqn: addition variance} provide the result for $x \pm y$, while Formula \eqref{eqn: multiplication mean} and \eqref{eqn: multiplication variance} give the result for $x y$:
\begin{align}
\label{eqn: addition mean}
\overline{x \pm y} &= \zeta(0, \kappa_x) x \pm \zeta(0, \kappa_x) y; \\
\label{eqn: addition variance}
\delta^2 (x \pm y) &= \zeta(2, \kappa_x) (\sigma_x)^2 + \zeta(2, \kappa_y) (\sigma_y)^2; \\
\label{eqn: multiplication mean}
\overline{x y} &= \zeta(0, \kappa_x) x \; \zeta(0, \kappa_y) y; \\
\label{eqn: multiplication variance}
\delta^2 (x y) &= \zeta(2, \kappa_x) (\sigma_x)^2 y^2 + x^2 \zeta(2, \kappa_y) (\sigma_y)^2 + \zeta(2, \kappa_x) (\sigma_x)^2 \; \zeta(2, \kappa_y) (\sigma_y)^2;
\end{align}
When $N \rightarrow \infty$, $\zeta(0, \kappa) \rightarrow 1$ and $\zeta(2, \kappa) \rightarrow 1$, making Formula \eqref{eqn: addition mean} and \eqref{eqn: addition variance} the convolution results for $x \pm y$ \cite{Probability_Statistics}, and Formula \eqref{eqn: multiplication mean} and \eqref{eqn: multiplication variance} the corresponding results of the product distribution for $x y$ \cite{Probability_Statistics}.

For any input distribution $\rho(\tilde{x}, \mu, \sigma)$ that is symmetric about its mean $\mu$, any bounding range $[\mu - \kappa \sigma, \mu + \kappa \sigma]$ satisfies $\zeta(2n+1) = 0$, which further simplifies the statistical Taylor expansion.
Both Gaussian and Uniform distributions are symmetric.



\subsection{Bounding Asymptote}

\iffalse
\begin{align*}
C(n, \kappa) \equiv& \int_{-\infty}^{\kappa} \tilde{x}^n \rho(\tilde{x} - \mu) d \tilde{x} 
	= \int_{-\infty}^{\kappa} \tilde{x}^n \frac{d}{d \tilde{x}} C(0, \tilde{x}) 
	= {\kappa}^n C(0, \kappa) - n \int_{-\infty}^{\kappa} \tilde{x}^{n - 1} C(0, \tilde{x}) d \tilde{x}  \\
\zeta(n, \kappa) =& \int_{\mu - \varrho \sigma}^{\mu + \kappa \sigma} \tilde{x}^n \rho(\tilde{x}) d \tilde{x} = C(n, \kappa) - C(n, \varrho);
\end{align*}
\fi

Empirically, Formula \eqref{eqn: bound moment UN}, \eqref{eqn: moment factor higher}, and \eqref{eqn:  fat-tail moment limit}  demonstrate that as $2n \rightarrow +\infty$, $\zeta(2n) \rightarrow \kappa^{2n} / (2n)$.



\subsubsection{Uniform Input Uncertainty}

For uniform distribution, the bounding range is $\kappa = \sqrt{3}$, and the bounding leakage $\epsilon = 0$.
Formula \eqref{eqn: bound moment UN} provides $\zeta(2n)$ while $\zeta(2n + 1) = 0$.
\begin{equation}
\label{eqn: bound moment UN}
\zeta(2n) = \int_{-\sqrt{3}}^{+\sqrt{3}} \frac{1}{2 \sqrt{3}} \tilde{z}^{2n} d \tilde{z} = \frac{(\sqrt{3})^{2n}}{2n + 1}; 
\end{equation}




\subsubsection{Gaussian Input Uncertainty}

\iffalse

\begin{align*}
\int_{0}^{\kappa}& \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}
		= - \int_{0}^{\kappa} \tilde{z}^{2n - 1} d\; e^{-\frac{1}{2} \tilde{z}^2}
		= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \tilde{z}^{2n - 1} e^{-\frac{1}{2} \tilde{z}^2} \Big |_{0}^{\kappa} \\
		&= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1) (2n - 3) \int_{0}^{\kappa} \tilde{z}^{2n - 4}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} 
			- (2n - 1) \kappa^{2n - 3} e^{-\frac{1}{2} \kappa^2} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1)!! \left( \int_{0}^{\kappa} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - e^{-\frac{1}{2} \kappa^2} \sum_{j=1}^{n} \frac{\kappa^{2n - 2j + 1}}{(2j - 1)!!}  \right) \\
		&= (2n - 1)!! \left(\sqrt{2} \int_{0}^{\frac{\kappa}{\sqrt{2}}} e^{-(\frac{\tilde{z}}{\sqrt{2}})^2} d \frac{\tilde{z}}{\sqrt{2}} 
			- e^{-\frac{1}{2} \kappa^2} \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!} \right) \\
		& = (2n - 1)!! e^{-\frac{1}{2} \kappa^2} \left( \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}})  e^{\frac{1}{2} \kappa^2} 
			- \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!}  \right) \\
		&=  e^{-\frac{1}{2} \kappa^2} \sum_{j=n}^{+\infty} \frac{(2n - 1)!!}{(2j + 1)!!} \kappa^{2j + 1}
		= e^{-\frac{1}{2} \kappa^2} \kappa^{2n} \sum_{j=1}^{+\infty} \frac{(2n - 1)!!}{(2n - 1 + 2j)!!} \kappa^{2j - 1};
\end{align*}
According to Eq (21) in https://mathworld.wolfram.com/DoubleFactorial.html:
\begin{align*}
\sum_{j=0}^{+\infty} \frac{\kappa^{2j + 1}}{(2j + 1)!!} = \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}}) e^{-\frac{1}{2} \kappa^2};
\end{align*}

\begin{align*}
\zeta(2n) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} \rho(\tilde{z}) d \tilde{z} = 2 N(\kappa) \kappa^{2n} \sum_{j=0}^{\infty} \kappa^{2j+1} \frac{(2n-1)!!}{(2j + 2n+1)!!}; \\
\zeta(2n) &= 2 N(\kappa) \frac{\kappa^{2n+1}}{2n + 1} + 2 N(\kappa) \frac{\kappa^{2n+2}}{2n+1} \sum_{j=1}^{\infty} \kappa^{2(j-1)+1} \frac{(2(n+1)- 1)!!}{(2(j-1)+2(n+1)+1)!!} \\
 &= 2 N(\kappa) \frac{\kappa^{2n + 1}}{2n + 1} + \frac{\zeta(\kappa, 2n + 2)}{2n + 1}; \\
\zeta(\kappa, 2n + 2) &= (2n + 1) \zeta(\kappa, 2n) - 2 N(\kappa) \kappa^{2n + 1};
\end{align*}

\fi

The central limit theorem states that the sum of many independent and identically distributed random variables converges toward a Gaussian distribution \cite{Probability_Statistics}.
This convergence occurs rapidly \cite{Prev_Precision_Arithmetic}.
In digital computation, multiplication is implemented as a sequence of shifts and additions, division as a sequence of shifts and subtractions, and general functions are calculated as sums of expansion terms \cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard}.
Consequently, uncertainty without explicit bounds is generally assumed to follow a Gaussian distribution \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}\cite{Probability_Statistics}.

Formula \eqref{eqn: bound moment} reduces to Formula \eqref{eqn: moment factor} and \eqref{eqn: moment factor series}:
\begin{align}
\label{eqn: moment factor} 
\zeta(2n, \kappa) =&\; (2n - 1)!! \left( \xi(\frac{\kappa}{\sqrt{2}}) - 2 N(\kappa) \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!} \right); \\
\label{eqn: moment factor series} 
	=&\; 2 N(\kappa) \kappa^{2n} \sum_{j=1}^{\infty} \kappa^{2j-1} \frac{(2n - 1)!!}{(2n-1 + 2j)!!} \\
\label{eqn: moment factor lower} 
 =&\; (2n - 1) \zeta(2n - 2, \kappa) - 2 N(\kappa) \kappa^{2n - 1}; \\
\label{eqn: moment factor higher} 
&\kappa^2 \ll 2n:\eqspace \zeta(2n, \kappa) \simeq 2 N(\kappa) \frac{\kappa^{2n+1}}{2n+1};
\end{align}
\begin{itemize}
\item For small $2n$, $\zeta(2n)$ can be approximated by $\zeta(2n) = (2n-1)!!$ according to Formula \eqref{eqn: moment factor lower}.  
When $\kappa =5$, and $n < 5$, the relative error $\left| \zeta(2n) / (2n - 1)!! - 1 \right|$ is less than $10^{-3}$.

\item For large $2n$, Formula \eqref{eqn: moment factor series} reduces to Formula \eqref{eqn: moment factor higher}, showing that $\zeta(2n)$ increases more slowly than $\kappa^{2n}$ as $2n$ grows.

\end{itemize}



\subsubsection{An Input Uncertainty with Limited Range}

\iffalse

\begin{align*}
\int_{0}^{+\infty}& \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}}{\lambda}} d \tilde{x} = \int_{0}^{+\infty} \tilde{z} e^{-\tilde{z}} d \tilde{z} 
	= - \tilde{z} e^{-\tilde{z}} \Big |_{0}^{+\infty} + \int_{0}^{+\infty} e^{-\tilde{z}} d \tilde{z} = - e^{-\tilde{z}} \Big |_{0}^{+\infty} = 1; \\
\int_{0}^{+\infty}& \tilde{x}^n \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}}{\lambda}} d \tilde{x} 
	= \lambda^n \int_{0}^{+\infty} \tilde{z}^{n+1} e^{-\tilde{z}} d \tilde{z} 
	= \lambda^n \left( (n+1)  \int_{0}^{+\infty} \tilde{z}^{n} e^{-\tilde{z}} d \tilde{z} -  \tilde{z}^{n+1} e^{-\tilde{z}} \Big |_{0}^{+\infty} \right) \\
	&= \lambda^n (n + 1)!; \\
 	&\overline{x}= 2 \lambda; \eqspace (\delta x)^2 = 6 \lambda^2 - 4 \lambda^2 = 2 \lambda^2;  
 		\eqspace \Longrightarrow \tilde{z} = \frac{\tilde{x} - 2 \lambda}{\sqrt{2} \lambda}; \\
\rho(\tilde{z}) &= \sqrt{2} (\sqrt{2} \tilde{z} + 2) e^{-(\sqrt{2} \tilde{z} + 2)}, \tilde{z} \in [-\sqrt{2}, +\infty); \\
\zeta(n) &= \int_{-\sqrt{2}}^{+\infty} \tilde{z}^n \sqrt{2} (\sqrt{2} \tilde{z} + 2) e^{-(\sqrt{2} \tilde{z} + 2)} d \tilde{z}
	= \frac{1}{\sqrt{2^n}} \int_{0}^{+\infty} (\tilde{z} - 2)^n \tilde{z} e^{-\tilde{z}} d \tilde{z}; \\
\zeta(0) &= 1; \\
\zeta(1) &= \frac{1}{\sqrt{2}} \int_{0}^{+\infty} \tilde{z}^2 e^{-\tilde{z}} d \tilde{z} - \sqrt{2} \int_{0}^{+\infty} \tilde{z} e^{-\tilde{z}} d \tilde{z} = \sqrt{2} - \sqrt{2} = 0; \\
\zeta(2) &= \frac{1}{2} \int_{0}^{+\infty} \tilde{z}^3 e^{-\tilde{z}} d \tilde{z} - 2 \int_{0}^{+\infty} \tilde{z}^2 e^{-\tilde{z}} d \tilde{z} 
				+ 2 \frac{1}{2} \int_{0}^{+\infty} \tilde{z} e^{-\tilde{z}} d \tilde{z} = 3 - 4  + 2 = 1; \\
\int_{\varrho}^{\kappa} \tilde{z}^{n + 1} e^{-\tilde{z}} d \tilde{z}
	&= (n+1) \int_{\varrho}^{\kappa} \tilde{z}^{n} e^{-\tilde{z}} d \tilde{z} - \tilde{z}^{n + 1} e^{-\tilde{z}} \Big |_{\varrho}^{\kappa}  \\
	&= (n + 1) \int_{\varrho}^{\kappa} \tilde{z}^{n} e^{-\tilde{z}} d \tilde{z} + e^{-\varrho} \varrho^{n + 1} - e^{-\kappa} \kappa^{n + 1} \\
	&= (n + 1) n \int_{\varrho}^{\kappa} \tilde{z}^{n - 1} e^{-\tilde{z}} d \tilde{z} + (n+1) e^{-\varrho} \varrho^{n} - (n+1) e^{-\kappa} \kappa^{n} + e^{-\varrho} \varrho^{n + 1} - e^{-\kappa} \kappa^{n + 1} \\
	&= (n + 1)! \left( \frac{\int_{\varrho}^{\kappa} \tilde{z}^{n - 1} e^{-\tilde{z}} d \tilde{z}}{(n - 1)!} + \frac{e^{-\varrho} \varrho^{n} - e^{-\kappa} \kappa^{n}}{n!} 
		 + \frac{e^{-\varrho} \varrho^{n + 1} - e^{-\kappa} \kappa^{n + 1}}{(n + 1)!} \right) \\
	&= (n + 1)! \left( e^{-\varrho} \sum_{j=0}^{n+1} \frac{\varrho^j}{j!} - e^{-\kappa} \sum_{j=0}^{n+1} \frac{\kappa^j}{j!} \right) \\
	&= (n + 1)! \left( e^{-\kappa} \sum_{j=n+2}^{+\infty} \frac{\kappa^j}{j!} - e^{-\varrho} \sum_{j=n+2}^{+\infty} \frac{\varrho^j}{j!} \right) \\
	&=  e^{-\kappa} \kappa^{n + 1} \sum_{j=1}^{+\infty} \frac{(n+1)!}{(n + 1 +j)!} \kappa^j
		- e^{-\varrho} \varrho^{n + 1} \sum_{j=1}^{+\infty} \frac{(n+1)!}{(n + 1 +j)!} \varrho^j; \\
\int_{\varrho}^{\kappa} e^{-\tilde{z}} d \tilde{z} &= e^{-\varrho} - e^{-\kappa}; \\
\int_{\varrho}^{\kappa} \tilde{z} e^{-\tilde{z}} d \tilde{z} &= e^{-\varrho} (1 + \varrho) - e^{-\kappa} (1 + \kappa); \\
\int_{\varrho}^{\kappa} \tilde{z}^2 e^{-\tilde{z}} d \tilde{z} 
	&= 2 e^{-\varrho}(1 + \varrho + \frac{\varrho^2}{2!}) - 2 e^{-\kappa}(1 + \kappa + \frac{\kappa^2}{2!}); \\
\int_{\varrho}^{\kappa} \tilde{z}^3 e^{-\tilde{z}} d \tilde{z} 
	&= 6 e^{-\varrho}(1 + \varrho + \frac{\varrho^2}{2!} + \frac{\varrho^3}{3!}) - 6 e^{-\kappa}(1 + \kappa + \frac{\kappa^2}{2!} + \frac{\kappa^3}{3!}); \\
\zeta(n) &= \frac{1}{\sqrt{2^n}} \int_{\varrho}^{\kappa} (\tilde{z} - 2)^n \tilde{z} e^{-\tilde{z}} d \tilde{z}
 	= \frac{1}{\sqrt{2^n}} \sum_{i = 0}^{n} (-1)^i \frac{n!}{i!\; (n - i)!} \int_{\varrho}^{\kappa} \tilde{z}^{i + 1} e^{-\tilde{z}} d \tilde{z}; \\
\zeta(1) &= \sqrt{2} e^{-\varrho}(1 + \varrho + \frac{\varrho^2}{2!}) - \sqrt{2} e^{-\kappa}(1 + \kappa + \frac{\kappa^2}{2!})
		- \sqrt{2} e^{-\varrho} (1 + \varrho) + \sqrt{2} e^{-\kappa} (1 + \kappa);
\end{align*}

\fi

\begin{align}
\label{eqn: fat-tail distribution}
\rho(\tilde{x}, \mu, \sigma) &= \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}}{\lambda}};
		\eqspace \mu = 2 \lambda; \eqspace \sigma = \sqrt{2} \lambda; \\
\label{eqn: fat-tail mean-preserving}
e^{-\varrho} \varrho^2 &= e^{-\kappa} \kappa^2; \\
\label{eqn: fat-tail moment limit}
\lim_{n \rightarrow +\infty} \zeta(n, \kappa) &= \frac{\frac{\kappa}{\sqrt{2}}^n}{n + 2} e^{-\kappa}; 
\end{align}

Formula \eqref{eqn: fat-tail distribution} shows a probability density function with $\tilde{x} \in [0, +\infty)$, whose bounding range $[\varrho \lambda, \kappa \lambda]$ satisfying $0 < \varrho < 2 < \kappa$.
Formula \eqref{eqn: fat-tail mean-preserving} gives its mean preserving equation.
And Formula \eqref{eqn: fat-tail moment limit} describes its asymptotic behavior.

\iffalse

\begin{align*}
\int_{0}^{+\infty}& \tilde{x}^{2n} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x} 
		= (2 \lambda^2)^n \int_{0}^{+\infty} \tilde{z}^{n} e^{- \tilde{z}} d \tilde{z}
		= (2 \lambda^2)^n \left( n\; \int_{0}^{+\infty} \tilde{z}^{n - 1} e^{- \tilde{z}} d \tilde{z} - \tilde{z}^{n} e^{- \tilde{z}} \Big |_{0}^{+\infty}  \right) \\
	&= (2 \lambda^2)^n\; n!  \int_{0}^{+\infty} \e^{- \tilde{z}} d \tilde{z} 
		= (2 \lambda^2)^n\; n! ;; \\
\int_{0}^{+\infty}& \tilde{x}^{2n + 1} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x}
		= \lambda^{2n + 1} \int_{0}^{+\infty} \tilde{z}^{2n + 2} e^{-\frac{\tilde{z}^2}{2}} d \tilde{z} \\
	&= \lambda^{2n + 1} \frac{1}{2} \left( (2n + 1) \int_{0}^{+\infty} \tilde{z}^{2n} e^{-\frac{\tilde{z}^2}{2}} d \tilde{z} 
			- \tilde{z}^{2n + 2} e^{-\frac{\tilde{z}^2}{2}} \Big |_{0}^{+\infty} \right) = \lambda^{2n + 1} \sqrt{\frac{\pi}{2}} (2n + 2)!!; \\
\overline{x} &=  \sqrt{\frac{\pi}{2}} \lambda; \\
\delta^2 x &= (2 - \frac{\pi}{2}) \lambda^2; \\
\int_{\varrho \lambda}^{\kappa \lambda}& \tilde{x}^{2n} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x} 
		= (2 \lambda^2)^n \int_{\frac{\varrho^2}{2}}^{\frac{\kappa^2}{2}} \tilde{z}^{n} e^{- \tilde{z}} d \tilde{z} 
		= \lambda^{2n} 2^n \left( n \int_{\frac{\varrho^2}{2}}^{\frac{\kappa^2}{2}} \tilde{z}^{n - 1} e^{- \tilde{z}} d \tilde{z} 
			- \tilde{z}^{n} e^{- \tilde{z}} \Big |_{\frac{\varrho^2}{2}}^{\frac{\kappa^2}{2}} \right) \\
 		&= \lambda^{2n} 2^n n! \left( \int_{\frac{\varrho^2}{2}}^{\frac{\kappa^2}{2}} e^{- \tilde{z}} d \tilde{z} 
	   		- e^{- \tilde{z}} \sum_{j = 1}^{n} \frac{\tilde{z}^{j}}{j!} \Big |_{\frac{\varrho^2}{2}}^{\frac{\kappa^2}{2}} \right)
	   	=  \lambda^{2n} 2^n n!\; e^{- \tilde{z}} \sum_{j = 0}^{n} \frac{\tilde{z}^{j}}{j!} \Big |_{\frac{\kappa^2}{2}}^{\frac{\varrho^2}{2}} \\
	   	&= \lambda^{2n} 2^n n! \left( e^{-\frac{\varrho^2}{2}} \sum_{j=0}^{n} \frac{(\frac{\varrho^2}{2})^j}{j!} 
	   		- e^{-\frac{\kappa^2}{2}} \sum_{j=0}^{n} \frac{(\frac{\kappa^2}{2})^j}{j!} \right); \\
&\lim_{n \rightarrow +\infty} \int_{\varrho \lambda}^{\kappa \lambda} \tilde{x}^{2n} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x} 
		= \lambda^{2n} \frac{1}{2} \kappa^{2n + 1} e^{-\frac{\kappa^2}{2}} \sum_{j=0}^{n} \frac{n!}{(n + 1 + j)!} (\frac{\kappa^2}{2})^j \\
\int_{\varrho \lambda}^{\kappa \lambda}& \tilde{x}^{2n - 1} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x}
		= \lambda^{2n-1} \int_{\varrho}^{\kappa} \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}; \\
&\lim_{n \rightarrow +\infty} \int_{\varrho \lambda}^{\kappa \lambda} \tilde{x}^{2n - 1} \frac{\tilde{x}}{\lambda^2} e^{-\frac{\tilde{x}^2}{2\lambda^2}} d \tilde{x}
		= \lambda^{2n - 1} N(\kappa) \frac{\kappa^{2n + 1}}{2n + 1};
\end{align*}

\fi


\subsection{One-Dimensional Examples}

Formula \eqref{eqn: exp mean} and \eqref{eqn: exp precision} give the mean and variance for $e^x$, respectively:
\begin{align}
\label{eqn: exp Taylor}
& e^{x + \tilde{x}} = e^x \sum_{n=0}^{\infty} \frac{\tilde{x}^n}{n!}; \\
\label{eqn: exp mean}
\frac{\overline{e^x}}{e^x}  &= \sum_{n=0}^{\infty} \sigma^n \zeta(n) \frac{1}{n!}; \\
\label{eqn: exp precision}
\frac{\delta^2 e^x}{(e^x)^2} &= \sum_{n=2}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{\zeta(n) - \zeta(j) \zeta(n - j)}{j!\;(n - j)!};
\end{align}

Formula \eqref{eqn: log mean} and \eqref{eqn: log precision} give the mean and variance for $\log(x)$, respectively:
\begin{align}
\label{eqn: log Taylor}
& \log(x + \tilde{x}) - \log(x) = \log(1 + \frac{\tilde{x}}{x}) = \sum_{j=1}^{\infty} \frac{(-1)^{j+1}}{j} \frac{\tilde{x}^j}{x^j}; \\
\label{eqn: log mean}
\overline{\log(x)}  &= \log(x) + \sum_{n=1}^{+\infty} P(x)^{n} \frac{(-1)^{n+1} \zeta(n)}{n}; \\
\label{eqn: log precision}
\delta^2 \log(x) &= \sum_{n=2}^{+\infty} P(x)^{n} \sum_{j=1}^{n-1} \frac{\zeta(n) - \zeta(j) \zeta(n - j)}{j (n-j)};
\end{align}

Formula \eqref{eqn: sin mean} and \eqref{eqn: sin precision} give the mean and variance for $\sin(x)$, respectively:
\begin{align}
\label{eqn: sin Taylor}
\sin(x + \tilde{x}) &= \sum_{n=0}^{\infty} \eta(n, x) \frac{\tilde{x}^{n}}{n!};
	\eqspace \eta(n, x) \equiv \begin{cases} 
		n = 4j: \eqspace  sin(x); \\ n = 4j + 1: \eqspace  cos(x); \\ n = 4j + 2: \eqspace  -sin(x); \\ n = 4j +3: \eqspace  -cos(x); 
	\end{cases} \\
\label{eqn: sin mean}
\overline{\sin(x)} =& \sum_{n=0}^{\infty} \sigma^n \eta(n, x) \frac{\zeta(n)}{n!}; \\
\label{eqn: sin precision}
\delta^2 \sin(x) =& \sum_{n=2}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{\eta(j, x)\eta(n - j, x)}{j! (n-j)!}
      	\big(\zeta(n) - \zeta(j) \zeta(n - j)\big); 
\end{align}

Formula \eqref{eqn: power mean} and \eqref{eqn: power precision} give the mean and variance for $x^c$, respectively:
\begin{align}
\label{eqn: power Taylor}
&(x + \tilde{x})^c = x^c (1 + \frac{\tilde{x}}{x})^c = x^c + x^c \sum_{n=1}^{\infty} \frac{\tilde{x}^n}{x^n} \begin{pmatrix} c \\ n \end{pmatrix};
	\eqspace \begin{pmatrix} c \\ n \end{pmatrix} \equiv \frac{\prod_{j=0}^{n-1} (c -j)}{n!}; \\
\label{eqn: power mean}
\frac{\overline{x^c}}{x^c}  &= 1 + 1 \sum_{n=1}^{\infty} P(x)^{n} \zeta(n) \begin{pmatrix} c \\ n \end{pmatrix}; \\
\label{eqn: power precision}
\frac{\delta^2 x^c}{(x^c)^2} &= \sum_{n=2}^{\infty} P(x)^{n} \sum_{j=1}^{n-1}
  \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ n - j \end{pmatrix} \big( \zeta(n) - \zeta(j) \zeta(n - j) \big);
\end{align}


\iffalse

\begin{align*}
\frac{x + \tilde{x}}{y + \tilde{y}} &\simeq \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y}
  		 - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2
		 + \frac{1}{y^3} \tilde{x} \tilde{y}^2 - \frac{x}{y^4} \tilde{y}^3; \\
(x + \tilde{x}) \frac{1}{y + \tilde{y}} &\simeq (x + \tilde{x}) \left( \frac{1}{y} - \frac{1}{y^2} \tilde{y} + \frac{1}{y^3} \tilde{y}^2 - \frac{1}{y^4} \tilde{y}^3 \right) \\
	&= \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y} - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2  + \frac{1}{y^3} \tilde{x} \tilde{y}^2
		 -  \frac{x}{y^4} \tilde{y}^3 - \frac{1}{y^4} \tilde{x} \tilde{y}^3;
\end{align*}

\fi

The result variance in statistical Taylor expansion reflects the inherent characteristics of the calculation, such as $\sigma \rightarrow P(e^x)$, $P(x) \rightarrow \delta \log(x)$, $\sigma \rightarrow \delta \sin(x)$, and $P(x) \rightarrow P(x^c)$.




\subsection{Low-Order Approximation}

\iffalse

\begin{align*}
e^{x + \tilde{x}} &= e^x \sum_{n=0}^{\infty} \frac{\tilde{x}^n}{n!}
 \simeq e^x \left(1 + \tilde{x} + \frac{1}{2} \tilde{x}^2 + \frac{1}{6} \tilde{x}^3 + \frac{1}{24} \tilde{x}^4 + \frac{1}{120} \tilde{x}^5 + \frac{1}{720} \tilde{x}^6 \right); \\
\overline{e^x} &= e^x \sum_{n=0}^{\infty} (\delta x)^{2n} \zeta(2n) \frac{1}{(2n)!} 
 \simeq e^x \left(1 + \frac{\eta(2)}{2!} (\delta x)^2 + \frac{\eta(4)}{4!} (\delta x)^4 + \frac{\eta(6)}{6!} (\delta x)^6 + \frac{\eta(8)}{8!} (\delta x)^8 \right); \\
P(e^x)^2 &\simeq \frac{\delta^2 e^x}{(e^x)^2} =  \sum_{n=1}^{\infty} (\delta x)^{2n} \left( \zeta(2n) \sum_{j=1}^{2n-1} \frac{1}{j!\;(2n - j)!} 
   	- \sum_{j=1}^{n-1} \frac{\zeta(2j)}{(2j)!}  \frac{\zeta(2n - 2j)}{(2n - 2j)!} \right); \\
 &= (\delta x)^2 + \left( \eta(4) (\frac{1}{4} + \frac{1}{3}) - \eta(2)^2 \frac{1}{4} \right) (\delta x)^4 + 
 		\left( \eta(6) (\frac{1}{36} + \frac{1}{24} + \frac{1}{60}) - \eta(2) \eta(4) \frac{1}{24} \right) (\delta x)^6 + o((\delta x)^6) \\
 &\simeq (\delta x)^2 + \frac{3}{2} (\delta x)^4 + \frac{7}{6} (\delta x)^6 + \frac{5}{8} (\delta x)^8
\end{align*}

\fi

\iffalse

\begin{align*}
& \log(x + \tilde{x}) - \log(x) = \log(1 + \frac{\tilde{x}}{x}) = \sum_{j=1}^{\infty} \frac{(-1)^{j+1}}{j} \frac{\tilde{x}^j}{x^j}; \\
\overline{\log(x)}  &= \log(x) -\sum_{n=1}^{+\infty} P(x)^{2n} \frac{\zeta(2n)}{2n}; \\
&\sum_{j=1}^{2n-1} \frac{1}{j} \frac{1}{2n - j} = \frac{1}{2n} \sum_{j=1}^{2n-1} \frac{1}{j} +  \frac{1}{2n - j} = \frac{2}{2n} \sum_{j=1}^{2n-1} \frac{1}{j}
 = \frac{1}{n} \sum_{j=1}^{2n-1} \frac{1}{j}; \\
\delta^2 \log(x) &= \sum_{n=1}^{+\infty} P(x)^{2n} \left(\sum_{j=1}^{2n-1} \frac{\zeta(2n)}{j (2n-j)}
   - \sum_{j=1}^{n-1} \frac{\zeta(2j)}{2j} \frac{\zeta(2n - 2j)}{2n - 2j} \right) \\
 &= \sum_{n=1}^{+\infty} P(x)^{2n} \left(\frac{\zeta(2n)}{n} \sum_{j=1}^{2n-1} \frac{1}{j} 
     - \sum_{j=1}^{n-1} \frac{\zeta(2j)}{(2j)} \frac{\zeta(2n - 2j)}{(2n - 2j)} \right) \\
 &\simeq P(x)^2 + P(x)^4 (\frac{11}{8} - \frac{1}{4}) + P(x)^6 (\frac{137}{24} - \frac{3}{4}) + P(x)^8 (\frac{1089}{32} - \frac{49}{16}) \\
 &\simeq P(x)^2 + P(x)^4 \frac{9}{8}  + P(x)^6 \frac{119}{24} + P(x)^8 \frac{991}{32};
\end{align*}

\fi

\iffalse

\begin{align*}
\sin(x + \tilde{x}) &= \sin(x) + \sum_{n=0}^{\infty} \cos(x) (-1)^n \frac{\tilde{x}^{2n+1}}{(2n + 1)!} + \sum_{n=1}^{\infty} \sin(x) (-1)^{n} \frac{\tilde{x}^{2n}}{(2n)!} \\
& \simeq \frac{1}{1!} \cos(x) \tilde{x} - \frac{1}{2!} \sin(x) \tilde{x}^2 - \frac{1}{3!} \cos(x) \tilde{x}^3 + \frac{1}{4!} \sin(x) \tilde{x}^4 + \\
 &\eqspace  \frac{1}{5!} \cos(x) \tilde{x}^5 - \frac{1}{6!} \sin(x) \tilde{x}^6 - \frac{1}{7!} \cos(x) \tilde{x}^7 + \frac{1}{8!} \sin(x) \tilde{x}^8; \\
\overline{\sin(x)} &= \sum_{n=0}^{\infty} \sin(x) (-1)^{n} \frac{\zeta(2n)}{(2n)!} (\delta x)^{2n} \\
 &\simeq  \sin(x) \left(1 - \frac{1!!}{2!} (\delta x)^2 + \frac{3!!}{4!} (\delta x)^4 - \frac{5!!}{6!} (\delta x)^6 + \frac{7!!}{8!} (\delta x)^8 \right); \\
\delta^2 \sin(x) &= \sum_{n=1}^{\infty} (\delta x)^{2n} (-1)^{n - 1}
	\left( \cos(x)^2 \sum_{j=0}^{n-1} \frac{\zeta(2n)}{(2j+1)!(2n-2j-1)!} - \sin(x)^2 \sum_{j=1}^{n-1} \frac{\zeta(2n) - \zeta(2j) \zeta(2n-2j)}{(2j)!(2n-2j)!} \right) \\ 
 &\simeq (\delta x)^2 \cos(x)^2 - (\delta x)^4 \left( \cos(x)^2 - \sin(x)^2 (\frac{3}{4} - \frac{1}{4}) \right) + \\
 &\eqspace (\delta x)^6 \left(\cos(x)^2 \frac{2}{3} - \sin(x)^2 (\frac{5}{8} - \frac{1}{8}) \right) -
    (\delta x)^8 \left(\cos(x)^2 \frac{1}{3} - \sin(x)^2 (\frac{21}{64} - \frac{7}{192}) \right)  \\
 &=  (\delta x)^2 \cos(x)^2 - (\delta x)^4 \left( \cos(x)^2 - \sin(x)^2 \frac{1}{2} \right) + \\
 &\eqspace (\delta x)^6 \left(\cos(x)^2 \frac{2}{3} - \sin(x)^2 \frac{1}{2} \right) - (\delta x)^8 \left(\cos(x)^2 \frac{1}{3} - \sin(x)^2 \frac{7}{24} \right); \\
 &=  (\delta x)^2 \cos(x)^2 - (\delta x)^4 (\cos(x)^2 \frac{3}{2} - \frac{1}{2}) + (\delta x)^6 (\cos(x)^2 \frac{7}{6} - \frac{1}{2})  - (\delta x)^8 (\cos(x)^2 \frac{5}{8} - \frac{7}{24});
\end{align*}

\fi

\iffalse

\begin{align*}
&(x + \tilde{x})^c = x^c (1 + \frac{\tilde{x}}{x})^c = x^c \sum_{n=1}^{\infty} (\frac{\tilde{x}}{x})^n \begin{pmatrix} c \\ n \end{pmatrix} \\
\frac{(x + \tilde{x})^c}{x^c} &\simeq c (\frac{\tilde{x}}{x}) + \frac{c(c-1)}{2!} (\frac{\tilde{x}}{x})^2
 + \frac{c(c-1)(c-2)}{3!} (\frac{\tilde{x}}{x})^3 + \frac{c(c-1)(c-2)(c-3)}{4!} (\frac{\tilde{x}}{x})^4 + \\
 &\eqspace \frac{c(c-1)(c-2)(c-3)(c-4)}{5!} (\frac{\tilde{x}}{x})^5 + \frac{c(c-1)(c-2)(c-3)(c-4)(c-5)}{6!} (\frac{\tilde{x}}{x})^6; \\
P(x^c)^2 &= \sum_{n=1}^{\infty} P(x)^{2n} \left( \sum_{j=1}^{2n-1} \zeta(2n) \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 - \sum_{j=1}^{n-1} \zeta(2j) \begin{pmatrix} c \\ 2j \end{pmatrix} \zeta(2n - 2j) \begin{pmatrix} c \\ 2n -2 j \end{pmatrix}  \right)\\
 &\simeq c^2 P(x)^2 + \frac{3}{2} c^2 (c-1) (c - \frac{5}{3}) P(x)^4 + \frac{7}{6} c^2 (c-1) (c-2)^2 (c - \frac{16}{7}) P(x)^6;
\end{align*}

\begin{align*}
(x + \tilde{x})^2 &= x^2 + 2 x \tilde{x} + \tilde{x}^2; \\
\overline{x^2} &= x^2 + (\delta x)^2; \\
(x + \tilde{x})^4 &= x^4 + 6 x^2 \tilde{x}^2 + \tilde{x}^4; \\
\delta^2 x^2 &= 4 x^2 (\delta x)^2 + 2 (\delta x)^4;
\end{align*}

\begin{align*}
& (x + \tilde{x})^3 = x^3 + 3 x^2 \tilde{x} + 3 x \tilde{x}^2 + \tilde{x}^3 \\
\overline{x^3} &= x^3 + 3 x (\delta x)^2; \\
\delta^2 x^3 &= 9 x^4  (\delta x)^2 +  (15 x^2 3 - 9 x^2) (\delta x)^4 + 15 (\delta x)^6;
\end{align*}

\begin{align*}
\frac{1}{x + \tilde{x}} &= \frac{1}{x} \left(1 - \frac{\tilde{x}}{x} + (\frac{\tilde{x}}{x})^2 - (\frac{\tilde{x}}{x})^3 + (\frac{\tilde{x}}{x})^4 
		 - (\frac{\tilde{x}}{x})^5 + (\frac{\tilde{x}}{x})^6 + \dots \right); \\
\frac{\overline{1/x}}{1/x} &= 1 + P(x)^2 + 3 P(x)^4 + 15 P(x)^6 + o(P(x)^8); \\
\frac{\delta^2 1/x}{(1/x)^2} &= P(x)^2 + 3 P(x)^4 3 - P(x)^4  + 5 P(x)^4 15 - 2 P(x)^6 3 + o(P(x)^8) \\
	&= P(x)^2 + 8 P(x)^4 + P(x)^4 + 69 P(x)^6 + o(P(x)^8) ;
\end{align*}

\begin{align*}
\frac{1/(x + \tilde{x})^2}{1/x^2} &= 1 - 2 (\frac{\tilde{x}}{x}) + 3 (\frac{\tilde{x}}{x})^2 -4  (\frac{\tilde{x}}{x})^3 
		+5 (\frac{\tilde{x}}{x})^4 - 6 (\frac{\tilde{x}}{x})^5 + 7 (\frac{\tilde{x}}{x})^7; \\
\frac{\overline{1/x^2}}{1/x^2} &= 3 P(x)^2 + 15 P(x)^4 + 105 P(x)^6 + o(P(x)^8); \\
\frac{\delta^2 1/x^2}{(1/x^2)^2} &= 4 P(x)^2 + (9 + 16) P(x)^4 3 - 9 P(x)^4 + (16 + 30+ 24) P(x)^6 15 - 90 P(x)^6 + o(P(x)^8)  \\
	&= 4 P(x)^2 + 66 P(x)^4 + 960 P(x)^6 + o(P(x)^8);
\end{align*}

\begin{align*}
\frac{\sqrt{x + \tilde{x}}}{\sqrt{x}} &= 1 + \frac{1}{2} (\frac{\tilde{x}}{x}) - \frac{1}{8} (\frac{\tilde{x}}{x})^2 + \frac{1}{16} (\frac{\tilde{x}}{x})^3 
		- \frac{5}{128} (\frac{\tilde{x}}{x})^4 + \frac{7}{256} (\frac{\tilde{x}}{x})^5 - \frac{21}{1024} (\frac{\tilde{x}}{x})^6; \\
\frac{\overline{\sqrt{x}}}{\sqrt{x}} &= 1 - \frac{1}{8} P(x)^2 - \frac{15}{128} P(x)^4 - \frac{315}{1024} P(x)^6; \\
\frac{\delta^2 \sqrt{x}}{(\sqrt{x})^2} &= \frac{1}{4} P(x)^2 + (\frac{1}{64} + \frac{1}{16}) P(x)^4 3 - \frac{1}{64} P(x)^4  
		+ (\frac{1}{256} + \frac{5}{512} + \frac{7}{256}) P(x)^4 15 - \frac{5}{512} P(x)^6 + o(P(x)^8) \\
	&= \frac{1}{4} P(x)^2 + \frac{7}{32} P(x)^4 + \frac{75}{128} P(x)^6 + o(P(x)^8) ;
\end{align*}

\fi


When $n < 5 \leq \kappa$, $\eta(n) \simeq n!!$. 
Under these conditions, Formula \eqref{eqn: exp precision}, \eqref{eqn: log precision}, \eqref{eqn: sin precision}, and \eqref{eqn: power precision} can be simplified as Formula \eqref{eqn: exp precision approx}, \eqref{eqn: log precision approx}, \eqref{eqn: sin precision approx}, and \eqref{eqn: power precision approx}, respectively.
\begin{align}
\label{eqn: exp precision approx}
\frac{\delta^2 e^x}{(e^x)^2} \simeq&\; \sigma^2 + \frac{3}{2} \sigma^4 + \frac{7}{6} \sigma^6 + \frac{5}{8} \sigma^8 + o((\delta x)^{10}); \\
\label{eqn: log precision approx}
\delta^2 \log(x) \simeq&\; P(x)^2 + P(x)^4 \frac{9}{8}  + P(x)^6 \frac{119}{24} + P(x)^8 \frac{991}{32} + o(P(x)^{10}); \\
\label{eqn: sin precision approx}
\delta^2 \sin(x) \simeq&\;  \sigma^2 \cos(x)^2 - (\delta x)^4 (\cos(x)^2 \frac{3}{2} - \frac{1}{2}) \nonumber \\
 	& + \sigma^6 (\cos(x)^2 \frac{7}{6} - \frac{1}{2}) - \sigma^8 (\cos(x)^2 \frac{5}{8} - \frac{7}{24}) + o((\delta x)^{10}); \\
\label{eqn: power precision approx}
\frac{\delta^2 x^c}{(x^c)^2} \simeq&\; c^2 P(x)^2 + \frac{3}{2} c^2 (c-1) (c - \frac{5}{3}) P(x)^4 \nonumber \\
	& + \frac{7}{6} c^2 (c-1) (c-2)^2 (c - \frac{16}{7}) P(x)^6 + o(P(x)^{8});
\end{align}
Formula \eqref{eqn: square precision}, \eqref{eqn: square root precision}, and \eqref{eqn: inversion precision} are special cases of Formula \eqref{eqn: power precision approx}.
\begin{align}
\label{eqn: square precision}
\delta^2 x^2 \simeq&\; 4 x^2 (\delta x)^2 + 2  (\delta x)^4; \\
\label{eqn: square root precision}
\frac{\delta^2 \sqrt{x}}{(\sqrt{x})^2} \simeq&\; \frac{1}{4} P(x)^2 + \frac{7}{32} P(x)^4 + \frac{75}{128} P(x)^6 + o(P(x)^8); \\
\label{eqn: inversion precision}
\frac{\delta^2 1/x}{(1/x)^2} \simeq&\; P(x)^2 + 8 P(x)^4 + 69 P(x)^6 + o(P(x)^8);
\end{align}




\subsection{Convergence}

\iffalse

\begin{align*}
\frac{\delta^2 x^c}{(x^c)^2} &\simeq \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \\
	&\lim_{2n \rightarrow +\infty}  \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	n c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} = \prod_{j=0}^{2n-1} \frac{c -j }{1 + j}; \\
 			c > 0:	n \begin{pmatrix} c \\ n \end{pmatrix}^2 = n \left( \prod_{j=0}^{n-1} \frac{c -j }{1 + j} \right)^2 < n;
 		\end{cases}\\
&\begin{pmatrix} c \\ j + 1 \end{pmatrix} \begin{pmatrix} c \\ 2n - j - 1 \end{pmatrix} 
		= \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} \frac{c - j}{j + 1} \frac{2n - j}{c - 2n + j + 1}; \\
&\sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j + 2 \end{pmatrix} = 
		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix}  \begin{pmatrix} c \\ 2 n - j \end{pmatrix}
		\frac{c - (2 n - j)}{2 n - j + 1} \frac{c - (2 n - j + 1)}{2 n - j + 2} \\
		&\largespace + \left(\frac{c^2}{2} + \frac{c - 2n}{2n + 1} c \right) \begin{pmatrix} c \\ 2n \end{pmatrix}; \\
&\lim_{2n \rightarrow +\infty} \sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n + 2 - j \end{pmatrix}  =
	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 
		+ \prod_{j=0}^{2 n - 1} \frac{c - j}{j + 1} \frac{1}{2} ((c - 1)^2 - 1)
\end{align*}

\begin{align}
\label{eqn: pow convergence}
\frac{\delta^2 x^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \nonumber \\
	&\frac{1}{2n} \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	\left| c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} \right| < \frac{\left| c \Gamma(c) \right|}{(2n - 1) !}; \\
 			c > 0:	\begin{pmatrix} c \\ n \end{pmatrix}^2 < 1;
 		\end{cases}
\end{align}


\fi

Formula \eqref{eqn: exp precision} for $e^{x \pm \delta x}$ and Formula \eqref{eqn: sin precision} for $\sin(x \pm \delta x)$ both converge unconditionally.
However, as shown later in this pap, $\delta^2 \sin(x \pm \delta x)$ can become negative for large $\delta x$, which imposes an upper-bound constraint on the input $\delta x$.

Formula \eqref{eqn: log precision} for $\log(x \pm \delta x)$ can be approximated by Formula \eqref{eqn: log convergence} as $n \rightarrow \infty$,
which converges when $P(x) < 1/\kappa$.
\begin{align}
\label{eqn: log convergence}
\delta^2 \log(x \pm \delta x) \simeq& \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n) \sum_{j=1}^{2n-1} \frac{1}{j} \frac{1}{2n-j} 
		= \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n) \frac{1}{n} \sum_{j=1}^{2n-1} \frac{1}{j}  \nonumber \\
	\simeq&\; 2\nu(\kappa) \log(2) \sum_{n = 1}^{+\infty} \frac{(P(x) \kappa)^{2n}}{(2n)^2},
		\begin{cases}		
 			\text{Gaussian}:	\nu(\kappa) = N(\kappa) \kappa\\
 			\text{Uniform}:	\nu(\kappa) = 1,\eqspace \kappa = \sqrt{3}
		\end{cases};
\end{align}

Formula \eqref{eqn: power precision} for $(x \pm \delta x)^c$ can be approximated by Formula \eqref{eqn: pow convergence} after applying Vandermonde's identity $\sum_{k=0}^{r} \begin{pmatrix} m \\ k \end{pmatrix} \begin{pmatrix} n \\ r - k \end{pmatrix} = \begin{pmatrix} m + n \\ r \end{pmatrix}$ .  
This expression converges when $P(x) \lesssim 1/\kappa$ although the precis upper bound for $P(x)$ varies with $c$.
\begin{align}
\label{eqn: pow convergence}
\frac{\delta^2 (x \pm \delta x)^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
	\simeq \nu(\kappa) \sum_{n = 1}^{+\infty} (P(x) \kappa)^{2n} \frac{\begin{pmatrix} 2c \\ 2n \end{pmatrix}}{2n};
\end{align}


\subsection{Statistical Bounding}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Normal_Bounding_Leakage.pdf} 
\captionof{figure}{
Measured bounding leakage $\epsilon(\kappa_s, N)$ (y-axis) for varying measuring bounding range $\kappa_s$ (x-axis) and sample count $N$ (legend).
}
\label{fig: Normal_Bounding_Leakage}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Bounding_Factor_Leakage.pdf} 
\captionof{figure}{
Measured bounding range $\kappa$ (left y-axis) and corresponding measured bounding leakage $\epsilon(\kappa)$ (right y-axis) for varying sample count $N$ (x-axis) when the underlying distribution is uniform or normal (legend), with different measuring bounding range $\kappa_s$ for the normal distribution.
}
\label{fig: Bounding_Factor_Leakage}
\end{figure}

When sampling from a distribution, the sample mean $\overline{x}$ and sample deviation $\delta x$ approach the distribution mean $\mu$ and distribution deviation $\sigma$ respectively as the sample count $N$ increases \cite{Probability_Statistics}.
This yields the \emph{sample bounding leakage} $\epsilon(\kappa, N)$ for the interval $[\overline{x} - \varrho \delta x, \overline{x} + \kappa \delta x]$, in contrast to the \emph{distributional bounding leakage} $\epsilon(\kappa)$ for the interval $[\mu - \varrho \sigma, \mu + \kappa \sigma]$.
Because $\epsilon(\kappa) \neq \epsilon(\kappa, N)$ for finite $N$, let $\epsilon(\kappa) = \epsilon(\kappa_s, N)$, where $\kappa_s$ is the \emph{measuring bonding range}, and $\kappa(\kappa_s, N)$ is the \emph{measured bounding range}.

When the underlying distribution is uniform, the portion of sampled range $[\overline{x} - \sqrt{3} \delta x, \overline{x} + \sqrt{3} \delta x]$ outside the actual range $[\mu - \sqrt{3} \sigma, \mu + \sqrt{3} \sigma]$ contributes to bounding leakage $\epsilon(N)$. 
Figure \ref{fig: Bounding_Factor_Leakage} shows that $0 < \epsilon(N) \sim N^{-0.564}$ empirically.
The measured bounding range $\kappa(N) = \sqrt{3} (1 - \epsilon(N))$, which should be used as $\kappa$ in Formula \eqref{eqn: bound moment}.

\begin{align}
\label{eqn: Gaussian theoretical bounding leakage} 
\epsilon(\kappa) =&\; 1 - \xi(\frac{\kappa}{\sqrt{2}}); \\
\label{eqn: Gaussian experimental bounding leakage}
\epsilon(\kappa_s, N) = &\; 1 - \frac{1}{2} \xi(\frac{|\kappa_s \delta x - \overline{x}|}{\sqrt{2}}) - \frac{1}{2} \xi(\frac{|\kappa_s \delta x + \overline{x}|}{\sqrt{2}});
\end{align}
When the underlying distribution is Normal, Formula \eqref{eqn: Gaussian theoretical bounding leakage} and \eqref{eqn: Gaussian experimental bounding leakage} give the distributional bounding leakage $\epsilon(\kappa)$ and the sample bounding leakage $\epsilon(\kappa_s, N)$ respectively, where $\xi()$ is the Normal error function \cite{Probability_Statistics}.
Figure \ref{fig: Normal_Bounding_Leakage} shows that $\epsilon(\kappa_s) < \epsilon(\kappa_s, N)$, and $\lim_{N \rightarrow \infty} \epsilon(\kappa_s, N) = \epsilon(\kappa_s)$.
It also shows that $\kappa(\kappa_s, N) < \kappa_s$ and $\lim_{N \rightarrow \infty} \kappa(\kappa_s, N) = \kappa_s$. 
Figure \ref{fig: Bounding_Factor_Leakage} further demonstrates that for smaller $\kappa_s$, $\kappa(\kappa_s, N)$ approaches $\kappa_s$ more rapidly as $N$ increases (e.g., $\kappa(2, 100) \simeq 2$ vs $\kappa(5, 1000) \simeq 5$), but converges to a larger stable bounding leakage (e.g., $\epsilon(2) = 4.55\;10^{-2}$ vs $\epsilon(5) = 5.73\;10^{-7}$).
Figure \ref{fig: Bounding_Factor_Leakage} also indicates that when $N \geq 30$, the difference between $\epsilon(4, N)$ and $\epsilon(5, N)$ is less than $10^{-3}$, suggesting that $\kappa(\kappa_s, N)$ becomes stable when $\kappa_s \geq 4$.
Moreover, according to  the 5-$\sigma$ rule, $\kappa(5, N)$ in Figure \ref{fig: Normal_Bounding_Leakage} should be used as $\kappa$ in Formula \eqref{eqn: bound moment}.

$\kappa_s = \sqrt{3}$ and $\kappa_s = 5$ are defined as the ideal bounding ranges for the uniform and Gaussian distributions respectively.


\subsection{Ideal Statistics}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Normal_Function.pdf} 
\captionof{figure}{
The ratio of resulting variance $\delta^2 f$ to the ideal variance $\widehat{\delta^2} f$ (left y-axis) and the bounding leakage (right y-axis) for varying sample count $N$ (x-axis) for the selected function $f(x=1 \pm 0.1)$ (legend) when the uncertainty distribution is Gaussian.
}
\label{fig: Normal_Function}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Uniform_Function.pdf} 
\captionof{figure}{
The ratio of resulting variance $\delta^2 f$ to the ideal variance $\widehat{\delta^2} f$ (left y-axis) and the bounding leakage (right y-axis) for varying sample count $N$ (x-axis) for the selected function $f(x=1 \pm 0.1)$ (legend) when the uncertainty distribution is Uniform.
}
\label{fig: Uniform_Function}
\end{figure}

\begin{align}
\label{eqn: identity leakage}
\zeta(2, \kappa) =& \frac{\delta^2 x}{(\delta x)^2}; \\
\label{eqn: sum leakage}
\alpha(x \pm y) =& \frac{\zeta_x(2, \kappa_x) (\delta x)^2 + \zeta_y(2, \kappa_y) (\delta y)^2}{(\delta x)^2 + (\delta y)^2};
\end{align}

When sample count $N \rightarrow \infty$, $\zeta(0) \rightarrow 1$, $\zeta(2) \rightarrow 1$ and $\delta^2 x \rightarrow (\delta x)^2 \rightarrow \sigma$ in Formula \eqref{eqn: identity leakage}.
Such relationship of $\delta^2 f$ versus sample count $N$ is general:
\begin{itemize}
\item
When the underlying uncertainty distribution is Gaussian, Figure \ref{fig: Normal_Function} shows that the resulting variance $\delta^2 f$ for a selected functions rise with $N$ until reaching the corresponding stable values $\widehat{\delta^2} f$ when $N \geq 50$.
When $N = 20$ and $\kappa_s = 5$, $\kappa = 3.8$ according to Figure \ref{fig: Normal_Bounding_Leakage}.
Therefore it is sufficient to compute the stable variances $\widehat{\delta^2} f$ with $\zeta(n, 5)$.

\item
When the underlying uncertainty distribution is Uniform, Figure \ref{fig: Uniform_Function} shows the same trend, however the stable variances are reached only when $N > 10^4$.
The stable variance $\widehat{\delta^2} f$ is computed using Formula \eqref{eqn: bound moment UN}.
\end{itemize}

Define the stable variance when the sample count $N$ is sufficient large as the \emph{ideal variance} $\widehat{\delta^2} f$, which assumes that the sample count $N$ is effectively infinite for each input.
Ideal variances are consistent with traditional variances \cite{Probability_Statistics}.

In both Figure \ref{fig: Normal_Function} and \ref{fig: Uniform_Function}, the bonding leakage $\epsilon(N)$ decreases as the sample count $N$ increases.
The stable resulting variances $\delta^2 f$ is reached only when $\epsilon(s, N) < 10^{-3}$.
All functions approach their corresponding ideal variances with exactly the same trend.
These observations suggest that bounding leakage is the reason for $\delta^2 f < \widehat{\delta^2} f$ and it is justified to infer confidential interval range $\kappa$ from bounding leakage $\epsilon(N)$.
Define $\alpha \equiv \delta^2 f/\widehat{\delta^2} f \in [0, 1]$ as the \emph{ideal ratio}, which quantifies the reliability for $\widehat{\delta^2} f$  due to insufficient sample count $N$ for the given underlying distribution.
Define the ideal ratio to be zero when the sample count $N$ is $1$.
When there are multiple inputs each with its own ideal ratio $\alpha=\zeta(2, \kappa)$, Formula \eqref{eqn: sum leakage} demonstrates an example on how to calculate the resulting ideal ratio $\alpha$ for $x \pm y$.

An ideal ratio also applies to the corresponding uncertainty bias.  
The trends of uncertainty biases $\overline{f} - f$ versus sample count $N$ are exactly as those in Figure \ref{fig: Normal_Function} and \ref{fig: Uniform_Function}, except that  $\overline{x} - x = 0$.
This similarity is expected because the first-order approximation in both Formula \eqref{eqn: Taylor 1d mean} and Formula \eqref{eqn: Taylor 1d variance} contains $\sigma^2 \zeta(2)$. 

Statistical Taylor expansion outputs ideal uncertainty, ideal uncertainty bias, and ideal ratio for the chosen ideal bounding ranges, in which the ideal ratio quantifies the reliability of the ideal uncertainty and ideal uncertainty bias.



\subsection{Dependency Tracing}

\iffalse

\begin{align*}
(f(x) g(x))^{(i)} &= (f(x)^{(1)} g(x) + f(x) g(x)^{(1)})^{(i-1)} \\ 
  &= (f(x)^{(2)} g(x) + 2  f(x)^{(1)} g(x)^{(1)} + f(x) g(x)^{(2)})^{(i-2)} \\
  &= (f(x)^{(3)} g(x) + 3  f(x)^{(2)} g(x)^{(1)} + 3  f(x)^{(1)} g(x)^{(2)} + f(x) g(x)^{(3)})^{(i-3)} \\
  &= \sum_{j=0}^{i} \frac{i!}{j! (i-j)!} f(x)^{(j)} g(x)^{(i-j)}; \\
(f(x)^2 g(x)^2)^{(i)} &=2 \left( (f g) (f g)^{(1)} \right)^{(i-1)} \\
  &= 2 \left( (f g)^{(1)} (f g)^{(1)} + (f g) (f g)^{(2)} \right)^{(i-2)} \\
  &= 2 \left( 3 (f g)^{(1)} (f g)^{(2)} + (f g) (f g)^{(3)} \right)^{(i-3)} \\
  &= 2 \left( 4 (f g)^{(1)} (f g)^{(3)} + 3 (f g)^{(2)} (f g)^{(2)} + (f g) (f g)^{(4)} \right)^{(i-4)}
\end{align*}

\begin{align*}
\delta^2 (c_1 f + c_0) &= c_1^2 \delta^2f; \\
\delta^2 (f + g) &= \overline{(f + g)^2} - \overline{(f + g)}^2 = \overline{f^2} - \overline{f}^2 + 2 \overline{f g} - 2 \overline{f} \overline{g} + \overline{g^2} - \overline{f}^2 \\
 &= \delta^2 f + \delta^2 g + 2 (\overline{fg} - \overline{f}\overline{g}); \\
\overline{f g} &= \sum_{n=0}^{\infty}(\delta x)^{2n} \frac{\zeta(2n)}{(2n)!} (f g)^{(2n)}_x 
  = \sum_{n=0}^{\infty}(\delta x)^{2n} \zeta(2n) \sum_{j=0}^{2n} \frac{f^{(j)}_x}{j!} \frac{g^{(2n - j)}_x}{(2n - j)!};  \\
\overline{f} \overline{g} &= \sum_{n=0}^{\infty}(\delta x)^{2n} \sum_{j=0}^{n} \frac{\zeta(2j) f^{(2j)}_x}{(2j)!} \frac{\zeta(2n - 2j) g^{(2n - 2j)}_x}{(2n - 2j)!};  \\
\delta^2 (f g) &= \overline{f^2 g^2} - \overline{f g}^2; \\
\delta^2 f(g) &= \overline{f(g)^2} - \overline{f(g)}^2;
\end{align*}

When $(f g)^{(n)} = f^{(n)} g^{(n)}$, $\overline{f^n g^n} = \overline{f^n} \overline{g^n}$.
\begin{align*}
\delta^2 (f + g) &= \delta^2 f + \delta^2 g + 2 (\overline{fg} - \overline{f}\overline{g}) =  \delta^2 f + \delta^2 g;  \\
\delta^2 (f g) &= \overline{f^2 g^2} - \overline{f g}^2 = \overline{f^2}\;\overline{g^2} - \overline{f}^2 \overline{g}^2
  = (\delta^2 f + \overline{f}^2)(\delta^2 g + \overline{g}^2) - \overline{f}^2 \overline{g}^2 \\
  &= (\delta^2 f) (\delta^2 g) + \overline{f}^2 (\delta^2 g) + (\delta^2 f) \overline{g}^2; 
\end{align*}

\fi

\begin{align}
\label{eqn: sum variance}
\delta^2 (f + g) =&\; \delta^2 f + \delta^2 g + 2 (\overline{fg} - \overline{f}\overline{g}); \\
\label{eqn: prod variance}
\delta^2 (f g) =&\; \overline{f^2 g^2} - \overline{f g}^2; \\
\label{eqn: composite variance}
\delta^2 f(g) =&\; \overline{f(g)^2} - \overline{f(g)}^2; \\
\label{eqn: linear variance}
\delta^2 (c_1 f + c_0) =&\; c_1^2 \delta^2f; \\
\label{eqn: Taylor inner dependency}
\delta^2 \left( \frac{f^{(m)}_x \tilde{x}^m}{m!} + \frac{f^{(n)}_x \tilde{x}^n}{n!} \right) =&\;
    \sigma^{2m} (\frac{f^{(m)}_x }{m!})^2 \eta(2m) + \sigma^{2n} (\frac{f^{(n)}_x }{n!})^2 \eta(2n) \\
&+ 2 \sigma^{m+n} \left( \frac{f^{(m)}_x f^{(n)}_x}{m! \;n!} \eta(m+n) - \frac{f^{(m)}_x}{m!} \eta(m) \;\frac{f^{(n)}_x}{n!}  \eta(n) \right);  \nonumber 
\end{align}


When all inputs satisfy the uncorrelated uncertainty assumption, statistical Taylor expansion traces dependencies through the intermediate steps.
For example:
\begin{itemize}
\item Formula \eqref{eqn: sum variance} expresses $\delta^2 (f + g)$, whose dependency tracing is illustrated by $\delta^2 (f - f) = 0$, and $\delta^2 (f(x) + g(y)) = \delta^2 f + \delta^2 g$, with the latter corresponding to Formula \eqref{eqn: addition variance}.   
Formula \eqref{eqn: Taylor inner dependency} shows that Formula \eqref{eqn: Taylor 1d variance} applies Formula \eqref{eqn: sum variance} between any two terms in the Taylor expansion in Formula \eqref{eqn: Taylor 1d}.

\item Formula \eqref{eqn: prod variance} expresses $\delta^2 (f g)$, illustrated by $\delta^2 (f/f) = 0$, $\delta^2 (f f) = \delta^2 f^2$, and $\delta^2 (f(x) g(y)) = \overline{f}^2 (\delta^2 g) + (\delta^2 f) \overline{g}^2 +  (\delta^2 f) (\delta^2 g)$, with the latter corresponding to Formula \eqref{eqn: multiplication variance}.  

\item Formula \eqref{eqn: composite variance} shows  $\delta^2 f(g(x))$, whose dependency tracing is demonstrated by $\delta^2 (f^{-1}(f)) = (\delta x)^2$.  

\item Formula \eqref{eqn: linear variance} gives the variance of the linear transformation of a function, which can be applied to Formula \eqref{eqn: sum variance} and \eqref{eqn: prod variance} for more general dependency tracing.

\end{itemize}
Variance arithmetic employs dependency tracing to ensure that the calculated mean and variance satisfy statistics rigorously.
However, dependency tracing comes at a cost: variance calculations are generally more complex than value calculations and exhibits a narrower convergence range for input variables.
Dependency tracing also implies that the results of statistical Taylor expansion must remain path independent.


\subsection{Traditional Execution and Dependency Problem}

\iffalse

\begin{align*}
\overline{f(x)} =& f(x) + \sum_{n=1}^{\infty}(\delta x)^{n} \frac{f^{(n)}_x \zeta(n)}{n!}; \\
\delta^2 f(x) =& \sum_{n=2}^{\infty} (\delta x)^{n} \sum_{j=1}^{n-1} \frac{f^{(j)}_x}{j!} \frac{f^{(n-j)}_x}{(n-j)!} \left(\zeta(n) - \zeta(j) \zeta(n -j) \right);  \\
\overline{f(g(x))} =&\; o((\delta x)^5) + f(g(x)) + (\delta x) f^{(1)} g^{(1)} \zeta(1) 
		+ (\delta x)^2 \frac{1}{2} \zeta(2) \left( f^{(2)} (g^{(1)})^2 +  f^{(1)} g^{(2)} \right)  \\
	&+ (\delta x)^3 \frac{1}{6} \zeta(3) \left( f^{(3)} (g^{(1)})^3 + 3 f^{(2)} g^{(1)} g^{(2)} +  f^{(1)} g^{(3)} \right)  \\
	&+ (\delta x)^4 \frac{1}{24} \zeta(4) \left( f^{(4)} (g^{(1)})^4 + 6 f^{(3)} (g^{(1)})^2 g^{(2)} 
		+ 3 f^{(2)} (g^{(2)})^2 + 4 f^{(2)} g^{(1)} g^{(3)} + f^{(1)} g^{(4)} \right);
\end{align*}

With $\zeta(2n+1) = 0$:
\begin{align*}
f(g)^{(4)} =& f^{(4)} (g^{(1)})^4 + 6 f^{(3)} (g^{(1)})^2 g^{(2)} 
		+ 3 f^{(2)} (g^{(2)})^2 + 4 f^{(2)} g^{(1)} g^{(3)} + f^{(1)} g^{(4)}; \\
\overline{f(g(x))} =&\; o((\delta x)^6) + f(g(x))
		+ (\delta x)^2 \frac{\zeta(2)}{2} \left( f^{(2)} (g^{(1)})^2 +  f^{(1)} g^{(2)} \right)  \\
		&+ (\delta x)^4 \frac{\zeta(4)}{24} f(g)^{(4)}; \\
\delta^2 f(g(x)) =&\; o((\delta x)^6) + (\delta x)^2 \frac{\zeta(2)^4}{4} \left( f^{(1)} g^{(1)} \right)^2 \\
   	&+ (\delta x)^4 \left( \frac{\zeta(4) - \zeta(2)^2}{4} \left( f^{(2)} (g^{(1)})^2 +  f^{(1)} g^{(2)} \right)^2 
   		+ \frac{\zeta(4)}{3} f^{(1)} g^{(1)} f(g)^{(4)} \right);
\end{align*}

\begin{align*}
(\delta g)^2 =&\; o((\delta x)^6) + (\delta x)^2 \frac{\zeta(2)}{2} (g^{(1)})^2 
		+ (\delta x)^4 \frac{\zeta(4)}{24} \left( 2 g^{(1)} g^{(3)} + (g^{(2)})^2 \right); \\
\overline{f(g)|_{g(x)}} =&\; o((\delta x)^6) + f(g(x)) + (\delta g)^2 \frac{\zeta(2)}{2} f^{(2)} + (\delta g)^4 \frac{\zeta(4)}{24} f^{(4)};\\
	=&\; o((\delta x)^6) + f(g(x)) + \left( (\delta x)^2 \frac{\zeta(2)}{2} (g^{(1)})^2 
			+ (\delta x)^4 \frac{\zeta(4)}{24} \left( 2 g^{(1)} g^{(3)} + (g^{(2)})^2 \right) \right) \frac{\zeta(2)}{2} f^{(2)}\\
		&+ (\delta x)^4 \frac{\zeta(2)^2}{4} (g^{(1)})^4 \frac{\zeta(4)}{24} f^{(4)} \\
	=&\; o((\delta x)^6) + f(g(x)) + (\delta x)^2 \frac{\zeta(2)^2}{4} f^{(2)} (g^{(1)})^2 \\
		&+ (\delta x)^4 \frac{\zeta(2) \zeta(4)}{48} \left( \frac{\zeta(2)}{2} f^{(2)} (g^{(1)})^4 
			+ f^{(2)} \left( 2 g^{(1)} g^{(3)} + (g^{(2)})^2 \right) \right);\\ 
\delta^2 f(g)|_{g(x)} =&\; o((\delta x)^6) + (\delta g)^2 \frac{\zeta(2)}{2} (f^{(1)})^2
		 + (\delta g)^4 \frac{\zeta(4)}{24} \left( 2 f^{(1)} f^{(3)} + (f^{(2)})^2 \right) \\
	=&\; o((\delta x)^6) + \left( (\delta x)^2 \frac{\zeta(2)}{2} (g^{(1)})^2 
			+ (\delta x)^4 \frac{\zeta(4)}{24} \left( 2 g^{(1)} g^{(3)} + (g^{(2)})^2 \right) \right) \frac{\zeta(2)}{2} (f^{(1)})^2 \\
		&+ (\delta x)^4 \frac{\zeta(2)^2}{4} (g^{(1)})^4 \frac{\zeta(4)}{24} \left( 2 f^{(1)} f^{(3)} + (f^{(2)})^2 \right) \\
	=&\; o((\delta x)^6) + (\delta x)^2 \frac{\zeta(2)^2}{4} (f^{(1)})^2 (g^{(1)})^2 \\
		&+ (\delta x)^4 \frac{\zeta(2) \zeta(4)}{48} \left( \frac{\zeta(2)}{2} \left( 2 f^{(1)} f^{(3)} + (f^{(2)})^2 \right) (g^{(1)})^4 
			+ (f^{(1)})^2 \left( 2 g^{(1)} g^{(3)} + (g^{(2)})^2 \right) \right);
\end{align*}

\fi

Dependency tracing requires an analytic form of the function to apply statistical Taylor expansion for the result mean and variance, as in Formula \eqref{eqn: Taylor 1d mean}, \eqref{eqn: Taylor 1d variance}, \eqref{eqn: Taylor 2d mean}, and \eqref{eqn: Taylor 2d variance}.
This requirement often conflicts with conventional numerical methods for analytic functions:
\begin{itemize}

\item
Traditionally, intermediate variables are widely used in computations; however, this practice disrupts dependency tracing by obscuring the relationships among the original input variables.

\item
Similarly, conditional executions are often employed to optimize performance and minimize rounding errors, for example, using Gaussian elimination to minimize floating-point rounding errors in matrix inversion \cite{Linear_Algebra}.  
For dependency tracing, such conditional executions should instead be replaced by direct matrix inversion as described in Section \ref{sec: matrix}.

\item 
Furthermore, traditional approaches frequently apply approximations to result values during execution.
Under the statistical Taylor expansion, Formula \eqref{eqn: Taylor 1d variance} shows that the variance converges more slowly than value in statistical Taylor expansion.
Consequently, approximation strategies should prioritize variances than values.
Section \ref{sec: matrix} illustrates this principle through a first-order approximation used in computing a matrix determinant.

\item
Traditionally, results from mathematical library functions are accepted without scrutiny, with accuracy assumed down to the last bit.
As demonstrated in Section \ref{sec: FFT}, statistical Taylor expansion enables the detection of numerical errors within these functions and requires that they be recalculated with uncertainty explicitly incorporated into the output.

\item 
In conventional practice, an analytic expression is often decomposed into simpler, ostensibly and independent arithmetic operations such as negation, addition, multiplication, division, square root, and library calls.
However, this decomposition introduces dependency problems in floating-point arithmetic, interval arithmetic, and statistical Taylor expansion.
For example, if $x^2 - x$ is calculated as $x^2 - x$, $x(x - 1)$, and $(x - \frac{1}{2})^2 - \frac{1}{4}$, only $(x - \frac{1}{2})^2 - \frac{1}{4}$ gives the correct result, while the other two give wrong results for wrong independence assumptions between $x^2$ and $x$, or between $x -1$ and $x$, respectively.

\item
Similarly, large calculations are often divided into sequential steps, such as computing $f(g(x))$ as $f(y)|_{y = g(x)}$.
This approach fails in statistical Taylor expansion because dependency tracing within $g(x)$ affects $f(g(x))$.
In this context, $\overline{f(g(x))} \neq \overline{f(y)|_{y = g(x)}}$ and $\delta^2 f(g(x)) \neq \delta^2 f(y)|_{y = g(x)}$.
The path dependence of $f(y)|_{y = g(x)}$ and $\delta^2 f(y)|_{y = g(x)}$ are evident in cases such as $\overline{(\sqrt{x})^2} > \overline{\sqrt{x^2}}$ and $\delta^2 (\sqrt{x})^2 > \delta^2 \sqrt{x^2}$.

\end{itemize}

Dependency tracing therefore removes nearly all flexibility from traditional numerical executions, effectively eliminating the associated dependency problems.
Consequently, all conventional numerical algorithms must be reevaluated or redesigned to align with the principles of statistical Taylor expansion.








\ifdefined\Verbose
\clearpage
\fi
\section{Variance Arithmetic}
\label{sec: variance arithmetic}

Variance arithmetic implements statistical Taylor expansion.
Because of the finite precision and limited range of conventional floating-point representation, $\zeta(n)$ can only be computed to limited terms.
Consequently, the following numerical rules are introduced:
\begin{itemize}
\item \emph{finite}: The resulting value and variance must remain finite.

\item \emph{monotonic}: As a necessary condition for convergence, the last $20$ terms of the expansion must decrease monotonically in absolute value, ensuring that the probability of the expansion exhibiting an absolute increase is no more than $2^{-20} = 9.53\; 10^{-7}$.
Unless all the remaining terms in the expansion are known to be precisely zeros, each expansion is executed to the full 448 terms for the monotonicity check.

\item \emph{positive}: At every order, the expansion variance must be positive.

\item \emph{stable}: To avoid truncation error \cite{Numerical_Recipes}, the value of the last expansion term must be less than $5.73\;10^{-7}$ times of both the result uncertainty and the result absolute value, in which $5.73\;10^{-7}$ is the bounding leakage for Gaussian distribution with bounding range $\kappa = 5$.
This rule ensures sufficiently fast convergence in the context of monotonic convergence.

\item \emph{reliable}: At every order, the uncertainty of the variance must be less than $1/5$ times of the value of the variance.

\end{itemize}

For simplicity of discussion, This paper confines the calculation of variances to ideal variances $\widehat{\delta^2} f$ and assumes that the input distribution is Gaussian with $\kappa_s = 5$.
Furthermore,the Taylor coefficients in Formula \eqref{eqn: Taylor 1d} and \eqref{eqn: Taylor 2d} are assumed to be precise.


\subsection{Numerical Representation}

Variance arithmetic represents an imprecise value $x \pm \delta x$ using a pair of 64-bit standard floating-point numbers.
All other conventional numerical numbers must be converted to this format.

If the least $20$ significant bits of the significand in a standard floating-point number are all $0$, the value is considered precise, representing a 2's fractional with a probability no less than $1 - 2^{-20} = 1 - 2.384\;10^{-7}$.
Otherwise, a standard floating-point value is considered imprecise with its uncertainty defined as $1/\sqrt{3}$ times of the ULP of the value, where ULP refers to the \emph{Unit in the Last Place} in conventional floating-point representation \cite{Floating_Point_Standard}.
This follows from the fact that the pure rounding error in round-to-nearest mode is uniformly distributed within half bit of the significand of a floating-point value \cite{Prev_Precision_Arithmetic}.

If an integer number is within the range $[-2^{53} + 1, +2^{53} - 1]$ of the significand of a 64-bit standard floating-point number, its uncertainty is zero.
Otherwise, it is first converted to a conventional floating-point value before being transformed into an imprecise value.

Variance arithmetic uses floating-point arithmetic for computation.


\subsection{Finite}

For $(1 \pm \delta x)^{-2}$, the Taylor coefficient increases with the expansion order $n$ as $(-1)^n (n + 1)$.
When $\delta x = 0.5$, this growth causes the result variance to diverge to infinity.


\subsection{Monotonic}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for Gaussian uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
}
\label{fig: Pow_Conv_Edge}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for uniform uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
}
\label{fig: Pow_Conv_Edge_Uniform}
\end{figure}

From Formula \eqref{eqn: log convergence}, the convergence condition applied to $\log(x \pm \delta x)$ is $P(x) \leq 1/\kappa = 1/5$, which is numerically confirmed as $P(x) \lesssim 0.20086$.
Beyond this upper bound, the expansion is no longer monotonic.
Variance arithmetic rejects the distributional zero of $\log(x)$ in the range of $[x - \delta x, x + \delta x]$ statistically due to the divergence of Formula \eqref{eqn: log precision} mathematically, with $\zeta(2n)$ providing the connection between these two perspectives.

For $e^{x \pm \delta x}$ the convergence holds for $\delta x \lesssim 19.864$ regardless of $x$, while the result $\delta \log(x \pm \delta x) \lesssim 0.213$  regardless of $x$.
These limits follow directly from the relationship $\delta x \rightarrow P(e^x)$ and $P(x) \rightarrow \delta \log(x)$, as indicated in  Formula \eqref{eqn: exp precision} and \eqref{eqn: log precision}.

From Formula \eqref{eqn: pow convergence}, and except when $c$ is a natural number, Formula \eqref{eqn: power precision} for $(x \pm \delta x)^c$ converges near $P(x) \simeq 1/\kappa = 1/5$, with the upper bound $P(x)$ increasing with $c$. 
This trend is approximately confirmed in Figure \ref{fig: Pow_Conv_Edge}, and the expansion is no longer monotonic beyond the upper bound $P(x)$.
Qualitatively, $\delta^2 1/x$ converges more slowly than $\delta^2 \sqrt{x}$,  consistent with Formula \eqref{eqn: inversion precision} and \eqref{eqn: square root precision}.

When the input uncertainty is Uniformly distributed, Figure \ref{fig: Pow_Conv_Edge_Uniform} illustrates that the input uncertainty upper bound is close to $1/\kappa=/\sqrt{3}$ according to Formula \eqref{eqn: pow convergence}.
The bounded momentum $\zeta(2n)$ for Uniform distribution is much less than that for Normal distribution, such that the expansion contains $652$ terms instead of $448$ terms.
The upper bounds of input uncertainty in Figure \ref{fig: Pow_Conv_Edge} and \ref{fig: Pow_Conv_Edge_Uniform} looks identical, however, the resulting uncertainty biases and uncertainties in Figure \ref{fig: Pow_Conv_Edge} are $3$ order-of-magnitude smaller than those in Figure \ref{fig: Pow_Conv_Edge_Uniform} when the exponent $c$ is less than $0$.
Even a result converges, it can still be abandoned due to too large uncertainty bias or uncertainty.




\subsection{Positive}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for Gaussian uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for uniform uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge_Uniform}
\end{figure}


In some cases, the variance expansion may yield negative results, as in Formula \eqref{eqn: sin precision} for $sin(x \pm \delta x)$.
Figure \ref{fig: Sin_Conv_Edge} shows that the upper bound of $\delta x$ for $sin(x \pm \delta x)$ varies periodically between $0.318 \pi$ and $0.416 \pi$.
Beyond this upper bound, the expansion is no longer positive.

In Figure \ref{fig: Sin_Conv_Edge_Uniform}, when the bounded momentum $\zeta(2n)$ for Uniform distribution is used instead of that for Normal distribution, the input uncertainty upper bound is $4$ times larger than that in Figure \ref{fig: Sin_Conv_Edge} but is still periodic.
The resulting uncertainty increases not much and still remains less than $1$.


\subsection{Stable}

The unstable condition happens rarely.


\subsection{Reliable}

The condition of not being reliable seldom happens.


\subsection{Comparison}

\iffalse

Statistically the less relation between two imprecise values $x \pm \delta x$ and $y \pm (\delta y)^2$ is calculated by Formula \ref{eqn: x < y}:
\begin{align}
& z \equiv \frac{\tilde{y} - y}{\delta y}; \eqspace \tilde{y} = \delta y z + y; \\
& x - \Delta x < z \delta y + y < x + \Delta x; \\ & x - \Delta x - y < z \delta y < x + \Delta x - y \\
& - \Delta y < z \delta y < \Delta y; \\
p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) & = 
  \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
    \int_{x - \Delta x}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
  \int_{-\frac{\Delta x}{\delta x}}^{\frac{\tilde{y} - x}{\delta x}} N(z) d z \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
      \frac{1}{2}(\frac{\tilde{y} - x}{\sqrt{2} \delta x}) - \zeta(\frac{-\Delta x}{\sqrt{2} \delta x})) \;d \tilde{y}; \\
& = \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x})\right) N(z) d z; \\
p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) & =     
   \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x})\right) N(z) d z;
\end{align}
Formula \eqref{eqn: x < y} seems correct because it predicts $p(x \le y) = p(y \ge x)$:
\begin{align}
& \frac{d}{d \tilde{y}} \int_{-\infty}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} = \rho(\tilde{y}, x, \delta x); \\
p(x< y) & = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \rho(\tilde{y}, x, \delta x) d \tilde{x} d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \;d \rho(\tilde{y}, x, \delta x) \\
& = 0 - \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) \;d \rho(\tilde{y}, y, \delta y) \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) 
\int_{\tilde{y}}^{+\infty} \rho(\tilde{x}, y, \delta y) 
d \tilde{x} \;d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{x}, x, \delta x) 
\int_{\tilde{x}}^{+\infty} \rho(\tilde{y}, y, \delta y) 
d \tilde{y} \;d \tilde{x}
\end{align}

Two imprecise values can be compared directly for less or greater relation when their ranges $(x - \Delta x, x + \Delta x)$ and $(y - \Delta y, y + \Delta y)$ do not overlap. 
Otherwise, statistically, Formula \eqref{eqn: x < y} and \eqref{eqn: x > y} gives the probability for less or greater relation between $x \pm (\delta x)^2 \le y \pm (\delta y)^2$, in which $\min()$ and $\max()$ are minimal and maximal functions, respectively.
Formula \eqref{eqn: no =} shows that two imprecise variable can not be equal statistically.
Let $x = y$, $\delta x = \delta y$ and $\Delta x = \Delta y$, Formula \eqref{eqn: x < y} and \eqref{eqn: x > y} shows that two conceptually equal imprecise values has $1/2$ chance to be one less than the other, and $1/2$ chance to be one more than the other.
Thus,
\begin{align}
\label{eqn: x < y}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) = 
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x})\right) N(z) d z; \\
\label{eqn: x > y}
& p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) =     
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x})\right) N(z) d z; \\
\label{eqn: no =}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) + p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) = 1;
\end{align}

\fi

Two imprecise values can be compared statistically based on their difference.

When the value difference is zero, the two imprecise values are considered equal.  
In statistics, such two values have a $50\%$ possibility of being either less than or greater to each other but zero probability of being exactly equal \cite{Probability_Statistics}.
In variance arithmetic, however, they are treated as neither less nor greater than each other and therefore are considered equal.

Otherwise, the standard z-statistic method \cite{Probability_Statistics} is applied to determine whether two imprecise values are statistically equal, less than, or greater than each other.
For example, the difference between $1.002 \pm 0.001$ and $1.000 \pm 0.002$ is $0.002 \pm 0.00224$, yielding $z = 0.002 / 0.00224$.
The probability that they are not equal is $\xi(|z|/\sqrt{2}) = 62.8\%$, in which $\xi(z)$ is the cumulative distribution function for Normal distribution \cite{Probability_Statistics}.
If the threshold probability for inequality is set at $50\%$, then $1.000 \pm 0.002 < 1.002 \pm 0.001$.
Alternatively, an equivalent bounding range for z can be used, such as $|z| \leq 0.67448975$ for an equal probability threshold of $50\%$.

Because the result of comparison depends on threshold probability which is application specific, comparison is not part of variance arithmetic.






\ifdefined\Verbose
\clearpage
\fi
\section{Verification of Variance Arithmetic}
\label{sec: validation}

\subsection{Verification Methods and Standards}

Analytic functions or algorithms with precisely known results are used to evaluate the outputs of variance arithmetic based on the following statistical properties: 
\begin{itemize}

\item \emph{Value error}: the difference between the numerical result and the corresponding known precise analytic result.

\item \emph{Value deviation}: the standard deviation of the value errors.

\item \emph{Normalized error}: the ratio of a value error to the corresponding uncertainty.

\item \emph{Error deviation}: the standard deviation of normalized errors.

\item \emph{Error distribution}: the histogram of the normalized errors.

\item \emph{Uncertainty mean}: the sample mean of the result uncertainties.

\item \emph{Uncertainty response}: the relationship between input uncertainties and output uncertainties.

\item \emph{Calculation response}: the relationship between the amount of calculation and output uncertainties.

\end{itemize}

One objective of uncertainty-based computation is to account precisely for all input errors from every source, thereby achieving \emph{ideal coverage}:
\begin{enumerate}
\item The error deviation is exactly $1$.

\item The error distribution should follow Normal distribution when an imprecise value is expected, or Delta distribution when a precise value is expected.

\end{enumerate}
If the precise result is unknown, the resulting normalized error distribution can be used to assess whether ideal coverage is achieved.

However, if the input uncertainty is known only to order of magnitude, \emph{proper coverage} is achieved when the error deviations fall within the range $[0.1, 10]$.

When an input contains unspecified errors, such as numerical errors in library functions, Gaussian noise with progressively increasing deviations can be added, until ideal coverage is attained.
The minimal noise deviation required provides a good estimate of the magnitude of the unspecified input uncertainties.
Achieving ideal coverage serves as a necessary verification step to ensure that Formula \eqref{eqn: Taylor 1d variance} or \eqref{eqn: Taylor 2d variance} have been applied correctly within the given context.
The input noise range that yields ideal coverage defines the ideal application range for input uncertainties.

Besides coverage, other validation criteria are also used:
\begin{itemize}
\item The uncertainty response should match the expected functional form.
For a linear function, the output uncertainties should increase linearly with the input uncertainties, with the ratio of output to input uncertainty means defined as \emph{uncertainty response ratio}.

\item The calculation response should follow the expected trend; for instance, increasing the number of calculations should result in larger output uncertainties.

\end{itemize}
Noises of varying deviations can also be added to the input to evaluate a functions uncertainty response.



\subsection{Types of Uncertainties}

There are five primary sources of result uncertainty in a calculation \cite{Statistical_Methods}\cite{Numerical_Recipes}\cite{Prev_Precision_Arithmetic}:
\begin{itemize}
\item Input uncertainties:
The examples presented in this paper demonstrate that when the precision of input uncertainties is $10^{-15}$ or larger, variance arithmetic can achieve ideal coverage for input uncertainties.

\item Rounding errors:
Empirical results indicate that variance arithmetic provides proper coverage for rounding errors.

\item Truncation errors:
Variance arithmetic avoids truncation errors with its stable rule.
However, using Formula \eqref{eqn: polynomial Taylor} for polynomial expansion can result in truncation errors when the choice of the expansion order is insufficient, as illustrated in Figure \ref{fig: Poly_x} .

\item External errors:
External errors are value errors not specified in the input uncertainties, such as numerical errors in library functions.
Section \ref{sec: FFT} examines the effects of numerical errors of library sine function, showing that when these external errors are sufficiently large, neither ideal coverage nor proper coverage can be achieved.
This finding indicates that library functions must be recalculated to explicitly include the corresponding uncertainty for each computed value.

\item Modeling errors: 
Modeling errors arise when an approximate analytic solution is used, or when a real-world problem is simplified to make a solution tractable.  
For example, Section \ref{sec: FFT} demonstrates that the discrete Fourier transform (DFT) is only an approximation of the mathematically defined continuous Fourier transform (FT), and therefore contains modeling errors.  
Conceptually, modeling errors originate in mathematics within and are thus outside the domain of statistical Taylor expansion.

\end{itemize}







\subsection{Types of Calculations to Verify}

Algorithms of distinct natures, with each representative of its respective category, are required to test the broad applicability of variance arithmetic  \cite{Prev_Precision_Arithmetic}.  
An algorithm can be categorized by comparing the amount of its input and output data as \cite{Prev_Precision_Arithmetic}:
\begin{itemize}
\item Application,
\item Transformation,
\item Generation,
\item Reduction.
\end{itemize}

An \emph{application} algorithm computes numerical values from an analytic formula. 
Through statistical Taylor expansion, variance arithmetic applies directly to analytic problems.
For example, for the catastrophic cancellation example of Formula \eqref{eqn: float num calc}, Formula \eqref{eqn: variance num calc} shows that variance arithmetic bound the rounding error of $1$ with uncertainty $1$:
\begin{align}
\label{eqn: variance num calc}
64919224 &\times 205117922 - 159018721 \times 83739041 =\nonumber \\ 
&13316075197586562. - 13316075197586560. \pm 1 = 2. \pm 1;
\end{align}

A \emph{transformation} algorithm produces output data of approximately the same quantity as its input, with the overall information content remaining largely unchanged.  
For reversible transformations, a unique requirement is to recover every original input for both value and uncertainty after a \emph{roundtrip} transformation that is performing a \emph{forward} transformation followed by its \emph{reverse} transformation.  
The discrete Fourier transform (DFT) is a typical reversible transformation algorithm: it has the same amount of input and output data, and its output can be transformed back into the input using essentially the same process.  
A test of variance arithmetic using the fast Fourier transform (FFT, which is an implementation of DFT) algorithms is presented in Section \ref{sec: FFT}.

A \emph{generation} algorithm produces substantially more output data than input data.  
Such algorithms encode mathematical knowledge into data.  
Certain generation algorithms are purely theoretical calculations that involve no imprecise input, so all resulting uncertainty arises solely from rounding errors.  
Section \ref{sec: recursion} presents a generation algorithm that generates a sine function table using trigonometric relations and two precise inputs: $sin(0)=0$ and $sin(\pi/2)=1$.  

A \emph{reduction} algorithm yields significantly fewer output data than input data, as in numerical integration, or in the statistical characterization of a data set.  
In this process, certain information is lost while other information is extracted.  
As a statistical approach, variance arithmetic inherently carries out statistical reduction.
For example, the result of averaging $N$ imprecise values with each uncertainty close to $\delta x$ is $\overline{x} \pm \frac{\delta x}{\sqrt{N}}$, reflecting central limit theorem \cite{Probability_Statistics} naturally.


\ifdefined\Verbose
\clearpage
\fi
\section{Polynomial}
\label{sec: polynomial}

Formula \eqref{eqn: polynomial Taylor} presents polynomial Taylor expansion:
\begin{align}
\label{eqn: polynomial Taylor}
\sum_{j=0}^{N} c_j (x + \tilde{x})^j &= \sum_{j=0}^{N} \tilde{x}^{j} P_j, \eqspace
	P_j \equiv \sum_{k=0}^{N-j} x^{k - j} c_{j + k} \begin{pmatrix} j + k \\ j \end{pmatrix};
\end{align}

\subsection{Tracking Rounding Error}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Poly_x.pdf}
\captionof{figure}{
Residual error of $\sum_{j=0}^{224} x^j - \frac{1}{1 - x}$ vs $x$ (x-axis).
The y-axis to the left shows both the value and the uncertainty of the residual errors.
The y-axis to the right indicates the expansion order needed to reach stable value for each $x$. 
}
\label{fig: Poly_x}
\end{figure}

Variance arithmetic can track rounding errors effectively without the need for additional rules.  

Figure \ref{fig: Poly_x} shows the residual error of $\sum_{j=0}^{224} x^j  - \frac{1}{1 - x}$, where the polynomial $\sum_{j=0}^{224} x^j$ is computed using Formula \eqref{eqn: polynomial Taylor}, $\frac{1}{1 - x}$ is computed using Formula \eqref{eqn: power Taylor}, and $x$ is initiated as a floating-point value.
Because $\eta(2n)$ is limited to $2n \leq 448$, Formula \eqref{eqn: polynomial Taylor} for polynomial evaluation is restricted to $N \leq 224$, so that $\sum_{j=0}^{224} x^j$ has lower expansion order than that of the statistical Taylor expansion of $\frac{1}{1 - x}$.
Figure \ref{fig: Poly_x} shows:
\begin{itemize}
\item
When $x \in [-0.73, 0.75]$, the required expansion order is no more than $224$, which indicates that the residual error reflects solely the rounding error between $\sum_{j=0}^{224} x^j$ and $\frac{1}{1 - x}$.
A detailed analysis indicates that the maximal residual error is $4$ times the ULP of $\frac{1}{1 - x}$.
The calculated uncertainty bounds the residual error effectively for all $x \in [-0.73, 0.75]$.

\item
When $x \not \in [-0.74, +0.75]$, the required expansion order exceeds $224$, so that the residual error arises from the insufficient expansion order of $\sum_{j=0}^{224} x^j$.
The residual error magnitude increases as $|x| \rightarrow 1$, reaching approximately $50$ when $x = 0.98$.
\end{itemize}



\subsection{Continuity}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_at_x=0.pdf} 
\captionof{figure}{
Histograms of normalized errors for $(x \pm 0.2)^n$, with $x = 0, -0.2, +0.2$, and $n = 2, 3$, as indicated in the legend.
}
\label{fig: Poly_Continuity}
\end{figure}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|} 
\hline 
$n \pm d$                     & $0 \pm 10^{-6}$           & $1 \pm 10^{-6}$           & $2 \pm 10^{-6}$                   & $3 \pm 10^{-6}$               \\ 
\hline 
Upper Bound  $\delta x$ & $0.2006$                       & $0.2014$                       & $0.2018$                              & $0.2020$                            \\ 
\hline 
Value                            & $1 \mp 2.155\;10^{-8}$ & $1 \pm 2.073\;10^{-8}$ & $1.041 \pm 6.065\;10^{-8}$  & $1.122 \pm 1.328\;10^{-7}$ \\
\hline 
Uncertainty                    & $0 + 2.127\;10^{-7}$    & $0.201 -1.358\;10^{-6}$ & $0.407 \pm 2.201\;10^{-7}$ & $0.654 \pm 2.784\;10^{-7}$ \\
\hline 
\end{tabular}
}
\captionof{table}{
The result value and uncertainty of $(1 \pm \delta x)^{n \pm d}$ vs $(1 \pm \delta x)^n$, in which $n$ is a natural number, $0 < d \ll 1$, and $\delta x$ is the upper bound for $(1 \pm \delta x)^{n \pm d}$.
The value and the uncertainty are expressed as the difference with the corresponding value and uncertainty of those of $(1 \pm \delta x)^n$.
}
\label{tbl: power continuity}
\end{table}


In variance arithmetic, the result mean, variance and histogram are generally continuous across parameter space.
For example, $\delta x$ has an upper bound for $(x \pm \delta x)^c$ to converge except when $c$ is a natural number.
The result mean, variance and histogram of $(x \pm \delta x)^c$ remain continuous around $c = n$.
Table \ref{tbl: power continuity} shows that the result of $(1 \pm \delta x)^{n \pm d}$ where $0 < d \ll 1$ is very close to that of $(1 \pm \delta x)^n$, even though the former has an upper bound for $\delta x$, while the latter does not.

A statistical bounding range in variance arithmetic can include a distributional pole if the analytic function is defined in its vicinity.
The presence of such poles does not disrupt the continuity of the result mean, variance, or histogram.
Figure \ref{fig: Poly_Continuity} illustrates the histograms of $(x \pm 0.2)^n$ when $x = 0, -0.2, +0.2$ and $n = 2, 3$.
\begin{itemize}
\item When the second derivative is zero, the resulting distribution is symmetric two-sided and Delta-like, such as when $n = 3, x = 0$.

\item When the second derivative is positive, the resulting distribution is right-sided Delta-like, such as the distribution when $n = 2, x = 0$, or when $n = 2, x = \pm 0.2$, or when $n = 3, x = 0.2$.

\item When the second derivative is negative, the resulted distribution is left-sided and Delta-like, such as when $n = 3, x = -0.2$, which is the mirror image of the distribution when $n = 3, x = 0.2$.

\end{itemize}
In each case, the transition from $x = 0$ to $x = 0.2$ is continuous.

A statistical bounding range in variance arithmetic cannot encompass more than one distributional pole, as this condition causes the corresponding statistical Taylor expansion becomes negative.
An illustrative example is $\sin(x)$ shown in Figure \ref{fig: Sin_Conv_Edge}.

A statistical bounding range in variance arithmetic cannot include any distributional zero because the result will diverge, such as at $x = 0$ for both $(x \pm \delta x)^c, c < 1$ and $\log(x \pm \delta x)$.







\ifdefined\Verbose
\clearpage
\fi
\section{Mathematical Library Functions}
\label{sec: Math Library}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Exp_Dev.pdf} 
\captionof{figure}{
Calculated uncertainties versus measured value deviations (left y-axis), along with the measured error deviations (right y-axis) for $e^{x \pm \delta x}$, for different $x$ (x-axis), and different $\delta x$ (legend).
}
\label{fig: Exp_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Log_Dev.pdf} 
\captionof{figure}{
Calculated uncertainties versus measured value deviations (left y-axis), along with the measured error deviations (right y-axis) for $\log(x \pm \delta x)$, for different $x$ (x-axis), and different $\delta x$ (legend).
}
\label{fig: Log_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Dev.pdf} 
\captionof{figure}{
Calculated uncertainties versus measured value deviations (left y-axis), along with the measured error deviations (right y-axis) for $(1 \pm \delta x)^c$, for different $c$ (x-axis), and different $\delta x$ (legend).
}
\label{fig: Pow_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Exp_Dev.pdf} 
\captionof{figure}{
Error deviation for $(1 \pm \delta x)^c$ as a function of $c$ and $\delta x$.
The x-axis represents $c$ value between $-2$ and $+3$.
The y-axis represents $\delta x$ value between $-10^{-16}$ and $1$.
The z-axis shows the corresponding error deviation. 
}
\label{fig: Pow_Exp_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.4in]{Sin_Dev.pdf} 
\captionof{figure}{
Calculated uncertainties versus measured value deviations (left y-axis), along with the measured error deviations (right y-axis) for $\sin(x \pm \delta x)$, for different $x$ (x-axis), and different $\delta x$ (legend).
}
\label{fig: Sin_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_X_Dev.pdf} 
\captionof{figure}{
Error deviation for $\sin(x \pm \delta x)$ as a function of $x$ and $\delta x$.
The x-axis represents $x$ value between $-\pi$ and $+\pi$.
The y-axis represents $\delta x$ value between $-10^{-16}$ and $1$.
The z-axis shows the corresponding error deviation. 
}
\label{fig: Sin_X_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{ExpLog_Error.pdf} 
\captionof{figure}{
Values and uncertainties of $\log(e^x) - x$ and $e^{\log(x)} - x$ as functions of $x$, evaluated at $0.1$ increment.
When $x$ is 2's fractional such as $1/2$ or $1$, the result uncertainties are significantly smaller.
}
\label{fig: ExpLog_Error}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Error.pdf} 
\captionof{figure}{
Normalized errors of $(x^p)^{\frac{1}{p}} - x$ as functions of $x$ and $p$.
}
\label{fig: Power_Error}
\end{figure}


Formula \eqref{eqn: exp precision}, \eqref{eqn: log precision}, \eqref{eqn: sin precision}, and \eqref{eqn: power precision} are evaluated using the corresponding mathematical library functions \textit{exp}, \textit{log}, \textit{sin}, and \textit{pow}, respectively.

At each point $x$ with an input uncertainty $\delta x$, the result uncertainty is calculated by variance arithmetic.
The corresponding error deviation is determined by sampling as:
\begin{enumerate}

\item Generate $10000$ samples from a Gaussian noise distribution, each with $\delta x$ as the distributional deviation, and construct $\tilde{x}$ by adding the sampled noise to $x$.  

\item For each $\tilde{x}$, use the corresponding library function to compute the value error as the difference between the outputs obtained  using $\tilde{x}$ and $x$ as inputs.

\item Divide the value error by the result uncertainty to obtain the normalized error.

\item Calculate the standard deviation of the normalized errors to determine the error deviation.

\end{enumerate}

\subsection{Exponential}

Figure \ref{fig: Exp_Dev} shows that the calculated uncertainties obtained using Formula \eqref{eqn: exp precision} align closely with the measured value deviations for $e^{x + \delta x}$.
Consequently, all error deviations remain very close to $1$,  even though both the uncertainty and the value deviations increase exponentially with $x$ and $\delta x$.


\subsection{Logarithm}

Because $\log(x)$ has a distributional zero at $x=0$ all $log(x \pm \delta x)$ values are rejected when $P(x) > 1/5$.
Figure \ref{fig: Log_Dev} shows that the uncertainties calculated using Formula \eqref{eqn: log precision} align closely with the measured value deviations for $\log(x + \delta x)$, and the resulting error deviations remain very close to $1$ except when $P(x) > 0.20086$.


\subsection{Power}

Figure \ref{fig: Pow_Dev} shows that the uncertainties calculated for $(1 \pm \delta x)^c$ using Formula \eqref{eqn: power precision} fit closely with the measured value deviations for $(1 + \delta x)^c$, with the error deviations remaining near $1$.
Figure \ref{fig: Pow_Exp_Dev} reveals that the error deviations of $\sin(x + \delta x)$ remain close to $1$ for all input exponents $c$ and input uncertainties $\delta x > 10^{15}$.



\subsection{Sine}

Figure \ref{fig: Sin_Dev} shows that the uncertainties calculated using Formula \eqref{eqn: sin precision} correspond closely to the measured value deviations for $\sin(x + \delta x)$.
It also reveals that $\delta^2 \sin(x)$ exhibits the same periodicity as $\sin(x)$:
\begin{itemize}
\item When $x=0$, $\sin(x) \simeq x$, so that $\delta^2 \sin(x) \simeq (\delta x)^2$.
\item When $x=\pi/2$, $\sin(x) \simeq 1$, so that $\delta^2 \sin(x) \simeq 0$.
\end{itemize}

Since $\sin(x)$ has distributional poles at $x=\pm \pi/2$, Figure \ref{fig: Sin_X_Dev} shows that the error deviation for $\sin(x + \delta x)$ equals $1$ except when $x=\pm \pi/2$ and $\delta x < 10^{-8}$, matching the expected Delta distribution these poles. Elsewhere, the error deviations remain close to $1$.



\subsection{Numerical Errors for Library Functions}

The combined numerical error of the library functions $e^x$ and $\log(x)$ is evaluated as either $\log(e^x) - x$ or $e^{\log(x)} - x$.
Using either variance arithmetic or conventional floating-point library functions produces identical value errors.
Figure \ref{fig: ExpLog_Error} shows the corresponding result uncertainties, which reach a minimum when the input is a 2's fraction such as $1$ or $1/2$.
Figure \ref{fig: ExpLog_Error} also shows that $e^{\log(x)}$ exhibits much smaller error than  $\log(e^x)$.
For $\log(e^x) - x$, the error deviation is $0.409$ when $|x| \leq 1$ or approaches zero otherwise.
The reason for the unexpectedly   small value errors for $e^{\log(x)} - x$ is not yet clear.

The numerical error of the library function $x^p$ is computed as $(x^p)^{1/p} - x$.
Figure \ref{fig: Power_Error} shows that the normalized errors do not depend on either $x$ or $p$, resulting in an error deviation $0.548$.

The numerical errors of the library functions $\sin(x)$, $\cos(x)$, and $\tan(x)$ will be examined in greater detail in Section \ref{sec: FFT}.


\subsection{Summary}

Formula \eqref{eqn: exp precision}, \eqref{eqn: log precision}, \eqref{eqn: power precision}, and \eqref{eqn: sin precision} provide effective estimates of the corresponding library functions.
When added noise exceeds $10^{-15}$ precision, ideal coverage is achieved except near a distributional pole where the error deviation approaches zero.
In all other cases, proper coverage is attainable.


\ifdefined\Verbose
\clearpage
\fi
\section{Matrix Calculations}
\label{sec: matrix}


\subsection{Matrix Determinant}

\iffalse

\begin{align*}
|\mathbf{M}| &\equiv \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n \prod_{k=1 \dots n} x_{k,p_{k}}; \\
|\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}| &\equiv \sum_{[p_1 \dots p_n]_n}^{k =1 \dots m:\; p_{i_k}=j_k} \$ [p_1\dots p_n]_n 
	\prod _{k = 1 \dots n }^{i_k \not\in \{i_1 \dots i_m\}} x_{k,p_{k}}; \\
|\mathbf{M}_{<1 \dots n>_n, [j_1 \dots j_n]_n}| &= \$ [j_1\dots j_n]_n; \\
|\mathbf{M}| &= \sum_{{[j_1 \dots j_m]}_n} |\mathbf{M} _{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}| \prod_{k=1 \dots m} x_{i_k, j_k}; \\
|\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}| &= \sum_{j \in \{j_1 \dots j_m\}} |\mathbf{M}_{<i_1 \dots i \dots i_m>_n, [j_1 \dots j \dots j_m]_n}| \;x_{i, j};
\end{align*}

\begin{align*}
|\widetilde{M}| &\equiv \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n  \prod_{i=1 \dots n} (x_{i,p_i} + \tilde{x}_{i,p_{i}}) \\
	&= \sum_{[p_1 \dots p_{n}]_n} \$[p_1 \dots p_{n}]_n \sum_{m=0 \dots n} \sum_{<i_1 \dots i_m>_n}
		\prod_{i=i_1 \dots i_m} \tilde{x}_{i,p_{i}} \prod_{k = 1 \dots n }^{i_k \not \in \{i_1 \dots i_m\}} x_{k, p_{k}} \\
	&= \sum_{m=0 \dots n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_{m}]_n} 
		M_{<i_1 \dots i_m>_n, [j_1 \dots j_{m}]_n} \prod_{i=1 \dots m}^{i \in \{i_1 \dots i_m\}} \tilde{x}_{i,p_{i}}
\end{align*}
Examples for $|\widetilde{M}|$:
\begin{align*}
|\tilde{M}| &= x_{0,0} + \tilde{x}_{0,0}; \\
|\tilde{M}| &= \left| \begin{matrix} 
            x_{0,0} + \tilde{x}_{0,0}, x_{0,1} + \tilde{x}_{0,1} \\ 
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1} + \tilde{x}_{1,1} \end{matrix} \right| 
	= (x_{0,0} + \tilde{x}_{0,0})(x_{1,1} + \tilde{x}_{1,1}) - (x_{0,1} + \tilde{x}_{0,1})(x_{1,0} + \tilde{x}_{1,0}) \\
	&= \left| \begin{matrix}  x_{0,0}, x_{0,1} \\  x_{1,0},  x_{1,1} \end{matrix} \right| 
	  + \left| \begin{matrix}  x_{0,0}, \tilde{x}_{0,1} \\  x_{1,0},  \tilde{x}_{1,1} \end{matrix} \right|
	  + \left| \begin{matrix}  \tilde{x}_{0,0}, x_{0,1} \\  \tilde{x}_{1,0},  x_{1,1} \end{matrix} \right|
	  + \left| \begin{matrix}  \tilde{x}_{0,0}, \tilde{x}_{0,1} \\  \tilde{x}_{1,0}, \tilde{x}_{1,1} \end{matrix} \right| \\
|\tilde{M}| &= \left| \begin{matrix} 
            x_{0,0} + \tilde{x}_{0,0}, x_{0,1} + \tilde{x}_{0,1}, x_{0,2} + \tilde{x}_{0,2} \\ 
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1} + \tilde{x}_{1,1}, x_{1,2} + \tilde{x}_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, x_{2,1} + \tilde{x}_{2,1}, x_{2,2} + \tilde{x}_{2,2} \end{matrix} \right|
	= \left| \begin{matrix} 
	        x_{0,0} + \tilde{x}_{0,0}, x_{0,1} + \tilde{x}_{0,1}, x_{0,2} \\ 
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1} + \tilde{x}_{1,1}, x_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, x_{2,1} + \tilde{x}_{2,1}, x_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} x_{0,0} + \tilde{x}_{0,0}, x_{0,1} + \tilde{x}_{0,1}, \tilde{x}_{0,2} \\ 
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1} + \tilde{x}_{1,1}, \tilde{x}_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, x_{2,1} + \tilde{x}_{2,1}, \tilde{x}_{2,2} \end{matrix} \right| \\
 &= \left| \begin{matrix} 
 		    x_{0,0} + \tilde{x}_{0,0}, x_{0,1}, x_{0,2} \\ 
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1}, x_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, x_{2,1}, x_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	        x_{0,0} + \tilde{x}_{0,0}, \tilde{x}_{0,1}, x_{0,2} \\ 
			x_{1,0} + \tilde{x}_{1,0}, \tilde{x}_{1,1}, x_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, \tilde{x}_{2,1}, x_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	        x_{0,0} + \tilde{x}_{0,0}, x_{0,1}, \tilde{x}_{0,2}  \\
			x_{1,0} + \tilde{x}_{1,0}, x_{1,1}, \tilde{x}_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, x_{2,1}, \tilde{x}_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	        x_{0,0} + \tilde{x}_{0,0}, \tilde{x}_{0,1}, \tilde{x}_{0,2} \\
			x_{1,0} + \tilde{x}_{1,0}, \tilde{x}_{1,1}, \tilde{x}_{1,2} \\ 
			x_{2,0} + \tilde{x}_{2,0}, \tilde{x}_{2,1}, \tilde{x}_{2,2} \end{matrix} \right| \\
 &= \left| \begin{matrix} 
        	x_{0,0}, x_{0,1}, x_{0,2} \\ 
			x_{1,0}, x_{1,1}, x_{1,2} \\ 
			x_{2,0}, x_{2,1}, x_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	  		\tilde{x}_{0,0}, x_{0,1}, x_{0,2} \\ 
			\tilde{x}_{1,0}, x_{1,1}, x_{1,2} \\ 
			\tilde{x}_{2,0}, x_{2,1}, x_{2,2} \end{matrix} \right|	
	  + \left| \begin{matrix} 
	  		x_{0,0}, \tilde{x}_{0,1}, x_{0,2} \\ 
			x_{1,0}, \tilde{x}_{1,1}, x_{1,2} \\ 
			x_{2,0}, \tilde{x}_{2,1}, x_{2,2} \end{matrix} \right|
      + \left| \begin{matrix} 
      		x_{0,0}, x_{0,1}, \tilde{x}_{0,2}  \\
			x_{1,0}, x_{1,1}, \tilde{x}_{1,2} \\ 
			x_{2,0}, x_{2,1}, \tilde{x}_{2,2} \end{matrix} \right| \\
	  &\eqspace + \left| \begin{matrix} 
	  		\tilde{x}_{0,0}, \tilde{x}_{0,1}, x_{0,2} \\ 
			\tilde{x}_{1,0}, \tilde{x}_{1,1}, x_{1,2} \\ 
			\tilde{x}_{2,0}, \tilde{x}_{2,1}, x_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	  		\tilde{x}_{0,0}, x_{0,1}, \tilde{x}_{0,2}  \\
			\tilde{x}_{1,0}, x_{1,1}, \tilde{x}_{1,2} \\ 
			\tilde{x}_{2,0}, x_{2,1}, \tilde{x}_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	  		x_{0,0}, \tilde{x}_{0,1}, \tilde{x}_{0,2} \\
			x_{1,0}, \tilde{x}_{1,1}, \tilde{x}_{1,2} \\ 
			x_{2,0}, \tilde{x}_{2,1}, \tilde{x}_{2,2} \end{matrix} \right|
	  + \left| \begin{matrix} 
	  		\tilde{x}_{0,0}, \tilde{x}_{0,1}, \tilde{x}_{0,2} \\
			\tilde{x}_{1,0}, \tilde{x}_{1,1}, \tilde{x}_{1,2} \\ 
			\tilde{x}_{2,0}, \tilde{x}_{2,1}, \tilde{x}_{2,2} \end{matrix} \right| \\
 &= |\mathbf{M}| + \sum_{i = 0}^{2} \sum_{j = 0}^{2} |\mathbf{M}_{i,j}| \tilde{x}_{i,j} 
 		+ \sum_{<j_1,j_2>} \sum_{[i_1,i_2]} |\mathbf{M}_{[i_1,i_2], <j_1,j_2>}| \tilde{x}_{i_1,j_1} \tilde{x}_{i_2,j_2} \\
 &+ \sum_{<j_1,j_2,j_3>} \sum_{[i_1,i_2,i_3]} \tilde{x}_{i_1,j_1} \tilde{x}_{i_2,j_2} \tilde{x}_{i_3,j_3};
\end{align*}

\begin{align*}
\delta^2 |\mathbf{M}| &= \sum_{m=1}^{n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_m]_n}
  		|\mathbf{M} _{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}|^2  \prod_{k=1 \dots n}^{i_k \in \{i_1 \dots i_m\}} (\delta x_{i_k, j_k})^2; \\
\delta^2 |\mathbf{M}_{i,j}| &= \sum_{m=2 \dots n} \sum_{<i_1 \dots i_m>_n}^{i \in \{i_1 \dots i_m\}} \sum_{[j_1 \dots j_m]_n}^{j \in \{j_1 \dots j_m\}}
  	|\mathbf{M} _{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}|^2  \prod_{k=1 \dots m}^{i_k \in \{i_1 \dots i_m\}, i_k \ne i} (\delta x_{i_k, j_k})^2;
\end{align*}
For each $m$, $\delta^2 |\mathbf{M}|$ has  $\frac{n!}{(n - m)! m!} \frac{n!}{(n - m)!}$ terms, while $\delta^2 |\mathbf{M}_{i,j}|$ has $\frac{(n - 1)!}{(n - m)! (m - 1)!} \frac{(n - 1)!}{(n - m)!}$ terms.  The ratio is $\frac{m^2}{n^2}$.
Thus, the following relation in the previous paper is wrong:
\begin{equation*}
\delta^2 |\mathbf{M}| = \sum_{i=1}^{n} \sum_{j=1}^{n} (\delta x_{i, j})^2 \left(|\mathbf{M}_{i, j}|^2 + \delta^2 |\mathbf{M}_{i,j}| \right);
\end{equation*}
The above relation only holds when $n=2$:
\begin{align*}
|\mathbf{M}| &= \left| \begin{matrix} x_{0,0}, x_{0,1} \\ x_{1,0}, x_{1,1} \end{matrix} \right| = x_{0,0} x_{1,1} - x_{0,1} x_{1,0};  \\
|\tilde{M}| - |\mathbf{M}| &= \tilde{x}_{0,0} x_{1,1} + x_{0,0} \tilde{x}_{1,1} + \tilde{x}_{0,0} \tilde{x}_{1,1}
	 - \tilde{x}_{0,1} x_{1,0} - x_{0,1} \tilde{x}_{1,0} - \tilde{x}_{0,1} \tilde{x}_{1,0}; \\
\delta^2 |\mathbf{M}| &= (\delta x_{0,0})^2 (x_{1,1})^2 + (x_{0,0})^2 (\delta x_{1,1})^2 +  (\delta x_{0,1})^2 (x_{1,0})^2 + (x_{0,1})^2 (\delta x_{1,0})^2 \\
		&+ (\delta x_{0,0})^2 (\delta x_{1,1})^2 + (\delta x_{0,1})^2 (\delta x_{1,0})^2; \\
	&= \delta^2 \left( (x_{0,0} \pm \delta x_{0,0}) (x_{1,1} \pm \delta x_{0,0}) -  (x_{0,1} \pm \delta x_{0,1}) (x_{1,0} \pm \delta x_{0,0}) \right);
\end{align*}
But it does not hold when $n=3$:
\begin{align*}
|\mathbf{M}| &= \left| \begin{matrix} x_{0,0}, x_{0,1}, x_{0,2} \\ x_{1,0}, x_{1,1}, x_{1,2} \\ x_{2,0}, x_{2,1}, x_{2,2} \end{matrix} \right| \\
&= x_{0,0} x_{1,1} x_{2,2} - x_{0,0} x_{1,2} x_{2,1} 
   	+ x_{0,1} x_{1,2} x_{2,0} - x_{0,1} x_{1,0} x_{2,2}
    + x_{0,2} x_{1,0} x_{2,1} -  x_{0,2} x_{1,1} x_{2,0}; \\
\delta^2 |\mathbf{M}| &= (\delta x_{0,0})^2 |\mathbf{M}_{0,0}|^2 + (\delta x_{0,1})^2 |\mathbf{M}_{0,1}|^2 + (\delta x_{0,2})^2 |\mathbf{M}_{0,2}|^2 \\
  &+ (\delta x_{1,0})^2 |\mathbf{M}_{1,0}|^2 + (\delta x_{1,1})^2 |\mathbf{M}_{1,1}|^2 + (\delta x_{1,2})^2 |\mathbf{M}_{1,2}|^2 \\
  &+ (\delta x_{2,0})^2 |\mathbf{M}_{2,0}|^2 + (\delta x_{2,1})^2 |\mathbf{M}_{2,1}|^2 + (\delta x_{2,2})^2 |\mathbf{M}_{2,2}|^2 \\
  &+ (\delta x_{0,0})^2 (\delta x_{1,1})^2 |x_{2,2}|^2 + (\delta x_{0,0})^2 (\delta x_{2,2})^2 |x_{1,1}|^2 + (\delta x_{1,1})^2 (\delta x_{2,2})^2 |x_{0,0}|^2 \\
  &+ (\delta x_{0,0})^2 (\delta x_{1,2})^2 |x_{2,1}|^2 + (\delta x_{0,0})^2 (\delta x_{2,1})^2 |x_{1,2}|^2 + (\delta x_{1,2})^2 (\delta x_{2,1})^2 |x_{0,0}|^2 \\
  &+ (\delta x_{0,1})^2 (\delta x_{1,2})^2 |x_{2,0}|^2 + (\delta x_{0,1})^2 (\delta x_{2,0})^2 |x_{1,2}|^2 + (\delta x_{1,2})^2 (\delta x_{2,0})^2 |x_{0,1}|^2 \\
  &+ (\delta x_{0,1})^2 (\delta x_{1,0})^2 |x_{2,2}|^2 + (\delta x_{0,1})^2 (\delta x_{2,2})^2 |x_{1,0}|^2 + (\delta x_{1,0})^2 (\delta x_{2,2})^2 |x_{0,1}|^2 \\
  &+ (\delta x_{0,2})^2 (\delta x_{1,0})^2 |x_{2,1}|^2 + (\delta x_{0,2})^2 (\delta x_{2,1})^2 |x_{1,0}|^2 + (\delta x_{1,0})^2 (\delta x_{2,1})^2 |x_{0,2}|^2 \\
  &+ (\delta x_{0,2})^2 (\delta x_{1,1})^2 |x_{2,0}|^2 + (\delta x_{0,2})^2 (\delta x_{2,0})^2 |x_{1,1}|^2 + (\delta x_{1,1})^2 (\delta x_{2,0})^2 |x_{0,2}|^2 \\
  &+ (\delta x_{0,0})^2 (\delta x_{1,1})^2 (\delta x_{2,2})^2 + (\delta x_{0,0})^2 (\delta x_{1,2})^2 (\delta x_{2,1})^2 + (\delta x_{0,1})^2 (\delta x_{1,2})^2 (\delta x_{2,0})^2 \\
  &+ (\delta x_{0,1})^2 (\delta x_{1,0})^2 (\delta x_{2,2})^2 + (\delta x_{0,2})^2 (\delta x_{1,0})^2 (\delta x_{2,1})^2 + (\delta x_{0,2})^2 (\delta x_{1,1})^2 (\delta x_{2,0})^2;\\
\end{align*}
\begin{align*}
& (\delta x_{2,2})^2 \delta^2 |\mathbf{M}_{2,2}| = (\delta x_{0,0})^2 (\delta x_{2,2})^2 |x_{1,1}|^2 + (\delta x_{1,1})^2 (\delta x_{2,2})^2 |x_{0,0}|^2 \\
  &+  (\delta x_{0,1})^2 (\delta x_{2,2})^2 |x_{1,0}|^2 + (\delta x_{1,0})^2 (\delta x_{2,2})^2 |x_{0,1}|^2 \\
  &+ (\delta x_{0,0})^2 (\delta x_{1,1})^2 (\delta x_{2,2})^2 + (\delta x_{0,1})^2 (\delta x_{1,0})^2 (\delta x_{2,2})^2; \\
& (\delta x_{2,1})^2 \delta^2 |\mathbf{M}_{2,1}| = (\delta x_{0,0})^2 (\delta x_{2,1})^2 |x_{1,2}|^2 + (\delta x_{1,2})^2 (\delta x_{2,1})^2 |x_{0,0}|^2 \\
  &+  (\delta x_{0,2})^2 (\delta x_{2,1})^2 |x_{1,0}|^2 + (\delta x_{1,0})^2 (\delta x_{2,1})^2 |x_{0,2}|^2 \\
  &+ (\delta x_{0,0})^2 (\delta x_{1,2})^2 (\delta x_{2,1})^2  + (\delta x_{0,2})^2 (\delta x_{1,0})^2 (\delta x_{2,1})^2; \\
& (\delta x_{2,0})^2 \delta^2 |\mathbf{M}_{2,0}| = (\delta x_{0,1})^2 (\delta x_{2,0})^2 |x_{1,2}|^2 + (\delta x_{1,2})^2 (\delta x_{2,0})^2 |x_{0,1}|^2 \\
  &+  (\delta x_{0,2})^2 (\delta x_{2,0})^2 |x_{1,1}|^2 + (\delta x_{1,1})^2 (\delta x_{2,0})^2 |x_{0,2}|^2 \\
  &+ (\delta x_{0,1})^2 (\delta x_{1,2})^2 (\delta x_{2,0})^2 + (\delta x_{0,2})^2 (\delta x_{1,1})^2 (\delta x_{2,0})^2; \\
\delta^2 |\mathbf{M}| & - \sum_{i=0}^{2} \sum_{j=0}^{2} (\delta x_{i,j})^2 |\mathbf{M}_{i,j}| - \sum_{j=0}^{2} (\delta x_{2,j})^2 \delta^2 |\mathbf{M}_{2,j}| \\
  &= (\delta x_{0,0})^2 (\delta x_{1,1})^2 |x_{2,2}|^2 + (\delta x_{0,0})^2 (\delta x_{1,2})^2 |x_{2,1}|^2 \\
  &+ (\delta x_{0,1})^2 (\delta x_{1,2})^2 |x_{2,0}|^2 + (\delta x_{0,1})^2 (\delta x_{1,0})^2 |x_{2,2}|^2 + \\
  &+ (\delta x_{0,2})^2 (\delta x_{1,0})^2 |x_{2,1}|^2 + (\delta x_{0,2})^2 (\delta x_{1,1})^2 |x_{2,0}|^2;
\end{align*}

\begin{align*}
|\mathbf{M}| &= x_{0,0} x_{1,1} x_{2,2} - x_{0,0} x_{1,2} x_{2,1} 
	&+ x_{0,1} x_{1,2} x_{2,0} - x_{0,1} x_{1,0} x_{2,2}
    &+ x_{0,2} x_{1,0} x_{2,1} -  x_{0,2} x_{1,1} x_{2,0}; \\
\end{align*}


Compare $\delta^2 |\mathbf{M}|$ with multiplication:
\begin{align*}
\delta^2 x_1 x_2 &= x_2^2 (\delta x_1)^2 + x_1^2 (\delta x_2)^2 + (\delta x_1)^2 (\delta x_2)^2 \\
\delta^2 x_1 x_2 x_3 &= \left( x_2^2 (\delta x_1)^2 + x_1^2 (\delta x_2)^2 + (\delta x_1)^2 (\delta x_2)^2 \right) x_3^2 + x_1^2 x_2^2 (\delta x_3^2) 
		+ \left( x_2^2 (\delta x_1)^2 + x_1^2 (\delta x_2)^2 + (\delta x_1)^2 (\delta x_2)^2 \right) (\delta x_3^2); \\
	&= (\delta x_1)^2 x_2^2 x_3^2 + x_1^2 (\delta x_2)^2 x_3^2 + x_1^2 x_2^2 (\delta x_3)^2 \\
	&+  x_1^2 (\delta x_2)^2 (\delta x_3)^2 + (\delta x_1)^2 x_2^2 (\delta x_3)^2 + (\delta x_1)^2 (\delta x_2)^2 x_3^2 + (\delta x_1)^2 (\delta x_2)^2 (\delta x_3)^2; \\
\delta^2 \prod_{k=1}^{n} x_k &= \sum_{m=1}^{n} \sum_{<i_1 \dots i_m>_n} \prod_{i \in <i_1 \dots i_m>_n} (\delta x_i)^2 \prod_{j \not \in <i_1 \dots i_m>_n} {x_j}^2;
\end{align*}

\fi

Let vector $[p_1, p_{2} \dots p_n]_n$ denote a permutation of the vector $(1,2\dots n)$ \cite{Linear_Algebra}.  
Let $\$[p_1, p_{2} \dots p_n]_n$ denote the permutation sign of $[p_1, p_{2} \dots p_n]_n$ \cite{Linear_Algebra}. 
Formula \eqref{eqn: determinant} defines the determinant of a $n$-by-$n$ square matrix $\mathbf{M}$ with the element $x_{i,j}, i,j=1 \dots n$ \cite{Linear_Algebra}.
The sub matrix $\mathbf{M_{i,j}}$ at index $(i, j)$ is formed by deleting the row $i$ and column $j$ of $M$, whose determinant is given by 
Formula \eqref{eqn: sub-determinant} \cite{Linear_Algebra}. 
For discussion simplicity, sub determinant $|\mathbf{M}|_{i,j}$ in Formula \eqref{eqn: sub-determinant} contains the permutation sign, which is different from the determinant of the sub matrix  $|\mathbf{M}_{i,j}|$ \cite{Linear_Algebra} that treats the sub matrix $\mathbf{M_{i,j}}$ as an independent matrix.
Formula \eqref{eqn: determinant sum 1} holds for the arbitrary row index $i$ or the arbitrary column index $j$ \cite{Linear_Algebra}.
\begin{align}
\label{eqn: determinant}
|\mathbf{M}| &\equiv \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n \prod_{k=1 \dots n} x_{k,p_{k}}; \\
\label{eqn: sub-determinant}
|\mathbf{M}|_{i,j} &\equiv \sum_{[p_1\dots p_n]_n}^{p_{i} = j} \$ [p_1\dots p_n]_n \prod_{k=1 \dots n}^{k \ne i} x_{k,p_{k}}; \\
\label{eqn: determinant sum 1}
|\mathbf{M}| &=\sum_{j=1 \dots n} |\mathbf{M}_{i,j}| x_{i,j} = \sum_{i=1 \dots n} |\mathbf{M}|_{i,j} x_{i,j};
\end{align}

Let ${<i_1,i_2,\dots>}$ denote an ordered permutation of a subset from $1\dots n$, and ${[i_1,i_2,\dots]}$ an unordered permutation \cite{Linear_Algebra}.
Apply Formula \eqref{eqn: determinant sum 1} progressively to $M_{i,j}$, to expand Formula \eqref{eqn: sub-determinant} as \eqref{eqn: sub-determinant generic}, and Formula \eqref{eqn: determinant} as \eqref{eqn: determinant sum}.
The $(n-m)$-by-$(n-m)$ sub matrix in $|\mathbf{M}_{<i_1 \dots i_m>_n[j_1 \dots j_m]_n}|$ is obtained by deleting the rows in $\{i_1 \dots i_m\}$ and the columns in $\{j_1 \dots j_m\}$.
This leads to Formula \eqref{eqn: sub-determinant equivalence}.
\begin{align}
\label{eqn: sub-determinant generic}
&|\mathbf{M}|_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n} \equiv \sum_{[p_1 \dots p_n]_n}^{k \in \{i_1 \dots i_m\}: p_k=j_k} \$ [p_1\dots p_n]_n 
	\prod_{k = 1 \dots n }^{k \not \in \{i_1 \dots i_m\}} x_{k,p_{k}}; \\
\label{eqn: determinant sum}
|\mathbf{M}| &= \sum_{{[j_1 \dots j_m]}_n} |\mathbf{M}| _{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n} \prod_{k=1}^{m} x_{i_k, j_k}; \\
\label{eqn: sub-determinant equivalence}
&\left| |\mathbf{M}|_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n} \right| = \left| |\mathbf{M}_{<i_1 \dots i_m>_n, <j_1 \dots j_m>_n|} \right|;
\end{align}

Formula \eqref{eqn: determinant Taylor expansion} gives the Taylor expansion $|\widetilde{\mathbf{M}}|$ of $|\mathbf{M}|$, which leads to Formula \eqref{eqn: determinant mean} and \eqref{eqn: determinant variance} for mean and variance of matrix determinant, respectively.
\begin{align}
\label{eqn: determinant Taylor expansion}
|\widetilde{\mathbf{M}}| &= \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n  \prod_{i=1 \dots n} (x_{i,p_i} + \tilde{x}_{i,p_{i}}) \\
	&= \sum_{m=0 \dots n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_{m}]_n} 
		\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_{m}]_n} \prod_{i=1 \dots m}^{i \in \{i_1 \dots i_m\}} \tilde{x}_{i,p_{i}}; \nonumber \\
\label{eqn: determinant mean}
\overline{|\mathbf{M}|} &= |\mathbf{M}|; \\
\label{eqn: determinant variance}
\delta^2 |\mathbf{M}| &= \sum_{m=1}^{n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_m]_n}
  	|\mathbf{M}_{<i_1 \dots i_m>_n, <j_1 \dots j_m>_n}|^2 \prod _{k=1 \dots n}^{i_k \in \{i_1 \dots i_m\}} (\delta x_{i_k, j_k})^2; 
\end{align}
Formula \eqref{eqn: determinant mean} and \eqref{eqn: determinant variance} assume that the uncertainties of matrix elements are all independent of each other.
This assumption maximized the result uncertainties.
For discussion simplicity, other uncertainty assumptions are ignored in this paper.



\subsection{Adjugate Matrix}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Uncertainty_vs_Size_Noise.pdf} 
\captionof{figure}{
Uncertainty means (z-axis) of adjugate matrix $\widetilde{\mathbf{M}}^A - \mathbf{M}^A$ as a function of matrix size (x-axis) and input noise precision (y-axis).
}
\label{fig: Adjugate_Uncertainty_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) of adjugate matrix $\widetilde{\mathbf{M}}^A - \mathbf{M}^A$ as a function of matrix size (x-axis) and input noise precision (y-axis).
}
\label{fig: Adjugate_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Forward_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) as a function of matrix size (x-axis) and input noise precision (y-axis) for the difference of the two sides of Formula \eqref{eqn: adjugate matrix}.
}
\label{fig: Forward_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Roundtrip_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) as a function of matrix size (x-axis) and input noise precision (y-axis) for the difference of the two sides of Formula \eqref{eqn: roundtrip matrix}.
}
\label{fig: Roundtrip_Error_vs_Size_Noise}
\end{figure}


The square matrix whose element is $a_{i, j} = (-1)^{i+j}|\mathbf{M}_{j,i}|$ is defined as the \emph{adjugate matrix} \cite{Linear_Algebra} $\mathbf{M}^A$ to the original square matrix $\mathbf{M}$.
Let $\mathbf{I}$ be the identity matrix for $\mathbf{M}$ \cite{Linear_Algebra}.
Formula \eqref{eqn: adjugate matrix} and \eqref{eqn: roundtrip matrix} show the relation of $\mathbf{M}^A$ and $\mathbf{M}$ \cite{Linear_Algebra}.
\begin{align}
\label{eqn: adjugate matrix}
\mathbf{M} \times \mathbf{M}^A &= \mathbf{M}^A \times \mathbf{M} = |\mathbf{M}| \mathbf{I}; \\
\label{eqn: roundtrip matrix}
|\mathbf{M}|\; \mathbf{M}^A &=|\mathbf{M}^A|\; \mathbf{M};
\end{align}

To test Formula \eqref{eqn: determinant variance}:
\begin{enumerate}
\item A matrix $\mathbf{M}$ is constructed using random integers uniformly distributed in the range of $[-2^8, +2^8]$, which has a distribution deviation of $2^8/\sqrt{3}$.
The integer arithmetic ensures that $|\mathbf{M}|$, $\mathbf{M}^A$, and $|\mathbf{M}^A|$ are precise.

\item Gaussian noises of specified input noise precision are added to $\mathbf{M}$, to construct an imprecise matrix $\widetilde{\mathbf{M}}$.
Variance arithmetic is used to calculate $|\widetilde{\mathbf{M}}|$, $\widetilde{\mathbf{M}}^A$, and $|\widetilde{\mathbf{M}}^A|$.
For example, to construct a $\widetilde{\mathbf{M}}$ with $10^{-3}$ input noise precision, the distributional deviation of the Gaussian noise is $10^{-3} \times 2^8/\sqrt{3}$.

\end{enumerate}

The difference between $\widetilde{\mathbf{M}}^A$ and $\mathbf{M}^A$ defines the \emph{Adjugate} Test.
As seen in Figure \ref{fig: Adjugate_Uncertainty_vs_Size_Noise}, the uncertainty means increase exponentially with both the input noises and the matrix size; however, such linear increase is segmented into two areas at input precision $10^{-10}$.
Figure \ref{fig: Adjugate_Error_vs_Size_Noise} shows that the ideal coverage is achieved for the input precision except at matrix size $8$ and input precision $10^{-17}$.
The existence of ideal coverage validates Formula \eqref{eqn: determinant variance}.

Formula \eqref{eqn: adjugate matrix} defines the \emph{Forward} Test.
Figure \ref{fig: Forward_Error_vs_Size_Noise} shows that the difference of the two sides of Formula \eqref{eqn: adjugate matrix} is precise zero, whether it is $\widetilde{\mathbf{M}} \times \widetilde{\mathbf{M}}^A - |\widetilde{\mathbf{M}}| \mathbf{I}$, or $\widetilde{\mathbf{M}}^A \times \widetilde{\mathbf{M}} - |\widetilde{\mathbf{M}}| \mathbf{I}$.   
The difference in Formula \eqref{eqn: adjugate matrix} is closer to Delta distribution for larger input precision, because without input noise, rounding errors dominate value errors.
The validation of Formula \eqref{eqn: determinant variance} leads naturally to the validation of Formula \eqref{eqn: adjugate matrix}. 

Formula \eqref{eqn: roundtrip  matrix} defines the \emph{Roundtrip} Test.
Similarly, Figure \ref{fig: Roundtrip_Error_vs_Size_Noise} validates Formula \eqref{eqn: roundtrip matrix}.
Because roundtrip test is no longer linear, error deviation in Figure \ref{fig: Roundtrip_Error_vs_Size_Noise} no longer increases linearly with input precision.



\subsection{Floating Point Rounding Errors}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Histogram_Noise_0.pdf} 
\captionof{figure}{
Histograms of normalized errors of the adjugate matrix as a function of matrix size without input noise (legend).
}
\label{fig: Adjugate_Histogram_Size_Noise_0}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Histogram_Noise_1e-11.pdf} 
\captionof{figure}{
Histograms of normalized errors of the adjugate matrix as a function of matrix size with $10^{-11}$ input noise (legend).
}
\label{fig: Adjugate_Histogram_Size_Noise_1e-11}
\end{figure}

\iffalse

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline 
Error Deviation & \multicolumn{4}{|c|}{Input Noise Precision}  \\ 
\hline 
Matrix Size & $0$ & $10^{-17}$ & $10^{-16}$ & $10^{-15}$ \\ 
\hline 
$4$ & $0$     &       $0$ & $0.68$ & $1.03$ \\
\hline 
$5$ & $0$     & $0.003$ & $0.60$ & $0.96$ \\
\hline 
$6$ & $0$     & $0.006$ & $0.80$ & $1.04$ \\
\hline 
$7$ & $0$     & $0.083$ & $1.05$ & $1.05$ \\
\hline 
$8$ & $4.62$ & $3.91$  & $3.01$ & $1.01$ \\
\hline 
\end{tabular}
\captionof{table}{
Error deviation as a function of matrix size and input noise precision for $\mathbf{M}^A$ in which $\mathbf{M}$ is generated by random integers between $[-2^8, +2^8]$.
The measured error deviations change slightly for different runs.
}
\label{tbl: matrix rounding errors}
\end{table}

Table \ref{tbl: matrix rounding errors} shows that variance arithmetic achieves proper coverage for the calculation rounding errors for adjugate matrix.

\fi

The significance of the conventional floating-point representation \cite{Floating_Point_Standard} has a $53$-bit resolution.
As shown in Figure \ref{fig: Adjugate_Histogram_Size_Noise_0}, the histogram of the normalized errors is Delta distributed for the matrix size less than $8$, because the adjugate matrix calculation involves about $8 \times 6 = 48$ significand bits for a matrix size $7$.
When the matrix size is $8$, $8 \times 7 = 56$ significand bits are needed so rounding occurs, which results in non-Delta distribution in Figure \ref{fig: Adjugate_Histogram_Size_Noise_0}.
The rounding error is also the reason why only at matrix size 8 and input noise precision $10^{-10}$, error deviation is no longer $1$ in Figure \ref{fig: Adjugate_Error_vs_Size_Noise}.

With $10^{-11}$ noises added to the input, Figure \ref{fig: Adjugate_Histogram_Size_Noise_1e-11}, the distribution becomes Gaussian with a hint of Delta distribution.
Such Delta-like distribution persists until the input noise precision reaches $10^{-10}$, which is also the transition of the two trends in Figure \ref{fig: Adjugate_Uncertainty_vs_Size_Noise}.
Figure \ref{fig: Adjugate_Histogram_Size_Noise_1e-11} shows the distribution when the input noise precision is $10^{-11}$, where the distinction due to matrix size vanishes.


\subsection{First Order Approximation}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Multiply_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviation (z-axis) of the first approximation calculation of $|\widehat{\mathbf{M}}|$ as a function of matrix size (x-axis) and input noise precision (y-axis).
}
\label{fig: Multiply_Error_vs_Size_Noise}
\end{figure}

Formula \eqref{eqn: determinant variance approx} shows the first order approximation of $|\widetilde{\mathbf{M}}|$ leads to the first order approximation of $\delta^2 |\mathbf{M}|$.   
It states that when the input precision is much less than $1$, the determinant $|\mathbf{M}|$ of an imprecise matrix $\mathbf{M}$ can be calculated in variance arithmetic using Formula \eqref{eqn: determinant} directly.
\begin{equation}
\label{eqn: determinant variance approx}
|\widetilde{\mathbf{M}}| \simeq \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n \prod_{k=1 \dots n} (x_{k,p_k} + \tilde{x}_{k,p_k}); \eqspace \Rightarrow  \eqspace
	\delta^2 |\mathbf{M}| \simeq \sum_{i}^{n} \sum_{j}^{n} M_{i,j} (\delta x_{i,j})^2;
\end{equation}
Figure \ref{fig: Multiply_Error_vs_Size_Noise} contains the result of applying Formula \eqref{eqn: determinant variance approx}.
It is very similar to Figure \ref{fig: Adjugate_Error_vs_Size_Noise}, validating Formula \eqref{eqn: determinant variance approx}.



\subsection{Matrix Inversion}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Matrix_Determinant_Prec_vs_Condition.pdf} 
\captionof{figure}{
Linear correlation between the precision of a matrix determinant (y-axis) to its condition number (x-axis).
The legend shows the size of the matrix, as well as the type of the matrix as \textit{Random} for randomly generated matrix, and \textit{Hilbert} as the Hilbert matrix.
}
\label{fig: Matrix_Determinant_Prec_vs_Condition}
\end{figure}

\iffalse

\begin{align*}
\mathbf{M} &= \left( \begin{matrix} w + \tilde{w}, x + \tilde{x} \\ y + \tilde{y}, z + \tilde{z} \end{matrix} \right);
\eqspace |\mathbf{M}| = w z - x y; \\
\mathbf{M}^{-1} &= \frac{\left( \begin{matrix} z + \tilde{w}, - x - \tilde{x} \\ - y -\tilde{y}, w + \tilde{w} \end{matrix} \right)}
			    		 {(w + \tilde{w})(z + \tilde{z}) - (x + \tilde{x})(y + \tilde{y})} 
	= \frac{\left( \begin{matrix} z + \tilde{z}, - x - \tilde{x} \\ - y -\tilde{y}, w + \tilde{w} \end{matrix} \right)}
			    {(w z - x y) - (x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w})} \\
	&= \left( \begin{matrix} z + \tilde{z}, - x - \tilde{x} \\ - y -\tilde{y}, w + \tilde{w} \end{matrix} \right)
	     \sum_{n=0} \frac{1}{|M|^{n+1}} \left( x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w} \right)^n \\
	&\simeq \left( \begin{matrix} z + \tilde{z}, - x - \tilde{x} \\ - y -\tilde{y}, w + \tilde{w} \end{matrix} \right)
		\left(\frac{1}{|M|} + \frac{x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w}}{|M|^2} \right) \\
	& = \left( \begin{matrix} \frac{z}{|M|} - , \frac{-x}{|M|} \\ \frac{-y}{|M|}, \frac{w}{|M|} \end{matrix} \right) 
		+ \left( \begin{matrix} \tilde{z}, -\tilde{x} \\-\tilde{y}, \tilde{w} \end{matrix} \right) 
			\frac{x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w}}{|M|^2}; \\
\overline{\mathbf{M}^{-1}} &\simeq \left( \begin{matrix} 
			\frac{z}{|M|} - \frac{w}{|M|^2} (\delta z)^2, 
			- \frac{x}{|M|} - \frac{y}{|M|^2} (\delta x)^2 \\ 
			- \frac{y}{|M|} - \frac{x}{|M|^2} (\delta y)^2, 
			\frac{w}{|M|} -  \frac{z}{|M|^2} (\delta w)^2 \end{matrix} \right); \\
& (w + \tilde{w})^2 \left( 1 + \frac{x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w}}{|M|}  \right)^2 \\
     &=  (w^2 + \tilde{w}^2) \left( 1 + \frac{(x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w})^2}{|M|^2 } \right) 
           + 2 w \tilde{w} \frac{x \tilde{y} + y \tilde{x} - w \tilde{z} - z \tilde{w}}{|M|} \\
	 &= \tilde{w}^2 + w^2 +  w^2 \frac{x^2 \tilde{y}^2 + y^2 \tilde{x}^2 + w^2 \tilde{z}^2 + z^2 \tilde{w}^2 }{|M|^2} - \frac{4 w z}{|M|} \tilde{w}^2; \\
\delta^2 \mathbf{M}^{-1}|_{1,1} &\simeq \frac{(\delta w)^2}{|M|^2}  
			\left(1 - \frac{4 w z}{|M|} \right) + \frac{w^2}{|M|^2} \left(x^2 (\delta y)^2 + y^2 (\delta x)^2 + z^2 (\delta w)^2 + w^2 (\delta z)^2 \right); 
\end{align*}

\begin{align*}
&\mathbf{M} = \left( \begin{matrix} w, x \\ y, z \end{matrix} \right);
\eqspace \mathbf{M}^{-1} = \frac{\left( \begin{matrix} z, - x \\ - y, w \end{matrix} \right)}{w z - x y} ;\\
&\frac{\partial}{\partial w} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} - z^2, x z \\ y z,  - x y \end{matrix} \right)}{(w z - x y)^2};
\eqspace \frac{\partial}{\partial x} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} y z, - w z \\ - y^2, w y \end{matrix} \right)}{(w z - x y)^2};
\eqspace\frac{\partial}{\partial y} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} x z, - x^2 \\ - w z, w x \end{matrix} \right)}{(w z - x y)^2};
\eqspace \frac{\partial}{\partial z} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} - x y, w x \\ w y, - w^2 \end{matrix} \right)}{(w z - x y)^2}; \\
& \delta^2 \mathbf{M}^{-1} \simeq \frac{
	 	\left( \begin{matrix} z^4, x^2 z^2 \\ y^2 z^2,  x^2 y^2 \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} y^2 z^2, w^2 z^2 \\ y^4, w^2 y^2 \end{matrix} \right) (\delta x)^2 +
	 	\left( \begin{matrix} x^2 z^2, x^4 \\ w^2 z^2, w^2 x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} x^2 y^2, w^2 x^2 \\ w^2 y^2, w^4 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^4}; \\
& \delta^2 \mathbf{M}^{-1} \simeq \frac{\left( \begin{matrix} 
			z^4 (\delta w)^2 + y^2 z^2 (\delta x)^2 + x^2 z^2 (\delta y)^2 + x^2 y^2 (\delta z)^2, 
	 		x^2 z^2 (\delta w)^2 + w^2 z^2 (\delta x)^2 + x^4 (\delta y)^2 + w^2 x^2 (\delta z)^2 \\ 
	 		y^2 z^2 (\delta w)^2 + y^4 (\delta x)^2 + w^2 z^2 (\delta y)^2 + w^2 y^2 (\delta z)^2,  
	 		x^2 y^2 (\delta w)^2 + w^2 y^2 (\delta x)^2 +  w^2 x^2 (\delta y)^2 + w^4 (\delta z)^2
	\end{matrix} \right)}{(w z - x y)^4}; \\
& \frac{1}{|\mathbf{M}|^2} \delta^2 \mathbf{M}^A + \mathbf{M}^A \delta^2 \frac{1}{|\mathbf{M}|} \simeq \frac{
		\left( \begin{matrix} (\delta z)^2, (\delta x)^2 \\ (\delta y)^2, (\delta w)^2 \end{matrix} \right) (w z - x y)^2 +
		\left( \begin{matrix} z^2,  x^2 \\ y^2, w^2 \end{matrix} \right) (z^2 (\delta w)^2 + y^2 (\delta x)^2 + x^2 (\delta y)^2 + w^2 (\delta z)^2)
	}{(w z - x y)^4}; \\
&\frac{\partial^2}{\partial^2 w^2} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} 2 z^3, -2 x z^2 \\ - 2 y z^2,  2 x y z \end{matrix} \right)}{(w z - x y)^3};
\eqspace \frac{\partial^2}{\partial x^2} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} 2 y^2 z, - 2 w y z \\  -2 y^3, 2 w y^2 \end{matrix} \right)}{(w z - x y)^3}; \\
&\eqspace \frac{\partial^2}{\partial y^2} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} 2 x^2 z, - 2 x^3 \\  - 2 w x z, 2 w x^2 \end{matrix} \right)}{(w z - x y)^3};
\eqspace \frac{\partial^2}{\partial z^2} \mathbf{M}^{-1} =  \frac{\left( \begin{matrix} 2 w x y, - 2 w^2 x \\ -2 w^2 y, 2 w^3 \end{matrix} \right)}{(w z - x y)^3}; \\
&\overline{\mathbf{M}^{-1}} - \mathbf{M}^{-1} \simeq \frac{
	 	\left( \begin{matrix} 2 z^3, -2 x z^2 \\ - 2 y z^2,  2 x y z \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} 2 y^2 z, - 2 w y z \\  -2 y^3, 2 w y^2 \end{matrix} \right) (\delta x)^2 +
	 	\left( \begin{matrix} 2 x^2 z, - 2 x^3 \\  - 2 w x z, 2 w x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} 2 w x y, - 2 w^2 x \\ -2 w^2 y, 2 w^3 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^3}; \\
\end{align*}

\begin{align*}
\mathbf{M} =& \left( \begin{matrix} x_{1,1},\; x_{1,2},\; x_{1,3} \\ x_{2,1},\; x_{2,2},\; x_{2,3} \\ x_{3,1},\; x_{3,2},\; x_{3,3} \end{matrix} \right); \\
\mathbf{M}^A =& \left( \begin{matrix} 
		x_{2,2} x_{3,3} - x_{2,3} x_{3,2},\; x_{1,3} x_{3,2} - x_{1,2} x_{3,3},\; x_{1,2} x_{2,3} - x_{1,3} x_{2,2} \\ 
		x_{2,3} x_{3,1} - x_{2,1} x_{3,3},\; x_{1,1} x_{3,3} - x_{1,3} x_{3,1},\; x_{1,3} x_{2,1} - x_{1,1} x_{2,3} \\ 
		x_{2,1} x_{3,2} - x_{2,2} x_{3,1},\; x_{1,2} x_{3,1} - x_{1,1} x_{3,2},\; x_{1,1} x_{2,2} - x_{1,2} x_{2,1} 
\end{matrix} \right); \\
|\mathbf{M}| =&\; x_{1,1} x_{2,2} x_{3,3} + x_{1,2} x_{2,3} x_{3,1} + x_{1,3} x_{2,1} x_{3,2} -
						x_{1,1} x_{2,3} x_{3,2} - x_{1,2} x_{2,1} x_{3,3} - x_{1,3} x_{2,2} x_{3,1}; 
\end{align*}
\begin{align*}
\frac{\partial \mathbf{M}^{-1}}{\partial x_{1,1}} =& \frac{\left( \begin{matrix} 
-|\mathbf{M}_{1,1}||\mathbf{M}_{1,1}|,  +|\mathbf{M}_{2,1}||\mathbf{M}_{1,1}|, -|\mathbf{M}_{3,1}||\mathbf{M}_{1,1}| \\
+|\mathbf{M}_{1,1}||\mathbf{M}_{1,2}|, -|\mathbf{M}_{2,1}||\mathbf{M}_{1,2}|,  +|\mathbf{M}_{3,1}||\mathbf{M}_{1,2}| \\
-|\mathbf{M}_{1,1}||\mathbf{M}_{1,3}|, +|\mathbf{M}_{2,1}||\mathbf{M}_{1,3}|, +|\mathbf{M}_{3,1}||\mathbf{M}_{1,3}|
\end{matrix} \right)}{|\mathbf{M}|^2}; \\
\frac{\partial \mathbf{M}^{-1}}{\partial x_{1,2}} =& \frac{\left( \begin{matrix} 
+|\mathbf{M}_{1,2}||\mathbf{M}_{1,1}|,  -|\mathbf{M}_{2,2}||\mathbf{M}_{1,1}|, +|\mathbf{M}_{3,2}||\mathbf{M}_{1,1}| \\
-|\mathbf{M}_{1,2}||\mathbf{M}_{1,2}|, +|\mathbf{M}_{2,2}||\mathbf{M}_{1,2}|,  -|\mathbf{M}_{3,2}||\mathbf{M}_{1,2}| \\
+|\mathbf{M}_{1,2}||\mathbf{M}_{1,3}|, -|\mathbf{M}_{2,2}||\mathbf{M}_{1,3}|, +|\mathbf{M}_{3,2}||\mathbf{M}_{1,3}|
\end{matrix} \right)}{|\mathbf{M}|^2}; \\
\frac{\partial \mathbf{M}^{-1}}{\partial x_{1,3}} =& \frac{\left( \begin{matrix} 
+|\mathbf{M}_{1,3}||\mathbf{M}_{1,1}|,  -|\mathbf{M}_{2,3}||\mathbf{M}_{1,1}|, +|\mathbf{M}_{3,3}||\mathbf{M}_{1,1}| \\
-|\mathbf{M}_{1,3}||\mathbf{M}_{1,2}|, +|\mathbf{M}_{2,3}||\mathbf{M}_{1,2}|,  -|\mathbf{M}_{3,3}||\mathbf{M}_{1,2}| \\
+|\mathbf{M}_{1,3}||\mathbf{M}_{1,3}|, -|\mathbf{M}_{2,3}||\mathbf{M}_{1,3}|, +|\mathbf{M}_{3,3}||\mathbf{M}_{1,3}|
\end{matrix} \right)}{|\mathbf{M}|^2}; \\
\end{align*}

\fi



\begin{align}
\label{eqn: inverse matrix}
\mathbf{M}^{-1} \equiv&\; \mathbf{M}^A / |\mathbf{M}; \\
\label{eqn: matrix inversion}
&\mathbf{M}^{-1} \times \mathbf{M} = \mathbf{M} \times \mathbf{M}^{-1} = \mathbf{I}; \\
\label{eqn: matrix double inversion}
&(\mathbf{M}^{-1})^{-1} = \mathbf{M};
\end{align}

An inverse matrix is defined by Formula \eqref{eqn: inverse matrix}, which satisfies Formula \eqref{eqn: matrix inversion} and \eqref{eqn: matrix double inversion} \cite{Linear_Algebra}.
However, this definition is seldom used conventionally to calculate inverse matrix due to large uncertainty response ratio which appears as small input uncertainties causing large output uncertainties \cite{Numerical_Recipes}\cite{Linear_Algebra}.
Traditionally, matrix condition number \cite{Linear_Algebra} is a proxy for uncertainty response ratio of matrix inversion.
In Formula \eqref{eqn: inverse matrix}, $\mathbf{M}^{-1}$ is dominated by $1/|\mathbf{M}|$, suggesting that the precision of $\mathbf{M}^{-1}$ is largely determined by the precision of $|\mathbf{M}|$.
Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} shows that there is a strong linear correlation between conditional numbers and the corresponding determinant precision of matrices.
As a reference, Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} presents the Hilbert matrix \cite{Linear_Algebra} for each matrix size, and shows that the Hilbert matrices also follow the linear relation between determinant precision and condition number.
Thus, determinant precision can replace condition number to estimate uncertainty response ratio of matrix inversion.

\begin{align}
\label{eqn: inverse matrix 2}
\mathbf{M} =&\; \left( \begin{matrix} w, x \\ y, z \end{matrix} \right);
\eqspace \mathbf{M}^{-1} = \frac{\left( \begin{matrix} z, - x \\ - y, w \end{matrix} \right)}{w z - x y};\\
\label{eqn: inverse matrix 2 variance}
\delta^2 \mathbf{M}^{-1} \simeq&\; \frac{
	 	\left( \begin{matrix} z^4, x^2 z^2 \\ y^2 z^2,  x^2 y^2 \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} y^2 z^2, w^2 z^2 \\ y^4, w^2 y^2 \end{matrix} \right) (\delta x)^2}{(w z - x y)^4} + \\
	&\; \frac{\left( \begin{matrix} x^2 z^2, x^4 \\ w^2 z^2, w^2 x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} x^2 y^2, w^2 x^2 \\ w^2 y^2, w^4 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^4}; \nonumber \\
\label{eqn: inverse matrix 2 bias}
\overline{\mathbf{M}^{-1}} - \mathbf{M}^{-1} \simeq&\; \frac{
	 	\left( \begin{matrix} 2 z^3, -2 x z^2 \\ - 2 y z^2,  2 x y z \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} 2 y^2 z, - 2 w y z \\  -2 y^3, 2 w y^2 \end{matrix} \right) (\delta x)^2}{(w z - x y)^3} + \\
	& \frac{\left( \begin{matrix} 2 x^2 z, - 2 x^3 \\  - 2 w x z, 2 w x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} 2 w x y, - 2 w^2 x \\ -2 w^2 y, 2 w^3 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^3}; \nonumber
\end{align}

Variance arithmetic is path-independent, so it computes inverse matrix directly from the definition of Formula \eqref{eqn: inverse matrix}.
For example, for the inverse matrix $\mathbf{M}^{-1}$ of size $2$ in Formula \eqref{eqn: inverse matrix 2}, Formula \eqref{eqn: inverse matrix 2 variance} and \eqref{eqn: inverse matrix 2 bias} present first-order approximations for variance $\delta^2 \mathbf{M}^{-1}$, and bias $\overline{\mathbf{M}^{-1}} - \mathbf{M}^{-1}$, respectively. 
The resulting uncertainties of determinant $\delta^2 |\mathbf{M}|$ and inversion $\delta^2 \mathbf{M}^{-1}$, as well as the resulting bias $\overline{\mathbf{M}^{-1}} - \mathbf{M}^{-1}$, are not linear to each input uncertainty $\delta x_{i,j}$.
Inverse variance $\delta^2 \mathbf{M}^{-1}$ contains sum of all input variance $(\delta x_{i,j})^2$ and their higher-order permutation products, such that the uncertainty response ratio for matrix inversion should roughly equal the matrix size.
Thus, an uncertainty response ratio for matrix inversion is inherently large, independent of computational path.
A seemly small uncertainty response ratio using Gaussian elimination \cite{Numerical_Recipes}\cite{Linear_Algebra} is probably a path-dependent artifact; while the conventionally ``bad'' method of applying Formula \eqref{eqn: inverse matrix} for matrix inversion \cite{Numerical_Recipes}\cite{Linear_Algebra} is actually the correct one, except missing resulting uncertainty when using floating-point arithmetic.
All conventional path-dependent results are questionable.
Only statistical Taylor expansion presents a complete picture of resulting uncertainty for an analytic expression.

It is doubtful if Formula \eqref{eqn: matrix double inversion} still holds for uncertainty in statistical Taylor expansion, because it seems that uncertainty response ratio can only increase in matrix inversion according to Formula \eqref{eqn: inverse matrix 2 variance}.
On the other hand, because the bias in Formula \eqref{eqn: inverse matrix 2 bias} can be either positive or negative, it is expected that Formula \eqref{eqn: matrix double inversion} still holds for value.






\ifdefined\Verbose
\clearpage
\fi
\section{Moving-Window Linear Regression}
\label{sec: Moving-Window Linear Regression}

\subsection{Moving-Window Linear Regression Algorithm}

\iffalse
\begin{align*}
\alpha_{j} &= \sum_{X=-H}^{H - 1} Y_{j-H+X}; \\
\beta_{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X} \\
 	&= \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} + H Y_{j - 2 H - 1} + H Y_{j} - \sum_{X=-H}^{H - 1} Y_{j-H+X} \\
 	&= \beta_{j - 1} + H Y_{j - 2 H - 1} + H Y_{j} - \alpha_{j}; \\
\delta^2 \alpha_{j} &= \sum_{X=-H}^{H - 1} (\delta Y_{j-H+X})^2; \\
\delta^2 \hat{\beta}_{j} &\equiv \sum_{X=-H}^{H} X (\delta Y_{j-H+X})^2 = \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} 
		+ H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
	&= \delta^2 \hat{\beta}_{j -1} + H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2 \\
	&= \sum_{X=-H - 1}^{H - 1} (X + 1)^2 (\delta Y_{j-H+X})^2 - H^2 (\delta Y_{j-2H-1})^2 + H^2 (\delta Y_{j})^2 
		 - \sum_{X=-H}^{H - 1} (2 X + 1) (\delta Y_{j-H+X})^2 \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2
		 - 2 \sum_{X=-H}^{H} 2 X (\delta Y_{j-H+X})^2 - \delta^2 \alpha_{j} \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2 - 2 \delta^2 \hat{\beta}_{j}  - \delta^2 \alpha_{j};
\end{align*}
\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Value.pdf} 
\captionof{figure}{ 
Result of fitting $\alpha + \beta\; Y$ to a time-series input $Y$ within a moving window of size $2*2 + 1$.
The x-axis indicates the time index.
The y-axis on the left corresponds to the value of $Y$, $\alpha$, and $\beta$, while the y-axis on the right corresponds to the uncertainty of $\alpha$ and $\beta$.
The uncertainty for $Y$ is fixed at $0.2$.
In the legend, \textit{Unadjusted} refers to results obtained by directly applying Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} using variance arithmetic, whereas \textit{Adjusted} refers to using Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} for $\alpha$ and $\beta$ values but Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} for their variances.
}
\label{fig: Moving_Linear_Fit_Value}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Error.pdf} 
\captionof{figure}{ 
Error deviations of the $\alpha + \beta\; Y$ fit vs time index. 
The x-axis represents the time index.
The y-axis on the left corresponds to the error deviation.
For reference, the input time-series signal $Y$ is also plotted, with its values indicated on the y-axis on the right.
}
\label{fig: Moving_Linear_Fit_Error}
\end{figure}



Formula \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} provide the least-square line-fit of $Y = \alpha + \beta X$ between two set of data ${Y_j}$ and ${X_j}$, where $j$ is an integer index identifying $(X, Y)$ pairs in the sets \cite{Numerical_Recipes}.
\begin{align}
\label{eqn: linear regression 0}
\alpha &= \frac{\sum_{j} Y_{j} }{\sum_{j} 1}; \\
\label{eqn: linear regression 1}
\beta &= \frac{\sum_{j} X_{j} Y_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} Y_{j}}
    {\sum_{j} X_{j} X_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} X_{j} };
\end{align}

In many applications, data set ${Y_j}$ denotes an input data stream where $j$ represents the time index or sequence index.
${Y_j}$ is thus referred to as a time-series input, with $j$ corresponding to $X_j$.  
A moving window algorithm \cite{Numerical_Recipes} is applied within a small window centered on each $j$.  
For each calculation window, ${X_j = -H, -H+1 \dots H-1, H}$ where $H$ is an integer constant specifying the half width of the window.
This choice ensures $\sum_{j} X_{j} = 0$, which simplifies Formula \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} into Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.

\begin{align}
\label{eqn: time-series linear regression 0}
\alpha _{j} &= \alpha \; 2 H = \sum_{X=-H+1}^{H} Y_{j-H+X}; \\
\label{eqn: time-series linear regression 1}
\beta _{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X}; 
\end{align}
The values of $(\alpha _{j}, \beta _{j})$ can be derived from the previous values $(\alpha _{j-1}, \beta _{j-1})$, allowing Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} to be reformulated into the progressive moving-window calculation given by Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.
\begin{align}
\label{eqn: moving-window linear regression 0}
\beta _{j} &= \beta _{j-1} - \alpha _{j-1} + H \left(Y_{j-2H-1} + Y_{j} \right); \\
\label{eqn: moving-window linear regression 1}
\alpha_{j} &= \alpha _{j-1} - Y_{j-2H-1} + Y_{j};
\end{align}


\subsection{Variance Adjustment} 

\begin{align}
\label{eqn: moving-window linear regression variance 0}
\delta^2 \alpha_{j} &= \sum_{X=-H+1}^{H} (\delta Y_{j-H+X})^2 = \delta^2 \alpha_{j-1} - (\delta Y_{j-2H})^2 + (\delta Y_{j})^2; \\
\label{eqn: moving-window linear regression variance 1}
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2;
\end{align}
When the time series contains uncertainty, directly applying Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} results in a loss of precision since both formulas reuse each input multiple times, thereby accumulating the variance of that input with every reuse.
To prevent this, $\alpha_j$ and $\beta_j$ should still be calculated progressively using Formula \eqref{eqn: moving-window linear regression 1} and \eqref{eqn: moving-window linear regression 0}, respectively, while the variances should instead be computed using Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1}, respectively.
Formula \eqref{eqn: moving-window linear regression variance 1} is not progressive because the progressive form of $\delta^2 \beta_j$ is more  expensive in computation than Formula \eqref{eqn: moving-window linear regression variance 1}.

Figure \ref{fig: Moving_Linear_Fit_Value} shows that the input signal $Y_j$ consists of the following components:
\begin{enumerate}
\item An increasing slope for $j = 0 \dots 9$.

\item A decreasing slope for $j = 1 \dots 39$.

\item A sudden jump of magnitude $+10$ at $j=40$

\item A decreasing slope for $j = 41 \dots 49$.
\end{enumerate}
For each increment of $j$, the increasing and the decreasing rates are $+1$ and $-1$, respectively.

The specified input uncertainty is fixed at $0.2$.
Normal noise with a deviation of $0.2$  is added to the slopes, except for the segment $j = 10 \dots 19$ where Normal noise with a deviation of $2$ is introduced, representing actual uncertainty $10$ times larger than the specified uncertainty.

Figure \ref{fig: Moving_Linear_Fit_Value} also presents the results of the moving window fitting of $\alpha + \beta\; Y$ versus the time index $j$.
The fitted values of $\alpha$ and $\beta$ follow the expected behave, exhibiting a characteristic delay of $H$ in $j$.
When \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are applied to compute the uncertainties of $\alpha$ and $\beta$, both uncertainties increase exponentially with the time index $j$.
In contrast, when Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are used exclusively for value calculation, while Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} are applied for variance computation, the resulting uncertainties of $\alpha$ and $\beta$ are $\frac{\delta Y}{\sqrt{2H+1}}$, and $\frac{\delta Y}{\sqrt{\frac{H (H+1)(2H+1)}{3}}}$.
Both are less than the input uncertainty $\delta Y$, due to the averaging effect of the moving window.


\subsection{Unspecified Input Error}

To determine the error deviations of $\alpha$ and $\beta$, the fitting procedure is applied to multiple time-series data sets, each generated with independent noise realizations.
Figure \ref{fig: Moving_Linear_Fit_Error} illustrates the resulting error deviation as a function of the time index $j$, which remains close to $1$ except within the range $j = 10 \dots 19$ where the actual noise is ten times greater than the specified value.
This observation suggests that an error deviation exceeding 1 may indicate the presence of unspecified additional input errors beyond rounding errors, such as numerical errors in mathematical library functions.







\ifdefined\Verbose
\clearpage
\fi
\section{FFT (Fast Fourier Transformation)}
\label{sec: FFT}

\subsection{DFT (Discrete Fourier Transformation)}

For each signal sequence $h[k]$, where $k = 0, 1 \dots  N-1$, and $N$ is a natural number, the discrete Fourier transform (DFT) $H[n]$, for $n = 0, 1 \dots  N-1$, along with its inverse transformation, is defined by Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse}, respectively \cite{Numerical_Recipes}.
In these expressions, $k$ denotes the \emph{time index} while $n$ represents the \emph{frequency index}.
The frequency index and time index are not necessarily associated with physical time unit or frequency unit, respectively; rather, the naming convention provides a convenient way to distinguish between the two opposite domains in DFT: the waveform domain $h[k]$ and the frequency domain $H[n]$.
\begin{align}
\label{eqn: Fourier forward}
& H[n]=\sum_{k=0}^{N-1} h[k] \; e^{\frac{i 2\pi}{N} k n}; \\
\label{eqn: Fourier reverse}
& h[k]=\frac{1}{N} \sum_{n=0}^{N-1} H[n] \; e^{-\frac{i 2\pi}{N} n k};
\end{align}




\subsection{Modeling Errors of DFT}

\iffalse

\begin{align*}
h[k] &= \sin(f \frac{2 \pi}{N} k); \\
H[n] &= \sum_{k=0}^{N-1} \sin(f \frac{2 \pi}{N} k) \; e^{\frac{i 2\pi}{N} k n}
		= \sum_{k=0}^{N-1} \frac{1}{2i}\left( e^{(n + f) \frac{i 2\pi}{N} k} - e^{(n - f) \frac{i 2\pi}{N} k} \right)   \\
	&= \frac{1}{2i} \left( \frac{1 - e^{(n + f) \frac{i 2\pi}{N} N}}{1 - e^{(n + f) \frac{i 2\pi}{N}}}
		- \frac{1 - e^{(n - f) \frac{i 2\pi}{N} N}}{1 - e^{(n - f) \frac{i 2\pi}{N}}} \right) \\
&= \begin{cases}
  i N/2, & f \text{ is integer} \\
  N/\pi, & f \text{ is integer} + 1/2 \\
  \frac{1}{2} \frac{\sin(2\pi f - 2\pi \frac{f}{N}) + \sin(2\pi \frac{f}{N})-\sin(2\pi f) e^{-i 2\pi \frac{n}{N}}}{\cos(2\pi \frac{n}{N})-\cos(2\pi \frac{f}{N})} & \text{otherwise}
\end{cases}
\end{align*}

\fi

\begin{figure}
\includegraphics[height=2.5in]{FFT_Unfaithful.pdf} 
\captionof{figure}{
The DFT spectrum $H[n]$ of signal $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, as intensity (y-axis) and phase (embedded y-axis) versus frequency index $n \in [0, 18]$ (x-axis and embedded x-axis) for different signal frequency $f$ (legend).
This result agrees with both theoretical formula \cite{Prev_Precision_Arithmetic} and numerical computation from any mathematical libraries such as \textit{SciPy}.
}
\label{fig: FFT_Unfaithful}
\end{figure}


Although mathematically self-consistent, DFT implies a periodic boundary condition in the time domain \cite{Prev_Precision_Arithmetic}.
Consequently, it is only an approximation for the mathematically defined continuous Fourier transform (FT) \cite{Prev_Precision_Arithmetic}.
For example, the FT spectrum of a sine function is a Delta function at the signal frequency $f$ with a phase $\pi/2$ \cite{Numerical_Recipes}.
Figure \ref{fig: FFT_Unfaithful} shows the DFT spectra of the sine function $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, where $f$ is its signal frequency. 
If DFT is regarded as the digital implementation of FT, the spectra exhibit no modeling error only when the input signal frequency $f$ is an integer, and display varying degrees of modeling errors otherwise.
Because of these modeling errors, the use of DFT as the digital implementation of FT is questionable, even though such usage is ubiquitous, and fundamental to many areas of applied mathematics \cite{Numerical_Recipes}.

To avoid the modeling errors inherent in DFT, only Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} are used in this paper.



\subsection{FFT (Fast Fourier Transformation)}

\iffalse

Forward:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 + i 1 - 2 - i 3 , 0 - 1 + 2 - 3, 0 - i 1 - 2 + i 3; \\
 & [0, 1, 0, -1]:& [0, i 2, 0, - i 2]; \\
 & [1, 0, -1, 0]:& [0, 2, 0, 2];
\end{align*}

Reverse:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 - i 1 - 2 + i 3 , 0 - 1 + 2 - 3, 0 + i 1 - 2 - i 3; \\
 & [0, i 2, 0, - i 2]:& [0, 4, 0, -4]; \\
 & [0, 2, 0, 2]:& [4, 0, -4, 0];
\end{align*}

\fi


When $N = 2^{L}$, where $L$ is a natural number, the generalized Danielson-Lanczos lemma \cite{Numerical_Recipes} can be applied to DFT to produce  FFT \cite{Numerical_Recipes}. 
\begin{itemize}

\item For each output, each input is used only once, therefore no dependency problem arises when decomposing FFT into arithmetic operations as Formula \eqref{eqn: addition mean}, \eqref{eqn: addition variance}, \eqref{eqn: multiplication mean}, and \eqref{eqn: multiplication variance}.

\item When $L$ is large, the substantial volume of input and output data enables high-quality statistical analysis.

\item The computational complexity is proportional to $L$, since increasing $L$ by 1 adds an additional step involving a sum of multiplications.

\item Each step in the forward transformation doubles the variance, so the uncertainty mean increases with the FFT order $L$ as $\sqrt{2}^L$.
Because the reverse transformation divides the result by $2^L$, its uncertainty mean decreases with $L$ as $\sqrt{1/2}^L$.
Consequently, the uncertainty mean for the roundtrip transformation is therefore $\sqrt{2}^L \times \sqrt{1/2}^L = 1$.

\item The forward and reverse transformations are identical except for a sign difference, meaning that they are essentially the same algorithm, and any observed difference arises solely from the input data.  

\end{itemize}

In normal usage, forward and reverse FFT transforms differ in their data prospective of time domain versus frequency domain:
\begin{itemize}
\item The forward transformation converts a time-domain sine or cosine signal into a frequency-domain spectrum in which most values are zeros, causing its uncertainties to grow more rapidly. 

\item In contrast, the reverse transformation spreads the precise frequency-domain spectrum (where most values are zeros) back into a time-domain sine or cosine signal, causing its uncertainties to grow more slowly. 
\end{itemize}


\subsection{Testing Signals}

\iffalse

The Fourier transformation of a linear signal $h[n] = n$. 
Let $y \equiv i 2\pi n /N$:
\begin{align*}
& G(y) = \sum_{k=0}^{N-1}  e^{y k} = \sum_{k=0}^{N-1}  (e^y)^k = \frac{e^{N y} - 1}{e^y - 1}
 = \begin{cases} y = 0: \eqspace N \\ y \neq 0: \eqspace 0 \end{cases}; \\
H[n] &= \sum_{k=0}^{N-1} k e^{\frac{i 2\pi n}{N} k} = \sum_{k=0}^{N-1} k e^{y k} 
 = \frac{d G}{y} = \frac{N e^{N y}}{e^y - 1} - \frac{e^{N y} - 1}{(e^y - 1)^2} e^y = \frac{N}{e^y - 1} \\
 &= \frac{N}{\cos(y) - 1 + i \sin(y)} = \frac{N}{2} \frac{\cos(y) - 1 -  i \sin(y)}{1 - \cos(y)} 
  = - \frac{N}{2}(1 + i \frac{2 \sin(\frac{y}{2}) \cos(\frac{y}{2})}{2 \sin^2(\frac{y}{2})}) \\
 &= \begin{cases} y = 0: \eqspace \frac{N^2}{2} \\ y \neq 0: \eqspace - \frac{N}{2}(1 + i \frac{1}{\tan(\frac{n}{N} \pi)}) \end{cases};
\end{align*}

\fi


The following signals are used for testing:
\begin{itemize}
\item \emph{Sin}: $h[k] = \sin(2\pi k f/N), f = 1, 2, ... N/2 -1$.

\item \emph{Cos}: $h[k] = \cos(2\pi k f/N), f = 1, 2, ... N/2 -1$.

\item \emph{Linear}: $h[k] = k$, whose DFT is given by Formula \eqref{eqn: Fourier spec for linear}.
\begin{align}
& y \equiv i 2\pi \frac{n}{N}: \eqspace G(y) = \sum_{k=0}^{N-1}  e^{y k} = \frac{e^{N y} - 1}{e^y - 1}; \nonumber \\
\label{eqn: Fourier spec for linear}
H[n] &= \frac{d G}{d y} = \begin{cases} n = 0: \eqspace \frac{N (N-1)}{2} \\ n \neq 0: \eqspace
 - \frac{N}{2}(1 + i \frac{\cos(n \frac{\pi}{N})}{\sin(n \frac{\pi}{N})}) \end{cases};
\end{align}

\end{itemize}

Empirically, except when using trigonometric library directly for FFT transformations of clean Sin and Cos signals:
\begin{itemize}
\item The results obtained from Sin and Cos signals are statistically indistinguishable.

\item Similarly, the results from Sin and Cos signals at different frequencies does not show significant differences statistically except in the situation of numerical error resonance.
\end{itemize}
Therefore, the results for Sin and Cos signals across all frequencies are pooled together for statistical analysis, under the unified category \emph{Sin/Cos} signals.


\subsection{Trigonometric Library Errors}

\begin{figure}[p]
\includegraphics[height=2.5in]{Sin_Diff.pdf} 
\captionof{figure}{
Difference between library and Quart $\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
The uncertainties of the Quart $\sin(x)$ is $\sin(x)$ ULP, which shows a periodicity of $\pi$.
}
\label{fig: Sin_Diff}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{Cot_Diff.pdf} 
\captionof{figure}{
Difference between the library and the Quart $\cos(x)/\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
}
\label{fig: Cot_Diff}
\end{figure}


Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} restrict the use of $\sin(x)$ and $\cos(x)$ to $x = 2\pi j /2^L$, where $L$ is the FFT order.
To minimize numerical errors in computing $\sin(x)$, the following \emph{indexed sine} can be used in place of  standard library sine functions:
\begin{enumerate}
\item Instead of a floating-point value $x$ as input for $\sin(x)$, an integer index $j$ defines the input as $\sin(\pi j/2^L)$, thereby eliminating the floating-point rounding error of $x$.

\item The values of $\sin(\pi j/2^L), j \in [0, 2^{L-2}]$ are library sine directly, while $\sin(\pi j/2^L), j \in [2^{L-2}, 2^{L-1}]$ are computed from library $\cos(\pi (2^{L - 1} - j)/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended from $j \in [0, 2^{L-1}]$ to $j \in [0, 2^{L + 1}]$ by exploiting the symmetry of $\sin(\pi j/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended to all the integer value of $j$ by leveraging the periodicity of $\sin(2\pi j/2^L)$.

\end{enumerate}
The constructed indexed $\sin(x)$ is referred to as the \emph{Quart} indexed sine function.
In contrast, the direct use of the standard library $\sin(x)$ is referred to as the \emph{Library} sine function.

Because the the Quart sine function strictly preserves the symmetry and periodicity of sine function, it provides numerical accuracy compared to Library function.
\begin{itemize}
\item Figure \ref{fig: Sin_Diff} shows that the value difference between the Quart and Library $\sin(x)$ and the Quart $\sin(x)$ increases approximately linearly with $|x|$.

\item Figure \ref{fig: Cot_Diff} shows the value difference between the Quart and Library $\cos(x)/\sin(x)$ also increases roughly linearly with $|x|$, but are $10^2$ times larger than those observed for $\sin(x)$.
Therefore, the linear spectrum in Formula \eqref{eqn: Fourier spec for linear} may contain significant numerical errors when computed using library sine functions.
\end{itemize}

For both sine functions, the uncertainty of each $\sin(x)$ is assumed to equal its ULP, which is displayed in Figure \ref{fig: Sin_Diff}. 
Because the Quart sine function has minimal Taylor expansion error due to its minimal range of $x$ in calls to the library $\sin(x)$, its true numerical errors are likely smaller than ULP, therefore its uncertainty is sightly overestimated.
In contrast, Figure \ref{fig: Sin_Diff} indicates that the Library sine function exhibits underestimated uncertainties.





\subsection{Using Quart Sine for Sin/Cos Signals}

\begin{figure}[p]
\includegraphics[height=2.75in]{FFT_Sin_Clean_6_3_Spec_Indexed.pdf} 
\captionof{figure}{
FFT value error specturm of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using the Quart sine function after the forward transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Sin_Clean_6_3_Spec_Indexed}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.75in]{FFT_Sin_Clean_6_3_Wave_Indexed.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using the the Quart sine function after the reverse transformation. The legend distinguishes between the uncertainty and the value error.
The x-axis represents the time index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Sin_Clean_6_3_Wave_Indexed}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_vs_Order_Indexed.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Sin/Cos signal versus FFT order (x-axis) and transformation types (legend) using the Quart sine function.
The variance arithmetic is implemented using python.
}
\label{fig: FFT_SinCos_Clean_vs_Order_Indexed}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Histograms of normalized errors of Sin/Cos signals for forward, reverse and roundtrip transformations (legend) using the Quart sine function.
The FFT order is $18$.  
}
\label{fig: FFT_SinCos_Clean_Histo_Indexed}
\end{figure}


Using the Quart sine function, for a sine wave with a frequency of $3$, Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Indexed} displays the value error spectrum for the forward transformation, while Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Indexed} presents the value error waveform for the reverse transformation.
\begin{itemize}
\item In the forward transformation, the value errors are slightly smaller than the corresponding result uncertainties, with an error deviation of $0.60$.
In the reverse transformation, the error deviation is $0.55$.
Both error deviations are less than $1$, confirming a slight overestimation of the sine uncertainty by ULP.

\item The uncertainty mean of the forward transformation is $7$ times larger than that of the reverse transformation.
Even with this difference, the uncertainty tracks the corresponding value error effectively.

\item The value error waveform for the roundtrip transformation is qualitatively similar to that in  Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Indexed} with a similar error deviation of $0.55$.
\end{itemize}

Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed} illustrates that the forward transformation differs from the reverse transformation significantly.
\begin{itemize}
\item In the forward transformation, error deviations reach their stable values around $0.65$ rapidly once the FFT order $L \geq 6$. 

\item In contrast, in the reverse transformation, error deviations stabilize more slowly with increasing FFT order, approaching their stable values around $0.90$ only when $L \geq 12$.
This slow convergence of normalized errors in the reverse transformation is attributed to its input data which consists entirely of precise zeros except at the frequency indexes.

\item At FFT order $18$, Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed} shows that the normalized error distribution for the reverse transformation is wider than that of the forward transformation, confirming the larger error deviation.
\end{itemize}
The value error increases at least $10^2$ times faster in the forward transformation than in the reverse transformation.
Despite these differences, error deviations are $0.65$ and $0.90$, suggesting that in variance arithmetic, uncertainties track value errors effectively in all cases, achieving proper coverage.



\subsection{Sensitivity to Quart sine functions}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_vs_Order_Cpp.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Sin/Cos signal versus FFT order (x-axis) and transformation types (legend) using the Quart sine function.
The variance arithmetic is implemented using C++.
}
\label{fig: FFT_SinCos_Clean_vs_Order_Cpp}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Quart_Difference.pdf} 
\captionof{figure}{
Histograms of normalized errors between the Quart sine functions of C++, Java, and Python in the unit of  least significant bits.
}
\label{fig: Quart_Difference}
\end{figure}


The reverse transformation of Sin/Cos signals is very sensitive to the underlying Quart sine function.
If variance arithmetic is implemented using C++ instead of Python, Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Cpp} shows that the error deviations of the reverse transform quickly reaches the stable values around $0.89$, while the forward transformation remains similar to its counterpart in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}.
Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Cpp} also shows identical uncertainty means to their counterparts in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}.
Figure \ref{fig: Quart_Difference} shows that the difference of the Quart sine functions between Python and C++ is only about $1\%$ LSBs.
Figure \ref{fig: Quart_Difference} also reveals that the difference of the Quart sine functions between Java and C++ is only about $0.1\%$ LSBs.
Indeed, the results between Java and C++ are much more similar.
Because the Python implementation gives smaller error deviations, its Quart sine function has less numerical errors on average.
For discussion simplicity, the rest results are only based on the python implementation.



\subsection{Using Library Sine for Sin/Cos Signals}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Spec_Lib.pdf} 
\captionof{figure}{
FFT value error specturm of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the forward transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Sin_Clean_6_3_Spec_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Wave_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the reverse transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the time index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Sin_Clean_6_3_Wave_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_vs_Order_Lib.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Sin/Cos signal versus FFT order (x-axis) and transformation types (legend) computed using the Library sine function.
}
\label{fig: FFT_SinCos_Clean_vs_Order_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Histograms of normalized errors of Sin/Cos signal for forward, reverse and roundtrip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_SinCos_Clean_Histo_Lib}
\end{figure}



Using the Library sine function, for a sine wave with a frequency of $3$, Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Lib} displays the value error spectrum for the forward transformation, while Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib} presents the vale error waveform for the reverse transformation:
\begin{itemize}
\item The uncertainties are identical to the corresponding values obtained using the Quart sine function, since the input uncertainties are the same for both sine functions.
This identical relationship persists across all FFT orders when comparing the corresponding uncertainty means in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed} and \ref{fig: FFT_SinCos_Clean_vs_Order_Lib}.

\item The error deviations for the forward and reverse transformations are $3.7$ and $3.0$ respectively, confirming that the numerical errors in the Library sine function are greater than those using the Quart sine function, as illustrated in Figure \ref{fig: Sin_Diff}.

\item
When compared with Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed}, Figure \ref{fig: FFT_SinCos_Clean_Histo_Lib} also confirms that at FFT order $18$, the normalized errors distribution is significantly broader when using the Quart sine function.

\item
Due to numerical errors in the Library sine function, error deviations reach their stable values much quicker with FFT order in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Lib} that these in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}.

\item
The stable error deviations using the Library sine function are much larger than their counterparts using Quart library functions: $10.1 > 0.65$ for the forward transformation, $6.3> 0.90$ for the reverse transformation, and $1.47 > 0.94$ for the roundtrip transformation.
\end{itemize}
Using the Library sine function, variance arithmetic barely achieves proper coverage for sine and cosine signals.



\subsection{Numerical Error Resonance Using Library Sine}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Cos_Clean_6_6_Wave_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\cos(6 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the reverse transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Cos_Clean_6_6_Wave_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Roundtrip_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the roundtrip transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, while the y-axis represents both the uncertainty and the value error.
}
\label{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Lib_Forward_Error_vs_Freq_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) of FFT forward transformation of $sin(f \frac{2 \pi}{2^L} j)$ versus frequency $f$ (x-axis) and FFT Order $L$ (y-axis).
}
\label{fig: Lib_Forward_Error_vs_Freq_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Lib_Reverse_Error_vs_Freq_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) of FFT reverse transformation of $sin(f \frac{2 \pi}{2^L} j)$ versus frequency $f$ (x-axis) and FFT Order $L$ (y-axis).
}
\label{fig: Lib_Reverse_Error_vs_Freq_Order}
\end{figure}

In the reverse transformation, value errors exhibit a clear trend of increasing with the time index, as shown in Figure \ref{fig: Lib_Forward_Error_vs_Freq_Order}. 
These large value errors appear systematic rather than random and visually resemble a resonant pattern.
Similar increases are observed at in other frequencies and FFT orders, as well as in computational results obtained using mathematical libraries such as \textit{SciPy}.
For example, Figure \ref{fig: FFT_Cost_Clean_6_6_Wave_Lib} shows that the FFT value error waveform of $\cos(6 \frac{2 \pi}{2^6} j)$ has  much stronger resonant pattern than that of $\sin(3 \frac{2 \pi}{2^6} j)$ in Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}.
In the forward transformation, although there is no visible resonant pattern in FFT value error specturm, the intensity of value errors increase with FFT order.
In contrast, such resonance is absent for the roundtrip transformation as shown in Figure \ref{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}.
Also, it is completely absent when using the Quart sine function, as seen in Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Indexed} and \ref{fig: FFT_Sin_Clean_6_3_Wave_Indexed}.
Figure \ref{fig: Lib_Forward_Error_vs_Freq_Order} and Figure \ref{fig: Lib_Reverse_Error_vs_Freq_Order} demonstrate that the error deviations increase with sine or cosine frequency.
Figure \ref{fig: Sin_Diff} indicates that the numerical errors using the Library sine function increase with a periodicity of $\pi$, which may resonate with a signal whose periodicity of an integer multiply of $\pi$, producing the resonant pattern in Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Lib}, \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}, and \ref{fig: FFT_Cos_Clean_6_6_Wave_Lib}.
At higher frequency, the resonant beats between the signal and the numerical errors in the Library sine function become stronger.
To suppress this numerical error resonances, an input noise level of approximately $10^{-14}$ input noise must be added to the sine or cosine signals.



\subsection{Using Quart Sine for Linear Signal}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_vs_Order_Indexed.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Linear signal versus FFT order (x-axis) and transformation types (legend) computed using the Quart sine function.
}
\label{fig: FFT_Linear_vs_Order_Indexed}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Histograms of normalized errors of Linear signals for forward, reverse and roundtrip transformations (legend) computed using the Quart sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Indexed}
\end{figure}

Figure \ref{fig: FFT_Linear_vs_Order_Indexed} shows that using the Quart sine function, the result uncertainties can track  the value errors of Linear signals with proper coverage.
Because the input to the reverse transformation no longer consists predominantly of precise zeros, the output uncertainty increases more rapidly with FFT order than its counterpart in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}.
Such increase results in larger error deviations: At FFT order $18$, $0.89 > 0.65$ for the forward transformation, $1.08 > 0.90$ for the reverse transformation, $1.67 > 0.90$ for the roundtrip transformation, and wider distributions of the normalized value errors in Figure \ref{fig: FFT_Linear_Clean_Histo_Indexed} versus the corresponding histograms in Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed}.
Using the Quart sine function, variance arithmetic can provide proper coverage for Sin signals, Cos signals, and Linear signal.




\subsection{Using Library Sine for Linear Signal}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_vs_Order_Lib.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Linear signal versus FFT order (x-axis) and transformation types (legend) computed using the Library sine function.
}
\label{fig: FFT_Linear_vs_Order_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Histograms of normalized errors of Linear signals for forward, reverse and roundtrip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Lib}
\end{figure}

As shown by the difference between Figure \ref{fig: Sin_Diff} and \ref{fig: Cot_Diff}, Linear signals computed using the Library sine function can introduce unspecified numerical errors up to $10^3$ times larger into the results through the Library $\cos(x)/sin(x)$.
The key question is whether variance arithmetic can effectively track these additional unspecified errors.

Figure \ref{fig: FFT_Linear_vs_Order_Lib} shows that proper coverage can no longer be achieved, as the value error increases faster than the uncertainty with rising FFT order.
Figure \ref{fig: FFT_Linear_Clean_Histo_Lib} demonstrates that the normalized error distribution for the reverse transformation is no longer effectively bounded within three standard deviations.
Although the normalized error distribution for the forward transformation using the Library sine function appears visually  similar to that obtained using the Quart sine function in Figure \ref{fig: FFT_Linear_Clean_Histo_Lib}, a detailed analysis reveals that the normalized error distribution contains extreme values at its two long tails, such as $-2.8\;10^3$ and $3.9\;10^3$ as the minimal and maximal of the normalized errors, respectively.
The presence of these extreme values is consistent with those delta-like numerical errors observed in Figure \ref{fig: Cot_Diff}.

The observed difficulty of variance arithmetic in tracking Linear signals computed using the Library sine function suggests that variance arithmetic may fail when the input contains excessive unspecified numerical errors.
A potential solution is to develop a new sine function library implemented directly within the variance arithmetic framework, ensuring that all numerical calculations are performed with explicit uncertainty tracking.



\subsection{Ideal Coverage}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_1e-3_vs_Order_Lib.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty mean (right y-axis) of Linear signals versus FFT order (x-axis) and transformation types (legend) computed using the Library sine function.
}
\label{fig: FFT_Linear_1e-3_vs_Order_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_1e-3_Histo_Lib.pdf} 
\captionof{figure}{
Histograms of normalized errors for Linear signals with $10^{-3}$ input noise for forward, reverse and roundtrip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_1e-3_Histo_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}


\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|} 
\hline 
Code         & Signal     & Sine function & Forward       & Reverse       & Roundtrip    \\ 
\hline 
Python      & Sin/Cos   & Quart           & $10^{-16}$  & $10^{-12}$ & $10^{-14}$ \\ 
\hline 
Java, C++ & Sin/Cos   & Quart           & $10^{-16}$  & $10^{-12}$ & $10^{-14}$ \\ 
\hline 
Python      & Sin/Cos   & Library         & $10^{-16}$  & $10^{-11}$ & $10^{-12}$ \\ 
\hline 
Java, C++ & Sin/Cos   & Library         & $10^{-16}$  & $10^{-11}$ & $10^{-13}$ \\ 
\hline 
Python      & Linear     & Quart           & $10^{-11}$  & $10^{-7}$   & $10^{-8}$ \\
\hline 
Java, C++ & Linear     & Quart           & $10^{-11}$  & $10^{-7}$   & $10^{-8}$ \\
\hline 
Python      & Linear     & Library         & $10^{-11}$  & $10^{-3}$   & $10^{-8}$  \\
\hline 
Java, C++ & Linear     & Library         & $10^{-11}$  & $10^{-3}$   & $10^{-8}$  \\
\hline 
\end{tabular}
\captionof{table}{
The measured minimal required noise to achieve ideal coverage for FFT transformations at FFT order $18$ for different signals and sine functions.
}
\label{tbl: ideal coverage}
\end{table}


Adding noise to the input can suppress unspecified input errors and improve the portion of specified versus unspecified input uncertainties.
Table \ref{tbl: ideal coverage} shows the measured minimal required noise to achieve ideal coverage for FFT transformations at FFT order $18$ for different signals and sine functions.

Figure \ref{fig:  FFT_Linear_1e-3_vs_Order_Lib} illustrates the error deviations and uncertainty means when applying an input noise of $10^{-3}$ to a Linear signal when using the Library sine function result in ideal coverage for all the transformations:
\begin{itemize}
\item As expected, the result uncertainty means for the forward transformations increase with FFT order $L$ as $\sqrt{2}^L$.

\item As expected, the result uncertainty means for the reverse transformations decrease with FFT order $L$ as $1/\sqrt{2}^L$.

\item As expected, the result uncertainty means for the roundtrip transformations remains equal to the corresponding input uncertainties of $10^{-3}$.

\item As expected, the result error deviations for the forward and reverse transformations remain constant at $1$.

\end{itemize}

Figure \ref{fig: FFT_Linear_1e-3_Histo_Lib} presents the corresponding histogram.
As expected, the normalized errors for the forward and reverse transformations follow Normal distributions, while those for the roundtrip transformation are Delta distributed around zero, indicating perfect recovery of the input uncertainties.
According to Figure \ref{fig: FFT_Linear_1e-3_Histo_Lib}, adding either Gaussian or white noise to input results in the same Gaussian distribution of the normalized errors.

Additionally, the result uncertainty means for both forward and reverse transformations are linearly proportional to the input uncertainties, as expected since FFT is a linear algorithm \cite{Numerical_Recipes}.

The range of ideal coverage depends on how accurately the specified input uncertainty represents the actual input noise.  
For Linear signals computed using the Library sine function, Figure \ref{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order} present the error deviation versus the added noise and FFT order for the forward and reverse transformations, respectively.
Ideal coverage corresponds to the region where the error deviation equals $1$.
Outside this region, proper coverage cannot be achieved for the reverse transformation.
Because uncertainties grow more slowly in the reverse transformation than in the forward transformation, the reverse transformation exhibits a smaller ideal coverage region.
Furthermore, as numerical errors increase with computational load, the range of input noise that produces ideal coverage decreases with increasing FFT order.
At sufficiently high FFT orders, visually beyond FFT order $25$ for the reverse transformation, ideal coverage may no longer be achievable.
Although FFT is widely regarded as one of the most robust numerical algorithms, and generally insensitive to input errors, it can still fail due to numerical errors in the Library sine function.
Such deterioration in calculation accuracy is not easily detectable when using conventional floating-point arithmetic.

In contrast, for Linear signals computed using the Quart sine functions, Figure \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order} show the results for the forward and reverse transformations, respectively. 
The ideal coverage region is significantly larger, and proper coverage is also achieved in adjacent regions.
The forward transformation exhibits a broader ideal coverage region than the reverse transformations.
While the forward transformation shows consistent over-estimation of the value errors in the proper coverage region, the reverse transformation demonstrates both over-estimation and under-estimation of the value errors in the proper coverage region. 

As a comparison, because Sin/Cos signals have fewer numerical calculation errors, using either Quart or the Library sine functions, the ideal coverage region is achieved once the added noise is large enough to cover the effect of rounding errors, and this condition is almost independent of FFT orders. 
The key difference is that when using the Quart sine functions, the error deviations within the proper coverage region differ from $1$ only marginally.

Adding noises has another obvious benefit: Different implementations of the variance arithmetic have almost identical result when the input noise is larger than $10^{-16}$. 
For example, Figure \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order} are almost identical using either C++, or Java, or Python. 


\subsection{Input Uncertainty Recovery}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Roundtrip_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the roundtrip transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Lib_Roundtrip_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the roundtrip transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_SinCos_Lib_Roudtrip_ErrorDev_vs_Noise_Order}
\end{figure}


With input uncertainty unspecified, the value error waveform for the roundtrip transformation is qualitatively similar to that of the the reverse transformation, with error deviations close to $1$, such as in Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}, \ref{fig: FFT_SinCos_Clean_vs_Order_Cpp}, \ref{fig: FFT_SinCos_Clean_vs_Order_Lib}, and \ref{fig: FFT_Linear_vs_Order_Indexed}, even when proper coverage can no longer achieve for both forward and reverse transformations in Figure \ref{fig: FFT_Linear_vs_Order_Lib}.
With input uncertainty unspecified, roundtrip transformation can always recover input uncertainty statistically.

With input uncertainty specified, roundtrip transformation can always recover input uncertainty numerically, and the quality of the recover increases with the quality of input uncertainty specification.
For example, when using the Library sine function:
\begin{itemize}
\item Figure \ref{fig:  FFT_Linear_1e-3_vs_Order_Lib} shows that the error deviations for the roundtrip transformation is less than $1$, suggesting that each input uncertainty is recovered numerically.
These error deviations increases with FFT orders, because the error deviations without added noises increases with the FFT orders according to Figure \ref{fig: FFT_Linear_vs_Order_Lib}.

\item As a comparison, for Sin/Cos signal, because Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Lib} shows little increase with the FFT orders of the error deviations without added noises, the improvement of the numerical recovering in Figure \ref{fig: FFT_SinCos_Lib_Roudtrip_ErrorDev_vs_Noise_Order} is almost independent of FFT orders.
\end{itemize} 
In both cases, error deviations decreases with the strength of the added noise.



\subsection{Prec Sine Function}

\begin{figure}[p]
\includegraphics[height=2.5in]{Sin_Error_Quart_Prec.pdf} 
\captionof{figure}{
Value errors of $\sin(x)^2 + \cos(x)^2 - 1, x = \pi j  /2^L$ (y-axis) versus $x$ (x-axis) and $L$ computed using either the Prec or the Quart sine function (legend).
}
\label{fig: Sin_Error_Quart_Prec}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_vs_Order_Prec.pdf} 
\captionof{figure}{
Error deviations (left y-axis) and uncertainty mean (right y-axis) of Linear signals versus FFT orders (x-axis) and transformation types (legend) computed using the the Prec sine function.
}
\label{fig: FFT_SinCos_Clean_vs_Order_Prec}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Prec.pdf} 
\captionof{figure}{
Histograms of normalized errorsfor forward, reverse and roundtrip transformations (legend) of Linear signals computed using the the Prec sine function.
}
\label{fig: FFT_Linear_Clean_Histo_Prec}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Prec_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for reverse transformations of Linear signals computed using the the Prec sine function.
}
\label{fig: FFT_Linear_Prec_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}



The Quart sine function slightly overestimates the uncertainty, while the Library sine function underestimates it significantly.
This raises the question of whether a sine function with a more accurate uncertainty quantification can be developed.
If the Quart sine function is constructed using the 128-bit floating-point library of gcc and then converted to 64-bit values, the difference between each 128-bit value and its corresponding 64-bit value can be used as the uncertainty for the 64-bit representation.
The resulting indexed sine function is referred to as the \emph{Prec} sine function.

Figure \ref{fig: Sin_Error_Quart_Prec} compares the value errors and the uncertainties of  $\sin(x)^2 + \cos(x)^2 - 1$ between the Prec sine function and the Quart sine function, and demonstrates that the Prec sine functions has smaller uncertainties numerically than the Quart sine functions, but larger value errors statically, presumably due to rounding errors introduced by the casting.
This difference indicates that casting a 128-bit floating-point value to a 64-bit floating-point value is less accurate in the 64-bit floating-point arithmetic, and the Prec sine function is expected to under-perform the Quart sine function due to under-estimation of the value errors by the uncertainties.

For Sin/Cos signals, Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Prec} is very similar Figure \ref{fig: FFT_SinCos_Clean_vs_Order_Indexed}, provided that the error deviations using Prec sine functions are about $1.7$ times larger than their counterparts using the Quart sine functions.

For Linear signals, when comparing Figure \ref{fig: FFT_Linear_Clean_Histo_Prec} and \ref{fig: FFT_Linear_Clean_Histo_Indexed} for the normalized value error distributions:
\begin{itemize}
\item For the forward transformations, using either Prec or Quart sine function has very similar distributions.

\item For the reverse transformations, using Prec sine function results in wider distributions that using Quart sine function.
\end{itemize}
Although the ideal coverage regions for the reverse transformations remains identical in Figure \ref{fig: FFT_Linear_Prec_Reverse_ErrorDev_vs_Noise_Order} when compared with Figure \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}, using Prec sine function results in larger error deviations in the proper coverage regions.
Thus, using Prec sine function is slightly inferior than using Quart sine function.
In another word, casting values from higher floating-point representation to a lower floating-point arithmetic is not necessarily superior.






\subsection{Calculation Response}

\begin{figure}
\includegraphics[height=2.5in]{FFT_Performance.pdf} 
\captionof{figure}{
The FFT execution time (y-axis) versus the FFT order (x-axis) for the implementation using either C++, Java, or Python (legend), with each calculation consisted of $36$ roundtrip FFT transformations.
The C++ executable is complied with \textit{-O3 -march=native} optimization.  
Without such optimization, the C++ \textit{-O0} implementation is about a order of magnitude slower than the Java implementation.
}
\label{fig: FFT_Performance}
\end{figure}

Figure \ref{fig: FFT_Performance} plots the execution time for the roundtrip FFT calculations using either C++, Java, or Python.
As expected, in Figure \ref{fig: FFT_Performance}, the execution time increases exponentially with the FFT order $L$ as $2^L$ for all the three implementations when the FFT order $L > 10$, validating the calculation response.

At first, it is surprising that in Figure \ref{fig: FFT_Performance} the Java implementation is 3 times faster than the C++ implementation which has been fully optimized for speed.
Because FFT contains a large amount of independent and logically identical branches, the dynamic just-in-time Java compiler \cite{J9VM} can optimize according to data in real-time, compile the bytecodes into native machine codes, and parallelize the execution to the multiple cores and GPUs on a modern computer.
Perhaps it is necessary to rethink multi-threading as a way to manually increase the time performance, because the dynamic just-in-time optimization of Java is superior to the static optimization of C++ for repeated and heavy computations with preload data such as FFT. 

 


\subsection{Summary}

The Library sine functions implemented using conventional floating-point arithmetic have been shown to contain numerical errors equivalent up to $10^{-3}$ of the input precision in the worst cases of FFT transformations.
These $\sin(x)$ numerical errors increase periodically with $x$, potentially resonating with periodic input signals.
Computational result obtained with floating-point arithmetic are highly sensitive to the input data, the scale of computation, and the numerical errors in library functions.
This implies that a small-scale test cannot reliably qualify or predict the behavior of large-scale calculations, but needs rigorous analysis for the ideal or proper coverage regions in each parameter space.
The impact of numerical errors within mathematical libraries has not received sufficient attention.
Moreover, such errors in floating-point arithmetic are inherently difficult to detect and quantify.

Variance arithmetic offers a robust alternative, as its computed values closely align with those obtained from conventional floating-point arithmetic while its associated uncertainties systematically trace all input errors. 
Furthermore, its resulting error deviations provide an objective basis for classifying of calculation quality as ideal, proper, or suspicious.







\ifdefined\Verbose
\clearpage
\fi
\section{Regressive Generation of Sin and Cos}
\label{sec: recursion}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Regression_Sin.pdf}
\captionof{figure}{
The uncertainties (y-axis) of $\sin(\pi j/2^{18})$ versus $j$ (x-axis) and $L$ using either Quart or regressive sine function (legend).
}
\label{fig: Regression_Sin}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Regression_Error.pdf}
\captionof{figure}{
Uncertainties, value errors (left y-axis), and error deviation (right y-axis) for $\sin(x)^2 + \cos(x)^2 - 1, x \in [0, \pi/4]$ versus regression order (x-axis) when compared with the Quart sine function (legend)
}
\label{fig: Regression_Error}
\end{figure}



Formula \eqref{eqn: phase sin} and Formula \eqref{eqn: phase cos} calculate $\sin(\pi j/2^L), \cos(\pi j/2^L), j = 0 \dots 2^{L - 2}$ repressively for regression order $L = 0 \dots 17$ starting from Formula \eqref{eqn: phase boundary}.
Formula \eqref{eqn: regr error} shows that such regression guarantees both $\sin(x)^2 + \cos(x)^2 = 1$ and $\sin(2x) = 2\sin(x) \cos(x)$, so that value errors will not accumulate when the regression order increases.
\begin{align}
\label{eqn: phase boundary}
& \sin(0) = \cos(\frac{\pi}{2}) = 0;\eqspace \sin(\frac{\pi}{2}) = \cos(0) = 1; \\
\label{eqn: phase sin}
& \sin(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 - \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 - \cos(\alpha) \cos(\beta) + \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: phase cos}
& \cos(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 + \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 + \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: regr error}
& \sin(\alpha + \beta) = 2 \sin(\frac{\alpha + \beta}{2}) \cos(\frac{\alpha + \beta}{2}) = \sqrt{1 - \cos(\frac{\alpha + \beta}{2})^2};
\end{align}

Formula \eqref{eqn: phase sin} is not suitable for computing $\sin(x)$ as $x \rightarrow 0$ because it suffers from behavior analogous to catastrophic cancellation, resulting in excessive uncertainty.
In Figure \ref{fig: Regression_Sin}, the Quart sine function exhibits uncertainties proportional to $\sin(\frac{\pi}{2^L})$, whereas the regression sine function shows increasing uncertainties as $x \rightarrow 0$.
Unlike catastrophic cancellation in floating-point arithmetic, variance arithmetic uses very coarse precision to signal that the regression algorithm is unfit to compute $\sin(x \rightarrow 0)$.

Figure \ref{fig: Regression_Error} evaluates the  Quart and Regression sine functions using $\sin(x)^2 + \cos(x)^2 - 1 = 0$.
The figure shows that the resulting uncertainties remain nearly constant and independent of the regression order $L$ when $L \geq 4$ for both the Regression sine and the Quart sine functions.
It also demonstrates that the value errors for both are comparable.
The Regression sine function has error deviations closer to $1$.





\ifdefined\Verbose
\clearpage
\fi
\section{Conclusion and Discussion}
\label{sec: conclusion and discussion}

\subsection{Summary of Statistical Taylor Expansion}

The starting point of statistical Taylor expansion is the assumption that all input variables obeys uncorrelated uncertainties. 
This is generally regarded as a reasonable and practical statistical requirement for input data \cite{Prev_Precision_Arithmetic}.
Once this assumption is satisfied, statistical Taylor expansion quantifies uncertainty as the deviation of the value errors and traces  track the variable dependencies in intermediate steps using standard statistical methods.
While this approach eliminates the dependency problem, it also requires a more rigorous process to determine the resulting mean and deviation for any analytic expression.
Furthermore, it can reject invalid calculations on unified statistical and mathematical grounds and explicitly incorporates sample counts and the corresponding underlying uncertainty distributions into the statistical analysis.



\subsection{Summary of Variance Arithmetic}

Variance arithmetic simplifies statistical Taylor expansion by assuming that the sample counts for all inputs are sufficiently large to achieve ideal variance.
Ill-formed problems can invalidate the resulting variance in several ways by producing non-convergent, unstable, negative, infinite, or unreliable results.

The presence of ideal coverage is a necessary condition for a numerical algorithm in variance arithmetic to be considered correct. 
Ideal coverage also defines the algorithms optimal applicable range. 
Under ideal coverage, the calculated uncertainty equals the deviation of the value errors, and the normalized errors follow either a Normal or Delta distribution depending on the context.

Variance arithmetic provides proper coverage for floating-point rounding errors.

The applicability of variance arithmetic has been demonstrated across a wide range of computational scenarios, including analytic calculations, progressive computation, regressive generalization, polynomial expansion, statistical sampling, and transformations.

The code and analysis framework for variance arithmetic are available as an open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}.

\iffalse

\begin{table}[H]
\centering
\begin{tabular}{|c|c|} 
\hline 
Context & Error Deviation \\
\hline 
Adjugate Matrix & $4$ \\
\hline 
$\sqrt[p]{x^p} - x$ & $0.55$ \\
\hline 
Forward FFT of Sin/Cos signal with indexed sine & $0.65$ \\
\hline 
Reverse FFT of Sin/Cos signal with indexed sine & $0.85$ \\
\hline 
Roundtrip FFT of Sin/Cos signal with indexed sine & $0.87$ \\
\hline 
Forward FFT of Linear signal with indexed sine & $0.8$ \\
\hline 
Reverse FFT of Linear signal with indexed sine & $1.0$ \\
\hline 
Roundtrip FFT of Linear signal with indexed sine & $1.25$ \\
\hline 
Regressive Calculation of sin and cos & $0.55$ \\
\hline 
\end{tabular}
\captionof{table}{
The error deviations for floating-point rounding errors in different contexts.
}
\label{tbl: floating-point rounding erros}
\end{table}

\fi



\subsection{Improvements Needed}

This paper presents variance arithmetic which is still in its early stage of development. 
From a theoretical standpoint, several important questions still need to be addressed.

Library mathematical functions should be recalculated using variance arithmetic, so that each output value is accompanied by its corresponding uncertainty.
Without this refinement, the value errors in the library functions can produce unpredictable and potentially significant effects on numerical results.
How to compute and validate these new uncertainty-bearing library functions will need innovation.
For example, the uncertainty of a 64-bit value can not directly determined by its difference with the corresponding 128-bit value,

Bound momentum $\zeta(2n)$ needs to be extended to all probability distributions.
Its asymptotic behavior $2n \rightarrow +\infty: \zeta(2n) \rightarrow \kappa^{2n} / (2n)$ needs to be generalized.
Perhaps the asymptotic behavior implies a generic new requirements on the probability distributions.

The choice of ideal bounding range needs further study.
For example, the choice of $\kappa_s = 5$ for Normal distribution is based on the $5-\sigma$ rule in practice only.
Such choice certainly can not be applied to other distributions.

Cases where $\delta^2 f < \widehat{\delta^2} f$ due to low sample count require further theoretical study and experimental verification.

Determining the uncertainty upper bounds for each analytic expression through analytic methods is an important next step.
These results can validate variance arithmetic, and provide performance enhancement to variance arithmetic when the convergence of an analytic expression has to be deduced numerically from the full expansion each time.

The performance of variance arithmetic must be improved for broader practical adoption.
The fundamental formulas of statistical Taylor expansion, Formula \eqref{eqn: Taylor 1d mean}, \eqref{eqn: Taylor 1d variance}, \eqref{eqn: Taylor 2d mean}, and \eqref{eqn: Taylor 2d variance} contain a large number of independent summations, making them excellent candidates for parallel processing.
Moreover, the inherently procedural nature of these formulas allows statistical Taylor expansion to be implemented efficiently at the hardware level.

When an analytic expression undergoes statistical Taylor expansion, the resulting expression can become highly complex, as in the case of matrix inversion.
However, modern symbolic computation tools such as \textit{SymPy} can greatly facilitate these calculations.
This observation suggests that it may be time to shift from purely numerical programming toward analytic programming, particularly for problems that possess  inherently analytic formulations.

As an enhancement to dependency tracing, source tracing identifies each inputs contribution to the overall result uncertainty.
This capability enables engineers to pinpoint the primary sources of measurement inaccuracy and in turn guide targeted improvements in data acquisition and processing strategies.
For example, Formula \eqref{eqn: sum leakage} can guide how to improve the ideal ratio of $x \pm y$.

A key open question is whether variance arithmetic can be adapted to achieve ideal coverage for floating-point rounding errors.
Except for integer multiplication, variance arithmetic does not track rounding errors during computation, because tracking rounding errors for every operation is too expensive to achieve in software.
The sensitivity to the library functions increases the difficulty to solve these types of problems.
Developing variance arithmetic with ideal coverage for such errors would be quite valuable because many theoretical calculations lack explicit input uncertainties.

Because traditional numerical approaches are based on floating-point arithmetic, they must be reexamined or even reinvented within the framework of variance arithmetic.
For instance, most conventional numerical algorithms aim to identify optimal computational paths, whereas variance arithmetic conceptually rejects all path-dependent calculations.
Reconciling these two paradigms may present a significant and ongoing challenge.

Establishing theoretical foundation for applying statistical Taylor expansion in the absence of a closed-form analytic solution, or when only limited low-order numerical derivatives are available, as in solving differential equations, remains an important  direction for future research.


\ifdefined\Verbose
\clearpage
\fi
\section{Statements and Declarations}

\subsection{Acknowledgments}

As an independent researcher without institutional affiliation, the author expresses sincere gratitude to Dr. Zhong Zhong (Brookhaven National Laboratory) and Prof Weigang Qiu (Hunter College) for their encouragement and valuable discussions.
Special thanks are extended to the organizers of \emph{AMCS 2005}, particularly Prof. Hamid R. Arabnia (University of Georgia), and to the organizers of the \emph{NKS Mathematica Forum 2007}. 
The author also gratefully acknowledges Prof Dongfeng Wu (Louisville University) for her insightful guidance on statistical topics.
Finally, heartfelt appreciation is extended to the editors and reviewers of \emph{Reliable Computing} for their substantial assistance in shaping and accepting an earlier version of this work, with special recognition to Managing Editor Prof. Rolph Baker Kearfott.


\subsection{Data  Availability Statement}

The data set used in this study are all generated in the open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}. 
The execution assistance and explanation of the above code are available from the author upon reasonable request.


\subsection{Competing Interests}

The author has no competing interests to declare that are relevant to the content of this article.

\subsection{Founding}

No funding was received from any organization or agency in support of this research.


\ifdefined\Verbose
\clearpage
\fi
\ifdefined\ManualReference

\begin{thebibliography}{10}

\bibitem{Statistical_Methods}
Sylvain Ehrenfeld and Sebastian~B. Littauer.
\newblock {\em Introduction to Statistical Methods}.
\newblock McGraw-Hill, 1965.

\bibitem{Precisions_Physical_Measurements}
John~R. Taylor.
\newblock {\em Introduction to Error Analysis: The Study of Output Precisions
  in Physical Measurements}.
\newblock University Science Books, 1997.

\bibitem{Basic_Constants_Measurements}
Jurgen Bortfeldt, editor.
\newblock {\em Fundamental Constants in Physics and Chemistry}.
\newblock Springer, 1992.

\bibitem{Probability_Statistics}
Michael~J. Evans and Jeffrey~S. Rosenthal.
\newblock {\em Probability and Statistics: The Science of Uncertainty}.
\newblock W. H. Freeman, 2003.

\bibitem{Lower_Order_Variance_Expansion}
Fredrik Gustafsson and Gustaf Hendeby.
\newblock Some relations between extended and unscented kalman filters.
\newblock {\em IEEE Transactions on Signal Processing}, 60-2:545--555, 2012.

\bibitem{Computer_Architecture}
John~P Hayes.
\newblock {\em Computer Architecture}.
\newblock McGraw-Hill, 1988.

\bibitem{Floating_Point_Arithmetic}
David Goldberg.
\newblock What every computer scientist should know about floating-point
  arithmetic.
\newblock {\em ACM Computing Surveys}, March 1991.

\bibitem{Floating_Point_Standard}
Institute of Electrical and Electronics Engineers.
\newblock {\em ANSI/IEEE 754-2008 Standard for Binary Floating-Point
  Arithmetic}, 2008.

\bibitem{Arithmetic_Digital_Computers}
U.~Kulish and W.M. Miranker.
\newblock The arithmetic of digital computers: A new approach.
\newblock {\em SIAM Rev.}, 28(1), 1986.

\bibitem{Rounding_Error}
J.~H. Wilkinson.
\newblock {\em Rounding Errors in Algebraic Processes}.
\newblock SIAM, 1961.

\bibitem{Algorithms_Accuracy}
Nicholas~J. Higham.
\newblock {\em Accuracy and Stability of Numerical Algorithms}.
\newblock SIAM, 2002.

\bibitem{Numerical_Recipes}
William~H. Press, Saul~A Teukolsky, William~T. Vetterling, and Brian~P.
  Flannery.
\newblock {\em Numerical Recipes in C}.
\newblock Cambridge University Press, 1992.

\bibitem{Precise_Numerical_Methods}
Oliver Aberth.
\newblock {\em Precise Numerical Methods Using C++}.
\newblock Academic Press, 1998.

\bibitem{Linear_Algebra}
J.~Hefferon.
\newblock Linear algebra.
\newblock \url{http://joshua.smcvt.edu/linearalgebra/}, 2011.

\bibitem{prob_interval}
Nicholas~J. Higham and Theo Mary.
\newblock A new approach to probabilistic rounding error analysis.
\newblock {\em SIAM Journal on Scientific Computing}, 41(5):A2815--A2835, 2019.

\bibitem{Error_Analysi_Digital_Filters}
B.~Liu and T.~Kaneko.
\newblock Error analysis of digital filters realized with floating-point
  arithmetic.
\newblock {\em Proc. IEEE}, 57:p1735--1747, 1969.

\bibitem{Floating-point_Digital_Filters}
B.~D. Rao.
\newblock Floating-point arithmetic and digital filters.
\newblock {\em IEEE, Transations on Signal Processing}, 40:85--95, 1992.

\bibitem{Chaotic_Dynamics}
Gregory~L. Baker and Jerry~P. Gollub.
\newblock {\em Chaotic Dynamics: An Introduction}.
\newblock Cambridge University Press, 1990.

\bibitem{rouding_higher_precision}
Brian Gladman, Vincenzo Innocente, John Mather, and Paul Zimmermann.
\newblock Accuracy of mathematical functions in single, double, double
  extended, and quadruple precision.
\newblock 2024.

\bibitem{Interval_Analysis}
R.E. Moore.
\newblock {\em Interval Analysis}.
\newblock Prentice Hall, 1966.

\bibitem{Worst_Case_Error_Bounds}
W.~Kramer.
\newblock A prior worst case error bounds for floating-point computations.
\newblock {\em IEEE Trans. Computers}, 47:750--756, 1998.

\bibitem{Interval_Analysis_Theory_Applications}
G.~Alefeld and G.~Mayer.
\newblock Interval analysis: Theory and applications.
\newblock {\em Journal of Computational and Applied Mathematics}, 121:421--464,
  2000.

\bibitem{Interval_Arithmetic}
W.~Kramer.
\newblock Generalized intervals and the dependency problem.
\newblock {\em Proceedings in Applied Mathematics and Mechanics}, 6:685--686,
  2006.

\bibitem{Interval_Analysis_Notations}
A.~Neumaier S.M. Rump S.P.~Shary B.~Kearfott, M. T.~Nakao and P.~Van
  Hentenryck.
\newblock Standardized notation in interval analysis.
\newblock {\em Computational Technologies}, 15:7--13, 2010.

\bibitem{Statistics_For_Interval_Arithmetic}
W.~T. Tucker and S.~Ferson.
\newblock {\em Probability bounds analysis in environmental risk assessments}.
\newblock Applied Biomathmetics, 100 North Country Road, Setauket, New York
  11733, 2003.

\bibitem{Affine_Arithmetic}
J.~Stolfi and L.~H. de~Figueiredo.
\newblock An introduction to affine arithmetic.
\newblock {\em TEMA Tend. Mat. Apl. Comput.}, 4:297--312, 2003.

\bibitem{Random_Interval_Arithmetic}
R.~Alt and J.-L. Lamotte.
\newblock Some experiments on the evaluation of functional ranges using a
  random interval arithmetic.
\newblock {\em Mathematics and Computers in Simulation}, 56:17--34, 2001.

\bibitem{Affine_Arithmetic_book}
J.~Stolfi and L.~H. de~Figueiredo.
\newblock {\em Self-validated numerical methods and applications}.
\newblock ftp:// ftp.tecgraf.puc-rio.br/pub/lhf/doc/cbm97.ps.gz, 1997.

\bibitem{Prev_Precision_Arithmetic}
C.~P. Wang.
\newblock A new uncertainty-bearing floating-point arithmetic.
\newblock {\em Reliable Computing}, 16:308--361, 2012.

\bibitem{Statistical_Arithmetic}
Propagation of uncertainty.
\newblock \url{http://en.wikipedia.org/wiki/Propagation_of_uncertainty}, 2011.
\newblock wikipedia, the free encyclopedia.

\bibitem{Statistical_Analysis}
S.~Ferson H.~M.~Regan and D.~Berleant.
\newblock Equivalence of methods for uncertainty propagation of real-valued
  random variables.
\newblock {\em International Journal of Approximate Reasoning}, 36:1--30, 2004.

\bibitem{Significance_Arithmetic}
Significance arithmetic.
\newblock \url{http://en.wikipedia.org/wiki/Significance_arithmetic}, 2011.
\newblock wikipedia, the free encyclopedia.

\bibitem{Digital_Significance_Arithmetic}
M.~Goldstein.
\newblock Significance arithmetic on a digital computer.
\newblock {\em Communications of the ACM}, 6:111--117, 1963.

\bibitem{Unnormalized_Arithmetic}
R.~L. Ashenhurst and N.~Metropolis.
\newblock Unnormalized floating-point arithmetic.
\newblock {\em Journal of the ACM}, 6:415--428, 1959.

\bibitem{Mathematica_Significance_Arithmetic}
G.~Spaletta M.~Sofroniou.
\newblock Precise numerical computation.
\newblock {\em The Journal of Logic and Algebraic Programming}, 65:113134,
  2005.

\bibitem{Stochastic_Arithmetic}
J.~Vignes.
\newblock A stochastic arithmetic for reliable scientific computation.
\newblock {\em Mathematics and Computers in Simulation}, 35:233--261, 1993.

\bibitem{CADNA_library}
C.~Denis N.~S.~Scott, F.~Jezequel and J.~M. Chesneaux.
\newblock Numerical 'health' check for scientific codes: the cadna approach.
\newblock {\em Computer Physics Communications}, 176(8):501--527, 2007.

\end{thebibliography}

\else
\bibliographystyle{unsrt}
\bibliography{VarianceArithmetic}
\fi






\end{document}
