\documentclass[twoside]{article}
%Latex for arxiv needs figures in pdf format.  To convert png to pdf, go to https://pngpdf.com/#google_vignette

\pagestyle{myheadings}
\setlength{\oddsidemargin}{44pt}
\setlength{\evensidemargin}{44pt}
\setcounter{page}{1}

\usepackage{url,intmacros,graphicx}
\usepackage{amssymb,amsmath}
\usepackage{capt-of}
\usepackage{float}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\numberwithin{equation}{section}

\newcommand{\email}[1]{\\ \small{\url{#1}} \\}
\newcommand{\institution}[1]{\\ \parbox{3.0in}{\small{#1}}}
\newcommand{\keywords}[1]{\small\textbf{Keywords: }#1}
\newcommand{\AMSsubj}[1]{\noindent\textbf{AMS subject classifications: }#1}
\newcommand\whenaccepted{}

\newcommand{\eqspace}{\;\;\;}
\newcommand{\largespace}{\;\;\;\;\;\;\;\;\;\;\;\;}
%\newcommand{\Verbose}{}
\newcommand{\ManualReference}{}



%-------------------------------------------------------


%  You may add additional packages here.  However, if they
%  are not available with the usual LaTeX distribution,
%  they must be supplied with the final, accepted LaTeX.

% Fill in your title here. (Retain the footnote.)
\title{Statistical Taylor Expansion: A New and Path-Independent Method for Uncertainty Analysis \footnote{\whenaccepted}}

% Delete the "\and" or add more as needed
\author{Chengpu Wang
\institution{40 Grossman Street, Melville, NY 11747, USA}
\email{Chengpu@gmail.com}}

% Put a short running title within the first argument to
% this command.  Do not alter the second argument
\markboth{CP Wang, \textit{Statistical Taylor Expansion}}
         {\textit{https://github.com/Chengpu0707/VarianceArithmetic}}




\begin{document}
\maketitle
\begin{abstract}

As a rigorous extension of conventional Taylor expansion, statistical Taylor expansion replaces precise input variables with random input variables of known distributions and sample counts, to compute the mean, standard deviation, and reliability of each result.
This approach tracks the propagation of input uncertainties through intermediate steps, making the final result path independent.
Therefore, it differs fundamentally from conventional approaches that are usually path dependent, such that it may standardize numerical computations for analytic expressions.
This study further presents an implementation of statistical Taylor expansion termed variance arithmetic, and demonstrates its performance across diverse mathematical applications.

The finding of this study also reveals the potential and substantial impact of numerical errors in library functions. 
\ifdefined\Verbose
The possible link between statistical Taylor expansion and quantum physics is discussed as well.
\fi
\end{abstract}

% Put keywords appropriate to your paper here, as shown
\keywords{computer arithmetic, error analysis, interval arithmetic, uncertainty, numerical algorithms.}

% Put your AMS subject classifications into the argument of
% the following command.
\AMSsubj{G.1.0}

Copyright \copyright{2024}


\section{Introduction}
\label{sec: introduction}

Let $x$ and  $\delta x$ denote a value and its uncertainty deviation, respectively.
As input, $x$ and $\delta x$ are usually the mean and standard deviation of a measurement, respectively \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
If $\delta x = 0$, then $x$ is a \emph{precise value}; otherwise, the pair specifies an \emph{imprecise value} $x \pm \delta x$.
Let $P(x) \equiv \delta x / |x|$ be the \emph{statistical precision} (hereafter referred to as precision) of $x \pm \delta x$.
A smaller $P(x)$ indicates a higher measurement quality of $x \pm \delta x$.

Statistical Taylor expansion determines the result $f \pm \delta f$ and reliability of a general analytic expression $f(x, \dots)$ on the basis of inputs $x \pm \delta x, \dots$ and their corresponding distributions and sample counts. 
\begin{itemize}
\item 
Previous studies have examined the effect of input uncertainties on output values for specific cases \cite{Lower_Order_Variance_Expansion}.
Statistical Taylor expansion generalizes these effects as uncertainty bias, as shown in Formulas \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 2d mean} of this study.

\item
The traditional variance-covariance framework accounts only for linear interactions between random variables through an analytic function \cite{Lower_Order_Variance_Expansion}\cite{Probability_Statistics}\cite{Numerical_Recipes}, whereas statistical Taylor expansion extends this framework to include higher-order interactions as expressed in Formula \eqref{eqn: Taylor 2d variance} in this study.

\item Calculating the reliability $[0, 1]$ of $f \pm \delta f$ from input uncertainty distributions and sample counts seems completely new.
\end{itemize}

As a new approach, statistical Taylor expansion is superior to all existing numerical arithmetic.

Conventional floating-point arithmetic \cite{Computer_Architecture}\cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard} computes only the result value $f$.
Because $f$ contains unknown amount of rounding error \cite{Rounding_Error}\cite{Precise_Numerical_Methods}\cite{Algorithms_Accuracy}, a floating-point representation with $10^{-16}$ resolution may not be good enough for inputs with $10^{-2}$ to $10^{-6}$ precision.
However, statistical Taylor expansion can account for rounding errors as part of the result deviation $\delta f$.

The bounding range in interval arithmetic \cite{Interval_Analysis}\cite{Worst_Case_Error_Bounds}\cite{Interval_Analysis_Theory_Applications}\cite{Interval_Arithmetic}\cite{Interval_Analysis_Notations} is inconsistent with the statistical nature of an $x \pm \delta x$ pair and it tends to over-estimate the result uncertainty because of its worst-case assumption \cite{Prev_Precision_Arithmetic}.
Conversely, statistical Taylor expansion is precise statistically.

Both conventional floating-point arithmetic and interval arithmetic depend strongly on the specific algebraic form of an analytic function, a phenomenon known as the \emph{dependency problem} \cite{Precise_Numerical_Methods}\cite{Algorithms_Accuracy}\cite{Interval_Analysis}\cite{Interval_Analysis_Theory_Applications}, which can make conventional numerical computation more an art than a science.
In contrast, statistical Taylor expansion is path independent.

To ensure mathematical and statistical rigor, statistical Taylor expansion abandons the significance arithmetic nature of its processor \cite{Prev_Precision_Arithmetic}.

As a statistical sampling process, stochastic arithmetic \cite{Stochastic_Arithmetic}\cite{CADNA_library} is computationally expensive, whereas statistical Taylor expansion provides a direct characterization without sampling.

The remainder of this paper is organized as follows:
\begin{itemize}
\item Section \ref{sec: statistical Taylor expansion} develops the theoretical foundation of statistical Taylor expansion.

\item Section \ref{sec: variance arithmetic} describes variance arithmetic as a numerical implementation of statistical Taylor expansion.

\item Section \ref{sec: validation} presents standards for validating variance arithmetic.

\item Section \ref{sec: polynomial} illustrates variance arithmetic in polynomial computation, demonstrating its ability to trace floating-point rounding errors and its continuity in parameter space.

\item Section \ref{sec: matrix} describes the applications of variance arithmetic to matrix inversion, distinguishing between distribution tests and value tests.

\item Section \ref{sec: Math Library} discusses the evaluation of variance arithmetic on common mathematical library functions, showing the effect of distributional pole.

\ifdefined\Verbose
\item Section \ref{sec: Moving-Window Linear Regression} reveals the accumulation of numerical errors by reusing an input multiple time in a moving-window progressive algorithm.
\fi

\item Section \ref{sec: FFT} examines the impact of numerical library errors and shows that these errors can be significant.

\item Section \ref{sec: recursion} showcases variance arithmetic in catching catastrophic cancellation in a regression algorithm.

\item Section \ref{sec: conclusion and discussion} concludes with a summary and a discussion of the findings.

\end{itemize}




\section{Statistical Taylor Expansion}
\label{sec: statistical Taylor expansion}

\subsection{Uncorrelated Uncertainty Condition}

\iffalse

\begin{align*}
& \frac{1}{\gamma_{P}} - 1 = \left(\frac{1}{\gamma} -1\right) \frac{1}{P^2}; \\
& P^2 \left( \frac{1}{\gamma_{P}} - 1 \right) + 1 = \frac{1}{\gamma};
\end{align*}

\fi

When inputs contain no systematic error \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}, their uncertainties are uncorrelated, even though their values can have significant correlations.
This condition can be characterized quantitatively in statistics as the \emph{uncorrelated uncertainty condition} \cite{Prev_Precision_Arithmetic}.


\subsection{Distributional Zero and Distributional Pole}

\iffalse

To solve for mode:
\begin{align*}
& \rho(\tilde{y}, y, \delta y) = \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z});
\eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x}; \\
& 0 = \frac{d \rho(\tilde{y}, y, \delta y)}{d \tilde{y}} = \frac{d^2 \tilde{z}}{d \tilde{y}^2} N(\tilde{z}) - \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z}) \tilde{z}; \\
& \frac{d^2 \tilde{z}}{d \tilde{y}^2} = \frac{d \tilde{z}}{d \tilde{y}} \tilde{z}; \eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x};
\end{align*}

The exponential function:
\begin{align*}
f(x) = e^x:& \eqspace 
\tilde{z} = \frac{\log(\tilde{y}) - x}{\delta x}, \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\tilde{y} \delta x} = \frac{1}{\frac{d}{d \tilde{z}} e^{x + \tilde{z} \delta x}}; \eqspace \\
\frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z})
&= e^{-\log(\tilde{y})} \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2}{2 \delta^2 x}}
 = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2 + 2 \log(\tilde{y}) \delta^2 x }{2 \delta^2 x}} \\
& = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - (x - \delta^2 x))^2 + 2 x \delta^2 x - (\delta^2 x)^2 }{2 \delta^2 x}}
 = N(\frac{\log(\tilde{y}) - (x - \delta^2 x)}{\delta x}) e^{-x + \frac{\delta^2 x}{2}}; \\
& 0 = \tilde{z} e^{x + \tilde{z} \delta x} + (\delta x) e^{x + \tilde{z} \delta x}; \eqspace
 \tilde{z} = -(\delta x) = \frac{1}{\delta x}(x - (\delta x)^2 - x);
\end{align*}

The log function $f(x) = \log(x)$:
\begin{align*}
& \tilde{z} = \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} e^{\tilde{y}} = \frac{1}{\delta x} (x + \tilde{z} \delta x)
 = \frac{1}{\frac{d}{d \tilde{z}} \log(x + \tilde{z} \delta x)}; \\
& \frac{1}{\delta x} e^{\tilde{y}} = (\frac{1}{\delta x} e^{\tilde{y}})^2 \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
(e^{\tilde{y}})^2 - x e^{\tilde{y}} - \delta^2 x = 0; \\
e^{\tilde{y}_m} &= \frac{x + \sqrt{x^2 + 4 \delta^2 x}}{2}; \\
& \eqspace 0 = \frac{\tilde{z}}{x + \tilde{z} \delta x} - (\delta x) \frac{1}{(x + \tilde{z} \delta x)^2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} - (\delta x); \\
& \tilde{z} = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 + 4 (\delta x)^2}}{2} - x);
\end{align*}

The power mode $f(x) = x^{\frac{1}{p}}$:
\begin{align*}
& \tilde{z} = \frac{\tilde{y}^p - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} p \tilde{y}^{p-1}; \\
& \frac{1}{\delta x} p (p - 1) \tilde{y}^{p-2} = (\frac{1}{\delta x} p \tilde{y}^{p-1})^2 \frac{\tilde{y}^p - x}{\delta x}; \\
& \tilde{y}^{2p} - x \tilde{y}^{p} - \frac{p - 1}{p} \delta^2 x = 0; \\
\tilde{y}_m^p &= \frac{1}{2} (x + \sqrt{x^2 + 4 \frac{p - 1}{p} \delta^2 x}); \\
& 0 = c (x + \tilde{z} \delta x)^{c-1} \tilde{z} + c (c-1) \delta x (x + \tilde{z} \delta x)^{c-2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} + (c - 1) (\delta x); \\
& \tilde{z}_m = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2} - x); \eqspace 
f^{-1}(\tilde{y}_m) = \frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2}; \\
p = 2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 2 \tilde{y}; \eqspace 
  \tilde{y}_m^{\frac{1}{2}} = \frac{1}{2} \left( x + \sqrt{x^2 + 2 \delta^2 x} \right); \\
p = 3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 3 \tilde{y}^2; \eqspace  
  \tilde{y}_m^{\frac{1}{3}} = \frac{1}{2} \left( x \pm \sqrt{x^2 + \frac{8}{3} \delta^2 x} \right); \\
p = 1/2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{2} \tilde{y}^{-\frac{1}{2}}; \eqspace 
  \tilde{y}_m^2 = \frac{1}{2} \left( x + \sqrt{x^2 - 4 \delta^2 x} \right); \\
p = 1/3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{3} \tilde{y}^{-\frac{2}{3}}; \eqspace
  \tilde{y}_m^2 = \frac{1}{2} \left( x \pm \sqrt{x^2 - 8 \delta^2 x} \right); \\
p = -1:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = - \frac{1}{\delta x} \tilde{y}^{-2}; \eqspace 
  \tilde{y}_m^{-1} = \frac{1}{2} \left( x + \sqrt{x^2 + 8 \delta^2 x} \right); 
\end{align*}


The distribution difference:
\begin{align*}
&\nu = \frac{x - f^{-1}(\tilde{y}_m)}{\delta x}: \eqspace 
 N(\frac{f^{-1}(\tilde{y}) - f^{-1}(\tilde{y}_m)}{\delta x} )
 = N(\tilde{z} + \nu) = N(\tilde{z}) e^{-\tilde{z} \nu} e^{-\frac{1}{2} \nu^2}; \\
\eta &\equiv \int |\varrho(\tilde{y}, y, \delta y) - \rho(\tilde{y}, y, \delta y)| d \tilde{y}
 =  \int |\rho(f^{-1}(\tilde{y}), f^{-1}(\tilde{y_m}), \delta x) - \rho(f^{-1}(\tilde{y}), x, \delta x)| d f^{-1}(\tilde{y}) \\
&= \int |N(\tilde{z} + \nu) - N(\tilde{z})| d \tilde{z} 
 = \int |e^{-\frac{1}{2} \nu^2} e^{-\tilde{z} \nu} - 1| N(\tilde{z}) d \tilde{z}; \\
&= | \int |\sum_{m=0}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=0}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n - 1| 
   N(\tilde{z}) d \tilde{z} | \\
&= | \int \left(\sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n
  - \frac{1}{2} \nu^2 - \nu \tilde{z} \right) N(\tilde{z}) d \tilde{z} | \\
&= \frac{1}{2} \nu^2 - \sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{\nu^{2n}}{2^n n!}
 = \frac{1}{2} \nu^2 - \sum_{m=2}^{\infty} \sum_{n=1}^{m-1} \frac{(-1)^m}{2^m (m - n)! n!} \nu^{2m} \\
&\simeq \frac{1}{2} \nu^2 - \frac{1}{4} \nu^4 + \frac{1}{8} \nu^6 - \frac{7}{192} \nu^8;
\end{align*}
When $f(x)=x^c$:
\begin{align*}
\nu &= \frac{x - \frac{1}{2} (x + \sqrt{x^2 + (1 - c) 4 \delta^2 x})}{\delta x} = \frac{1 - \sqrt{1 + (1 - c) 4 P(x)^2}}{2 P(x)} \\
&= - \sum_{m=1} \frac{(1 - c)^m (2P(x))^{2m - 1}}{m!} \prod_{n=1}^{m} \frac{\frac{3}{2} -n}{n} \\
&\simeq -(1 - c) P(x) + \frac{1}{4} (1 - c)^2 P(x)^3 - \frac{1}{4} (1 - c)^3 P(x)^5; \\
\eta &= \frac{1}{2} (1-c)^2 P(x)^2 - \frac{1}{4} (1-c)^3 (2-c) P(x)^4 + 1/8 (1-c)^4 (c^2-4c+7) P(x)^6
\end{align*}

\begin{align}
\label{eqn: power distribution}
&y = x^c: \eqspace \rho(\tilde{y}, \mu_y, \sigma_y) = c \tilde{y}^{\frac{1}{c}-1} \frac{1}{\sigma} N(\frac{\tilde{y}^\frac{1}{c} - \mu}{\sigma}); 
\end{align}

\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Distribution.pdf} 
\captionof{figure}{
Probability density function of $\tilde{y} = \tilde{x}^2$, for various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\sqrt{\tilde{y}}$.
}
\label{fig: Square_Distribution}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Root_Distribution.pdf} 
\captionof{figure}{
Probability density function for $\tilde{y} = \sqrt{\tilde{x}}$, for various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with the distributional mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\tilde{y}^2$.
}
\label{fig: Square_Root_Distribution}
\end{figure}

Let $\rho(\tilde{x}, \mu, \sigma)$ denote the probability density function of a random variable $\tilde{x}$ with distribution mean $\mu$ and distribution deviation $\sigma$.
Let $\tilde{y} = f(\tilde{x})$ be a strictly monotonic function, such that its inverse function $\tilde{x} = f^{-1}(\tilde{y})$ exists.
Formula \eqref{eqn: function distribution} shows the probability density function of $\tilde{y}$ \cite{Statistical_Methods}\cite{Probability_Statistics}.
In Formula \eqref{eqn: function distribution}, the same distribution can be expressed in terms of either $\tilde{x}$ or $\tilde{y}$, which are simply different representations of the same underlying random variable.
\begin{align}
\label{eqn: function distribution}
\rho(\tilde{x}, \mu, \sigma) d\tilde{x} &= \rho(f^{-1}(\tilde{y}), \mu, \sigma) \frac{d\tilde{x}}{d\tilde{y}} d\tilde{y} 
= \rho(\tilde{y}, \mu_y, \sigma_y) d\tilde{y};
\end{align}

Viewed in the $f^{-1}(\tilde{y})$ coordinate, $\rho(\tilde{y}, \mu_y, \sigma_y)$ is $\rho(\tilde{x}, \mu, \sigma)$ multiplied by $1/f^{(1)}_x$, in which $f^{(1)}_x$ is the first derivative of $f(x)$ with respect to $x$.
\begin{itemize}
\item 
A \emph{distributional pole} occurs when $f^{(1)}_x=0 \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = \infty$.
For example, $(\mu \pm 1)^2$ has a distributional pole at $\tilde{x} = 0$, as shown in Figure \ref{fig: Square_Distribution}.

\item 
A \emph{distributional zero} occurs when $f^{(1)}_x=\infty \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = 0$.
For example  $\sqrt{\mu \pm 1}$ has a distributional zero at $\tilde{x} = 0$, as shown in Figure \ref{fig: Square_Root_Distribution}.

\end{itemize}

In both Figures \ref{fig: Square_Distribution} and \ref{fig: Square_Root_Distribution}, $\rho(\tilde{y}, \mu_y, \sigma_y)$ closely resembles $\rho(\tilde{x}, \mu, \sigma)$ when the mode of $\rho(\tilde{x}, \mu, \sigma)$ lies sufficiently far away from either a distributional pole or a distributional zero, such as  $(5 \pm 1)^2$ in Figure \ref{fig: Square_Distribution} and $\sqrt{5 \pm 1}$ in Figure \ref{fig: Square_Root_Distribution}, thereby allowing for characterization of the output $f(x)$ using mean $\overline{f(x)}$ and deviation $\delta f(x)$ only. 



\subsection{Statistical Taylor Expansion}

Define $\tilde{z} \equiv (\tilde{x} - x)/\delta x$ and let $\rho(\tilde{z})$ be the normalized form of $\rho(\tilde{x}, x, \delta x)$ such that $\tilde{z}$ has distribution mean $0$ and distribution deviation $1$.

\begin{align}
\label{eqn: bound moment}
\zeta(n, \kappa) &\equiv \int_{\varrho}^{\kappa} \tilde{z}^n \rho(\tilde{z}) d \tilde{z};\\
\label{eqn: mean-reverting bounding}
\zeta(1, \kappa) &= 0;
\end{align}
Let $\tilde{z} \in [\varrho, \kappa]$ where $\varrho, \kappa$ specify the \emph{bounding ranges}. 
Formula \eqref{eqn: bound moment} defines the corresponding \emph{bound moment} $\zeta(n, \kappa)$, which further satisfies the \emph{mean-reverting condition} of Formula \eqref{eqn: mean-reverting bounding} such that $\kappa$ determines $\varrho$.
For any symmetric probability distribution: $\rho(-\tilde{z}) = \rho(\tilde{z})$, $\varrho = -\kappa$, and $\zeta(2n+1, \kappa) = 0$.

\begin{align}
\label{eqn: Taylor 1d} 
f(x + \tilde{x}) &= f(x + \tilde{z} \delta x) = f(x) + \sum_{n=1}^{\infty} \frac{f^{(n)}_x}{n!} \tilde{z}^n (\delta x)^n; \\
\label{eqn: Taylor 1d mean}
\overline{f(x)} &= \int_{-\varrho}^{+\kappa} f(x + \tilde{z}) \rho(\tilde{z}) d \tilde{z} = f(x) + \sum_{n=1}^{\infty}(\delta x)^n \frac{f^{(n)}_x}{n!} \zeta(n, \kappa); \\
\label{eqn: Taylor 1d variance}
\delta^2 f(x) &= \overline{(f(x) - \overline{f(x)})^2} = \overline{f(x)^2} - \overline{f(x)}^2 \nonumber \\
	&= \sum_{n=1}^{\infty} (\delta x)^n \sum_{j=1}^{n-1} \frac{f^{(j)}_x}{j!} \frac{f^{(n-j)}_x}{(n-j)!} \big(\zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n-j, \kappa) \big);
\end{align}
An analytic function $f(x)$ can be accurately evaluated in a range using the Taylor series as shown in Formula \eqref{eqn: Taylor 1d}.
Formulas \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 1d variance} yield the mean $\overline{f(x)}$ and variance $\delta^2 f(x)$ of $f(x)$, respectively.
The difference $\overline{f(x)} - f(x)$ is defined as the \emph{uncertainty bias}, representing the effect of input uncertainty on the resulting value.

\begin{align}
\label{eqn: Taylor 2d}
f(x + \tilde{x}, y + \tilde{y}) &= \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{f^{(m,n)}_{(x,y)}}{m! n!} \tilde{x}^m \tilde{y}^n; \\
\label{eqn: Taylor 2d mean}
\overline{f(x,y)} &=  \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} (\delta x)^m (\delta y)^n \frac{f^{(m,n)}_{(x,y)}}{m!\;n!} \zeta_x(m, \kappa_x) \zeta_y(n, \kappa_y);  \\
\label{eqn: Taylor 2d variance}
\delta^2 f(x, y) &= \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} (\delta x)^m (\delta y)^n \sum_{i=0}^{m} \sum_{j=0}^{n} 
		\frac{f^{(i,j)}_{(x,y)}}{i!\;j!}\frac{f^{(m-i, n-j)}_{(x,y)}}{(m-i)!\;(n-j)!} \nonumber \\
	&\big( \zeta_x(m, \kappa_x) \zeta_y(n, \kappa_y) - \zeta_x(i, \kappa_x)\zeta_x(m-i, \kappa_x)\; \zeta_y(j, \kappa_y)\zeta_y(n-j, \kappa_y) \big);
\end{align}
Under the uncorrelated uncertainty condition, Formulas \eqref{eqn: Taylor 2d mean} and \eqref{eqn: Taylor 2d variance} compute the mean and variance of the Taylor expansion given in Formula \eqref{eqn: Taylor 2d}, where $\zeta_x(m, \kappa_x)$ and $\zeta_y(n, \kappa_y)$ denote the bound moments for $x$ and $y$, respectively.
Although Formula \eqref{eqn: Taylor 2d variance} is only 2-dimensional problem space, it can be extended easily to any dimension.

With the mean reverting condition of Formula \eqref{eqn: mean-reverting bounding}:
\begin{align}
\label{eqn: addition mean}
\overline{x \pm y} &= \zeta(0, \kappa_x) x \pm \zeta(0, \kappa_x) y; \\
\label{eqn: addition variance}
\delta^2 (x \pm y) &= \zeta(2, \kappa_x) (\delta x)^2 + \zeta(2, \kappa_y) (\delta y)^2; \\
\label{eqn: multiplication mean}
\overline{x y} &= \zeta(0, \kappa_x) x \; \zeta(0, \kappa_y) y; \\
\label{eqn: multiplication variance}
\delta^2 (x y) &= \zeta(2, \kappa_x) (\delta x)^2 y^2 + x^2 \zeta(2, \kappa_y) (\delta y)^2 + \zeta(2, \kappa_x) (\delta x)^2 \; \zeta(2, \kappa_y) (\delta y)^2;
\end{align}
When $N \rightarrow \infty$, $\zeta(0, \kappa) \rightarrow 1$ and $\zeta(2, \kappa) \rightarrow 1$, making Formulas \eqref{eqn: addition mean} and \eqref{eqn: addition variance} the convolution results for $x \pm y$ \cite{Probability_Statistics}, and Formulas \eqref{eqn: multiplication mean} and \eqref{eqn: multiplication variance} the corresponding results of the product distribution for $x y$ \cite{Probability_Statistics}.





\subsection{One-Dimensional Examples}

\iffalse
\begin{align}
\label{eqn: exp Taylor}
& e^{x + \tilde{x}} = e^x \sum_{n=0}^{\infty} \frac{\tilde{x}^n}{n!}; \\
\label{eqn: log Taylor}
& \log(x + \tilde{x}) - \log(x) = \log(1 + \frac{\tilde{x}}{x}) = \sum_{j=1}^{\infty} \frac{(-1)^{j+1}}{j} \frac{\tilde{x}^j}{x^j}; \\
\label{eqn: sin Taylor}
\sin(x + \tilde{x}) &= \sum_{n=0}^{\infty} \eta(n, x) \frac{\tilde{x}^{n}}{n!};
	\eqspace \eta(n, x) \equiv \begin{cases} 
		n = 4j: \eqspace  sin(x); \\ n = 4j + 1: \eqspace  cos(x); \\ n = 4j + 2: \eqspace  -sin(x); \\ n = 4j +3: \eqspace  -cos(x); 
	\end{cases} \\
\label{eqn: power Taylor}
&(x + \tilde{x})^c = x^c (1 + \frac{\tilde{x}}{x})^c = x^c + x^c \sum_{n=1}^{\infty} \frac{\tilde{x}^n}{x^n} \begin{pmatrix} c \\ n \end{pmatrix};
	\eqspace \begin{pmatrix} c \\ n \end{pmatrix} \equiv \frac{\prod_{j=0}^{n-1} (c -j)}{n!};
\end{align}

\begin{align*}
\frac{x + \tilde{x}}{y + \tilde{y}} &\simeq \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y}
  		 - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2
		 + \frac{1}{y^3} \tilde{x} \tilde{y}^2 - \frac{x}{y^4} \tilde{y}^3; \\
(x + \tilde{x}) \frac{1}{y + \tilde{y}} &\simeq (x + \tilde{x}) \left( \frac{1}{y} - \frac{1}{y^2} \tilde{y} + \frac{1}{y^3} \tilde{y}^2 - \frac{1}{y^4} \tilde{y}^3 \right) \\
	&= \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y} - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2  + \frac{1}{y^3} \tilde{x} \tilde{y}^2
		 -  \frac{x}{y^4} \tilde{y}^3 - \frac{1}{y^4} \tilde{x} \tilde{y}^3;
\end{align*}


\fi

Formulas \eqref{eqn: exp mean} and \eqref{eqn: exp precision} give the mean and variance for $e^x$, respectively:
\begin{align}
\label{eqn: exp mean}
\frac{\overline{e^x}}{e^x}  &= 1 + \sum_{n=1}^{\infty} (\delta x)^n \zeta(n, \kappa) \frac{1}{n!}; \\
\label{eqn: exp precision}
\frac{\delta^2 e^x}{(e^x)^2} &= \sum_{n=2}^{\infty} (\delta x)^n \sum_{j=1}^{n-1} \frac{\zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n - j, \kappa)}{j!\;(n - j)!};
\end{align}

Formulas \eqref{eqn: log mean} and \eqref{eqn: log precision} give the mean and variance for $\log(x)$, respectively:
\begin{align}
\label{eqn: log mean}
\overline{\log(x)}  &= \log(x) + \sum_{n=1}^{+\infty} P(x)^{n} \frac{(-1)^{n+1} \zeta(n, \kappa)}{n}; \\
\label{eqn: log precision}
\delta^2 \log(x) &= \sum_{n=2}^{+\infty} P(x)^{n} \sum_{j=1}^{n-1} \frac{\zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n - j, \kappa)}{j (n-j)};
\end{align}

Formulas \eqref{eqn: sin mean} and \eqref{eqn: sin precision} give the mean and variance for $\sin(x)$, respectively:
\begin{align}
\label{eqn: sin Taylor}
\sin(x + \tilde{x}) &= \sum_{n=0}^{\infty} \eta(n, x) \frac{\tilde{x}^{n}}{n!};
	\eqspace \eta(n, x) \equiv \begin{cases} 
		n = 4j: \eqspace  sin(x); \\ n = 4j + 1: \eqspace  cos(x); \\ n = 4j + 2: \eqspace  -sin(x); \\ n = 4j +3: \eqspace  -cos(x); 
	\end{cases} \\
\label{eqn: sin mean}
\overline{\sin(x)} =& \sum_{n=0}^{\infty} (\delta x)^n \eta(n, x) \frac{\zeta(n, \kappa)}{n!}; \\
\label{eqn: sin precision}
\delta^2 \sin(x) =& \sum_{n=2}^{\infty} (\delta x)^n \sum_{j=1}^{n-1} \frac{\eta(j, x)\eta(n - j, x)}{j! (n-j)!}
      	\big(\zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n - j, \kappa)\big); 
\end{align}

Formulas \eqref{eqn: power mean} and \eqref{eqn: power precision} give the mean and variance for $x^c$, respectively:
\begin{align}
\label{eqn: power mean}
\frac{\overline{x^c}}{x^c}  &= 1 + \sum_{n=1}^{\infty} P(x)^{n} \zeta(n, \kappa) \begin{pmatrix} c \\ n \end{pmatrix}; \\
\label{eqn: power precision}
\frac{\delta^2 x^c}{(x^c)^2} &= \sum_{n=2}^{\infty} P(x)^{n} \sum_{j=1}^{n-1}
  \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ n - j \end{pmatrix} \big( \zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n - j, \kappa) \big);
\end{align}

The input and output in statistical Taylor expansion reflects the inherent characteristics of the calculation, such as $\delta x \rightarrow P(e^x)$, $P(x) \rightarrow \delta \log(x)$, $\delta x \rightarrow \delta \sin(x)$, and $P(x) \rightarrow P(x^c)$.



\subsection{Convergence of Variance}

\iffalse

\begin{align*}
\int_{0}^{\kappa}& \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}
		= - \int_{0}^{\kappa} \tilde{z}^{2n - 1} d\; e^{-\frac{1}{2} \tilde{z}^2}
		= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \tilde{z}^{2n - 1} e^{-\frac{1}{2} \tilde{z}^2} \Big |_{0}^{\kappa} \\
		&= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1) (2n - 3) \int_{0}^{\kappa} \tilde{z}^{2n - 4}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} 
			- (2n - 1) \kappa^{2n - 3} e^{-\frac{1}{2} \kappa^2} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1)!! \left( \int_{0}^{\kappa} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - e^{-\frac{1}{2} \kappa^2} \sum_{j=1}^{n} \frac{\kappa^{2n - 2j + 1}}{(2j - 1)!!}  \right) \\
		&= (2n - 1)!! \left(\sqrt{2} \int_{0}^{\frac{\kappa}{\sqrt{2}}} e^{-(\frac{\tilde{z}}{\sqrt{2}})^2} d \frac{\tilde{z}}{\sqrt{2}} 
			- e^{-\frac{1}{2} \kappa^2} \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!} \right) \\
		& = (2n - 1)!! e^{-\frac{1}{2} \kappa^2} \left( \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}})  e^{\frac{1}{2} \kappa^2} 
			- \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!}  \right)
\end{align*}
According to Eq (21) in https://mathworld.wolfram.com/DoubleFactorial.html:
\begin{align*}
\sum_{j=0}^{+\infty} \frac{\kappa^{2j + 1}}{(2j + 1)!!} = \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}}) e^{\frac{1}{2} \kappa^2};
\end{align*}
Thus:
\begin{align*}
\int_{0}^{\kappa} \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}
		=  e^{-\frac{1}{2} \kappa^2} \sum_{j=n}^{+\infty} \frac{(2n - 1)!!}{(2j + 1)!!} \kappa^{2j + 1}
		= e^{-\frac{1}{2} \kappa^2} \kappa^{2n} \sum_{j=1}^{+\infty} \frac{(2n - 1)!!}{(2n - 1 + 2j)!!} \kappa^{2j - 1};
\end{align*}


\begin{align*}
\zeta(2n) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} \rho(\tilde{z}) d \tilde{z} = 2 N(\kappa) \kappa^{2n} \sum_{j=0}^{\infty} \kappa^{2j+1} \frac{(2n-1)!!}{(2j + 2n+1)!!}; \\
\zeta(2n) &= 2 N(\kappa) \frac{\kappa^{2n+1}}{2n + 1} + 2 N(\kappa) \frac{\kappa^{2n+2}}{2n+1} \sum_{j=1}^{\infty} \kappa^{2(j-1)+1} \frac{(2(n+1)- 1)!!}{(2(j-1)+2(n+1)+1)!!} \\
 &= 2 N(\kappa) \frac{\kappa^{2n + 1}}{2n + 1} + \frac{\zeta(\kappa, 2n + 2)}{2n + 1}; \\
\zeta(\kappa, 2n + 2) &= (2n + 1) \zeta(\kappa, 2n) - 2 N(\kappa) \kappa^{2n + 1};
\end{align*}

\begin{align*}
\int_{-\kappa}^{+\kappa} \frac{1}{2 \sqrt{3}} \tilde{z}^{2n} d \tilde{z} = \frac{1}{\sqrt{3}} \frac{\kappa^{2n+1}}{2n + 1};
\end{align*}

\fi

\iffalse

\begin{align*}
\frac{\delta^2 x^c}{(x^c)^2} &\simeq \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \\
	&\lim_{2n \rightarrow +\infty}  \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	n c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} = \prod_{j=0}^{2n-1} \frac{c -j }{1 + j}; \\
 			c > 0:	n \begin{pmatrix} c \\ n \end{pmatrix}^2 = n \left( \prod_{j=0}^{n-1} \frac{c -j }{1 + j} \right)^2 < n;
 		\end{cases}\\
&\begin{pmatrix} c \\ j + 1 \end{pmatrix} \begin{pmatrix} c \\ 2n - j - 1 \end{pmatrix} 
		= \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} \frac{c - j}{j + 1} \frac{2n - j}{c - 2n + j + 1}; \\
&\sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j + 2 \end{pmatrix} = 
		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix}  \begin{pmatrix} c \\ 2 n - j \end{pmatrix}
		\frac{c - (2 n - j)}{2 n - j + 1} \frac{c - (2 n - j + 1)}{2 n - j + 2} \\
		&\largespace + \left(\frac{c^2}{2} + \frac{c - 2n}{2n + 1} c \right) \begin{pmatrix} c \\ 2n \end{pmatrix}; \\
&\lim_{2n \rightarrow +\infty} \sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n + 2 - j \end{pmatrix}  =
	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 
		+ \prod_{j=0}^{2 n - 1} \frac{c - j}{j + 1} \frac{1}{2} ((c - 1)^2 - 1)
\end{align*}

\begin{align}
\label{eqn: pow convergence}
\frac{\delta^2 x^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \nonumber \\
	&\frac{1}{2n} \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	\left| c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} \right| < \frac{\left| c \Gamma(c) \right|}{(2n - 1) !}; \\
 			c > 0:	\begin{pmatrix} c \\ n \end{pmatrix}^2 < 1;
 		\end{cases}
\end{align}


\begin{align}
\label{eqn: variance asymptotic}
\lim_{n \rightarrow +\infty}& \zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n-j, \kappa) = \frac{\kappa^n}{n}; \\
\label{eqn: log convergence}
\delta^2 \log(x \pm \delta x) \simeq& \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n, \kappa) \sum_{j=1}^{2n-1} \frac{1}{j} \frac{1}{2n-j} 
		= \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n, \kappa) \frac{1}{n} \sum_{j=1}^{2n-1} \frac{1}{j}  \nonumber \\
	\simeq&\; 2\nu(\kappa) \log(2) \sum_{n = 1}^{+\infty} \frac{(P(x) \kappa)^{2n}}{(2n)^2},
		\begin{cases}		
 			\text{Gaussian}:	\nu(\kappa) = N(\kappa) \kappa\\
 			\text{Uniform}:	\nu(\kappa) = 1,\eqspace \kappa = \sqrt{3}
		\end{cases}; \\
\label{eqn: pow convergence}
\frac{\delta^2 (x \pm \delta x)^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n, \kappa) 
 	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
	\simeq \nu(\kappa) \sum_{n = 1}^{+\infty} (P(x) \kappa)^{2n} \frac{\begin{pmatrix} 2c \\ 2n \end{pmatrix}}{2n};
\end{align}

\fi



\ifdefined\Verbose
\begin{figure}
\centering
\includegraphics[height=2.5in]{Gaussian_5_Moment.pdf} 
\captionof{figure}{
When $\kappa = 5$, the value of bound moment  $\zeta(2n, \kappa)$ (left y-axis) for Gaussian distribution with order $2n$ (x-axis) versus $2 N(\kappa) \frac{\kappa^{2n+1}}{2n+1}$ (left y-axis), and their ratio (right y-axis).
}
\label{fig: Gaussian_5_Moment}
\end{figure}

\begin{align}
\label{eqn: uniform moment}
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} \frac{1}{2 \sqrt{3}} d \tilde{z} = 2 \rho(\kappa) \frac{\kappa^{2n+1}}{2n + 1}, \;\; 0 \leq \kappa \leq \sqrt{3}; \\
\label{eqn: symetric asymptotic moment} 
\lim_{n \rightarrow +\infty}& \zeta(2n, \kappa) = 2 \rho(\kappa) \frac{\kappa^{2n+1}}{2n+1};
\end{align}
\fi

\begin{align}
\label{eqn: variance asymptotic}
\forall j&: \lim_{n \rightarrow +\infty} \zeta(n, \kappa) - \zeta(j, \kappa) \zeta(n-j, \kappa) = \frac{\kappa^n}{n};
\end{align}
Formula \eqref{eqn: variance asymptotic} shows that the asymptotic behavior of $\zeta(n, \kappa)$ when $n \rightarrow \infty$ determines if Formula \eqref{eqn: Taylor 1d variance} will converge.
\ifdefined\Verbose
For example, Formula \eqref{eqn: uniform moment} gives $\zeta(2n, \kappa)$ for Uniform distributions, Formula \eqref{eqn: symetric asymptotic moment} shows the asymptoticity for any symmetric distribution $\rho(\tilde{z})$, and Figure \ref{fig: Gaussian_5_Moment} shows $\zeta(2n, 5)$ for Normal distribution when $2n \rightarrow +\infty$.
\fi
\begin{itemize}
\item Formula \eqref{eqn: exp precision} for $e^{x \pm \delta x}$ and Formula \eqref{eqn: sin precision} for $\sin(x \pm \delta x)$ both converge unconditionally.

\item Formula \eqref{eqn: log precision} for $\log(x \pm \delta x)$ can be approximated by Formula \eqref{eqn: log convergence}, which converges when $P(x) < 1/\kappa$.

\item Formula \eqref{eqn: power precision} for $(x \pm \delta x)^c$ can be approximated by Formula \eqref{eqn: pow convergence}, which converges when $P(x) \lesssim 1/\kappa$ although the precise upper bound for $P(x)$ varies with $c$.
\end{itemize}
\begin{align}
\label{eqn: log convergence}
\rho(-\tilde{z}) =\rho(\tilde{z})&: \delta^2 \log(x \pm \delta x) \sim \sum_{n = 1}^{+\infty} \frac{(P(x) \kappa)^{2n}}{(2n)^2}; \\
\label{eqn: pow convergence}
\rho(-\tilde{z}) =\rho(\tilde{z})&: \frac{\delta^2 (x \pm \delta x)^c}{(x^c)^2} \sim \sum_{n = 1}^{+\infty} (P(x) \kappa)^{2n} \frac{\begin{pmatrix} 2c \\ 2n \end{pmatrix}}{2n};
\end{align}
Statistical Taylor expansion rejects the distributional zero of $\log(x)$ or $(x \pm \delta x)^c$ in the range of $P(x) > 1/\kappa$ statistically because of the divergence of Formulas \eqref{eqn: log convergence} and \eqref{eqn: pow convergence} mathematically, with $\zeta(2n, \kappa)$ providing the connection between these two perspectives.





\subsection{Statistical Bounding}

\iffalse
\begin{align}
\label{eqn: Uniform moment} 
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \frac{1}{2 \sqrt{3}} \tilde{z}^{2n} d \tilde{z} = \frac{1}{\sqrt{3}} \frac{\kappa^{2n+1}}{2n + 1}; \\
\label{eqn: Gaussian moment} 
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} N(\tilde{z}) d \tilde{z}
		= 2 N(\kappa) \kappa^{2n} \sum_{j=1}^{\infty} \kappa^{2j-1} \frac{(2n - 1)!!}{(2n-1 + 2j)!!} \\
	& \zeta(2, \kappa) = \zeta(0, \kappa) - 2 N(\kappa) \kappa;
\end{align}
\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Normal_Bounding_Leakage.pdf} 
\captionof{figure}{
Measured linear leakage $\omega(\kappa_s, N)$ (y-axis) for varying measuring bound range $\kappa_s$ (x-axis) and sample count $N$ (legend).
}
\label{fig: Normal_Bounding_Leakage}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Bounding_Factor_Leakage.pdf} 
\captionof{figure}{
Measured linear leakage $\omega(\kappa, N)$ (left y-axis) and corresponding measured bound range $\kappa$ (right y-axis) for varying sample count $N$ (x-axis) when the underlying distribution is Gaussian, with different measuring bound range $\kappa_s$ (legend).
}
\label{fig: Bounding_Factor_Leakage}
\end{figure}

\ifdefined\Verbose
\begin{figure}[p]
\else
\begin{figure}
\fi
\centering
\includegraphics[height=2.5in]{Normal_Function.pdf} 
\captionof{figure}{
Ideal leakage (y-axis) for varying sample count $\kappa_s$ (x-axis) for the selected function $f(x=1 \pm 0.1)$ (legend) when $\hat{\kappa} = 5$ for Gaussian input uncertainties.
}
\label{fig: Normal_Function}
\end{figure}

\ifdefined\Verbose
\begin{figure}[p]
\includegraphics[height=2.5in]{Normal_Function_Bounding.pdf} 
\captionof{figure}{
Ideal leakage (y-axis) for varying bound range $\kappa$ (x-axis) for the selected function $f(x=1 \pm 0.1)$ (legend) when $\hat{\kappa} = 6$ for Gaussian input uncertainties.
}
\label{fig: Normal_Function_Bounding}
\end{figure}
\fi

\begin{align}
\label{eqn: offset approximation}
f(x) - \overline{f(x)} &\simeq (\delta x)^2 \zeta(2, \kappa) f_x^{(2)}; \\
\label{eqn: variance approximation}
\delta^2 f(x) &\simeq (\delta x)^2 \zeta(2, \kappa) (f_x^{(1)})^2;
\end{align}

Figures \eqref{eqn: offset approximation} and \eqref{eqn: variance approximation} show that the first-order approximation of Formulas \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 1d variance} both contain the term $(\delta x)^2 \zeta(2, \kappa)$.
As $\kappa \rightarrow +\infty$, $\zeta(2, \kappa) \rightarrow 1$ such that $\delta^2 f(x)$ reaches a stable value but the convergence range of $\delta^2 f(x)$ reduces toward zero for Formulas \eqref{eqn: log precision} and \eqref{eqn: power precision}.
As a trade-off, the choice of $\kappa$ to calculate the stable $f(x) - \overline{f(x)}$ and $\delta^2 f(x)$ is defined as the \emph{ideal bound range} $\hat{\kappa}$, and the result is the \emph{ideal uncertainty bias} $\widehat{\overline{f(x)}} - f(x)$ and \emph{ideal variance} $\hat{\delta^2} f(x)$.
For a Uniform distribution, by definition $\hat{\kappa} = \sqrt{3}$.
For a Gaussian distribution, according to the 5-$\sigma$ rule for determining the statistical significance of experimental result \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}\cite{Probability_Statistics},  $\hat{\kappa} = 5$ by default.

Define \emph{linear leakage} $\omega(\kappa) \equiv 1 - \zeta(2, \kappa)$.
When sampling from a distribution, the sample mean $\overline{x}$ and sample deviation $\delta x$ approach the distribution mean $\mu$ and distribution deviation $\sigma$ respectively as the sample count $N$ increases \cite{Probability_Statistics}.
This yields the \emph{sample linear leakage} $\omega(\kappa, N)$ for the interval $[\overline{x} - \varrho \delta x, \overline{x} + \kappa \delta x]$, in contrast to the \emph{distributional linear leakage} $\omega(\kappa)$ for the interval $[\mu - \varrho \sigma, \mu + \kappa \sigma]$.
Because $\omega(\kappa) \neq \omega(\kappa, N)$ for finite $N$, let $\omega(\kappa) = \omega(\kappa_s, N)$, where $\kappa_s$ is the \emph{measuring bound range}, and $\kappa(\kappa_s, N)$ is the \emph{measured bound range}, with the latter used in calculating $\zeta(n, \kappa)$.

\begin{align}
\label{eqn: Gaussian result bounding leakage} 
\epsilon(\kappa) &= \omega(\kappa) + 2 N(\kappa) \kappa; \\
\label{eqn: Gaussian theoretical bounding leakage} 
\epsilon(\kappa) &= 1 - \xi(\frac{\kappa}{\sqrt{2}}); \\
\label{eqn: Gaussian experimental bounding leakage}
\epsilon(\kappa_s, N) &= 1 - \frac{1}{2} \xi(\frac{|\kappa_s \delta x - \overline{x}|}{\sqrt{2}}) - \frac{1}{2} \xi(\frac{|\kappa_s \delta x + \overline{x}|}{\sqrt{2}});
\end{align}
Define \emph{bounding leakage} $\epsilon(\kappa) \equiv 1 - \zeta(0, \kappa)$.
When the underlying distribution is Normal, Formula \eqref{eqn: Gaussian result bounding leakage} presents the difference between the linear leakage $\omega(\kappa)$ and the bounding leakage $\epsilon(\kappa)$, while Formulas \eqref{eqn: Gaussian theoretical bounding leakage} and \eqref{eqn: Gaussian experimental bounding leakage} give the distributional bounding leakage $\epsilon(\kappa)$ and the sample bounding leakage $\epsilon(\kappa_s, N)$ respectively, where $\xi()$ is the Normal error function \cite{Probability_Statistics}.
Figure \ref{fig: Normal_Bounding_Leakage} shows that $\varepsilon(\kappa_s) < \varepsilon(\kappa_s, N)$, and $\lim_{N \rightarrow \infty} \varepsilon(\kappa_s, N) = \varepsilon(\kappa_s)$ along the y-axis direction, and that $\kappa(\kappa_s, N) < \kappa_s$ and $\lim_{N \rightarrow \infty} \kappa(\kappa_s, N) = \kappa_s$ along the x-axis direction. 
Figure \ref{fig: Bounding_Factor_Leakage} sections Figure \ref{fig: Normal_Bounding_Leakage} along the y-axis direction for $\kappa_s=4,5,6$, and shows that to reach the stable variance for a $\kappa_s$, the required sample count is $N \gtrsim 10^3$, whereas the stable result bounding leakage is smaller for larger measuring bounding range $\kappa_s$.
It also shows that linear leakage converges to $0$ with increasing sample count $N$ faster when $\kappa_s$ is larger.

\begin{align}
\label{eqn: sum leakage}
\varepsilon(x \pm y) &= \frac{\omega_x (\delta x)^2 + \omega_y (\delta y)^2}{(\delta x)^2 + (\delta y)^2};
\end{align}
Define \emph{ideal leakage} as $\varepsilon \equiv 1 - \delta^2 f/\widehat{\delta^2} f \simeq \omega$, which quantifies the reliability of $\hat{\delta^2} f$ from the underlying distributions and the corresponding sample counts of all inputs, with Formula \eqref{eqn: sum leakage} as  an example.
Figure \ref{fig: Normal_Function} compares ideal leakage $\varepsilon$ and linear leakage $\omega$ for the selected functions when the input uncertainty distribution is Gaussian, and shows that the stable ideal leakages are less than $10^{-4}$ when $\hat{\kappa} = 5$, which is small enough for most applications. 
\ifdefined\Verbose
Figure \ref{fig: Normal_Function_Bounding} compares the ideal leakages for $\hat{\delta^2} f$ for the selected functions when different $\hat{\kappa}$ is used. 
\fi



\subsection{Dependency Tracing}

\begin{align}
\label{eqn: sum variance}
\delta^2 (f + g) =&\; \delta^2 f + \delta^2 g + 2 (\overline{fg} - \overline{f}\overline{g}); \\
\label{eqn: linear variance}
\delta^2 (c_1 f + c_0) =&\; c_1^2 \delta^2f;
\end{align}


When all inputs satisfy the uncorrelated uncertainty assumption, statistical Taylor expansion traces dependencies through the intermediate steps.
For example:
\begin{itemize}
\item Formula \eqref{eqn: sum variance} expresses $\delta^2 (f + g)$, The dependency tracing of is illustrated by $\delta^2 (f - f) = 0$, and $\delta^2 (f(x) + g(y)) = \delta^2 f + \delta^2 g$, with the latter corresponding to Formula \eqref{eqn: addition variance}.   

\item The dependence tracing of $\delta^2 (f g)$ is illustrated by $\delta^2 (f/f) = 0$, $\delta^2 (f f) = \delta^2 f^2$, and $\delta^2 (f(x) g(y)) = \overline{f}^2 (\delta^2 g) + (\delta^2 f) \overline{g}^2 +  (\delta^2 f) (\delta^2 g)$, with the latter corresponding to Formula \eqref{eqn: multiplication variance}.  

\item The dependency tracing of $\delta^2 f(g(x))$ is demonstrated by $\delta^2 (f^{-1}(f(x))) = (\delta x)^2$.  
For a reversible transformation such as matrix reversion or FFT (Fast Fourier Transformation), after a \emph{round-trip transformation} which is a forward transformation followed by a reverse transformation, the original inputs should be restored.

\item Formula \eqref{eqn: linear variance} gives the variance of the linear transformation of a function, which can be applied to other formulas for more general dependency tracing.

\end{itemize}
Statistical Taylor expansion employs dependency tracing to ensure that the calculated mean and variance satisfy statistics rigorously.
Dependency tracing also implies that the results of statistical Taylor expansion must remain path independent.
However, dependency tracing comes at a cost: variance calculations are generally more complex than value calculations and exhibits a narrower convergence range for input variables.


\subsection{Traditional Execution and Dependency Problem}

Dependency tracing requires applying statistical Taylor expansion on an analytic form of the function in one step.
This requirement often conflicts with conventional numerical methods for analytic functions:
\begin{itemize}

\item 
In conventional practice, an analytic expression is often decomposed into simpler, ostensibly independent arithmetic operations such as negation, addition, multiplication, division, square root, and library calls.
However, this decomposition introduces dependency problems.
For example, if $x^2 - x$ is calculated as $x^2 - x$, $x(x - 1)$, and $(x - \frac{1}{2})^2 - \frac{1}{4}$, only $(x - \frac{1}{2})^2 - \frac{1}{4}$ gives the correct result, while the other two give wrong results for wrong independence assumptions between $x^2$ and $x$, or between $x -1$ and $x$, respectively.

\item
Large calculations are often divided into sequential steps, such as computing $f(g(x))$ as $f(y)|_{y = g(x)}$.
This approach also introduces the dependency problem by ignoring dependency tracing within $g(x)$ affecting $f(g(x))$, such as $\overline{(\sqrt{x})^2} > \overline{\sqrt{x^2}}$ and $\delta^2 (\sqrt{x})^2 > \delta^2 \sqrt{x^2}$.

\item
Conditional executions are often employed to optimize performance and minimize rounding errors, for example, using Gaussian elimination to minimize floating-point rounding errors in matrix inversion \cite{Linear_Algebra}.  
For dependency tracing, such conditional executions should instead be replaced by direct matrix inversion as described in Section \ref{sec: matrix}.

\item
Traditionally, intermediate variables are widely used in computations; however, this practice disrupts dependency tracing by obscuring the relationships among the original input variables.

\end{itemize}
Dependency tracing therefore removes nearly all flexibility from traditional numerical executions, effectively eliminating the associated dependency problems.
Consequently, all conventional numerical algorithms must be reevaluated or redesigned to align with the principles of statistical Taylor expansion.








\section{Variance Arithmetic}
\label{sec: variance arithmetic}

Variance arithmetic implements statistical Taylor expansion.
It represents an imprecise value $x \pm \delta x$ using a pair of 64-bit standard floating-point numbers and uses floating-point arithmetic for computation.

Because of the finite precision and limited range of the floating-point representation, $\zeta(n, \kappa \leq 5)$ can be computed only to limited terms.
Consequently, the following numerical rules are introduced:
\begin{itemize}
\item \emph{finite}: The resulting value and variance must remain finite.

\item \emph{monotonic}: As a necessary condition for convergence, the last $20$ terms of the expansion must decrease monotonically in absolute value, ensuring that the probability of the expansion exhibiting an absolute increase is no more than $2^{-20} \simeq 9.53\; 10^{-7}$.

\item \emph{stable}: To avoid truncation error \cite{Numerical_Recipes}, the absolute value of the last expansion term must be less than $\epsilon$ times of both the result deviation and the result absolute value, in which $\epsilon \simeq 5.73\;10^{-7}$ is the bounding leakage for Gaussian distribution with $\hat{\kappa} = 5$.
This rule ensures sufficiently fast convergence in the context of monotonic convergence.

\item \emph{positive}: At every expansion order, the expansion variance must be positive.

\item \emph{reliable}: At every order, the deviation of the variance must be less than $1/5$ times the value of the variance.

\end{itemize}

For simplicity of discussion, the Taylor coefficients in Formulas \eqref{eqn: Taylor 1d} and \eqref{eqn: Taylor 2d} are assumed to be precise.


\subsection{Monotonic}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for Gaussian uncertainty.
The corresponding resulting uncertainty bias and deviation are also shown (right y-axis).
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
}
\label{fig: Pow_Conv_Edge}
\end{figure}

\ifdefined\Verbose

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for uniform uncertainty.
The corresponding resulting uncertainty bias and deviation are also shown (right y-axis).
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
When $c \in [0, 1]$, uncertainty bias is negative so it does not show up in the plot.
}
\label{fig: Pow_Conv_Edge_Uniform}
\end{figure}

\fi

Beyond an upper bound $\delta x$, the expansion is no longer monotonic for $e^{x \pm \delta x}$, $\log(x \pm \delta x)$, and $(x \pm \delta x)^c$.
For Gaussian input uncertainty with $\hat{\kappa}=5$:
\begin{itemize}
\item 
For $e^{x \pm \delta x}$, $\delta x \lesssim 19.864$ and $P(e^{x \pm \delta x}) \lesssim 1681.767$ regardless of $x$.
These limits follow directly from the relationship $\delta x \rightarrow P(e^x)$, as indicated in Formula \eqref{eqn: exp precision}.

\item 
For $\log(x \pm \delta x)$, $P(x) \lesssim 0.20086$ and $\delta \log(x \pm \delta x) \lesssim 0.213$ regardless of $x$.
These limits follow directly from the relationship $P(x) \rightarrow \delta \log(x)$, as indicated in Formula \eqref{eqn: log precision}.

\item
For $(x \pm \delta x)^c$, except when $c$ is a natural number, the upper bound $P(x)$ is close to $1/5$ but increasing with $c$. 
This trend is shown in Figure \ref{fig: Pow_Conv_Edge}.
\end{itemize}
\ifdefined\Verbose
Similar trends holds when the input uncertainty follows other distribution.
Figure \ref{fig: Pow_Conv_Edge_Uniform} shows the upper bound $P(x)$ for $(x \pm \delta x)^c$ when the input uncertainty distribution is Uniform.
It has almost the same trend of upper bounds $P(x)$ increasing with $c$ as shown in Figure \ref{fig: Pow_Conv_Edge_Uniform} but just with different y-scaling: $1/\sqrt{3}$ instead of by $1/5$.
\else
Similar trends holds when the input uncertainty follows Uniform distribution.
\fi 



\subsection{Positive}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for Gaussian uncertainty.
The corresponding resulting uncertainty bias and deviation are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge}
\end{figure}

\ifdefined\Verbose
\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for uniform uncertainty.
The corresponding resulting uncertainty bias and deviation are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge_Uniform}
\end{figure}
\fi

In addition to convergence, the variance expansion may yield negative results, as in Formula \eqref{eqn: sin precision} for $sin(x \pm \delta x)$.
Figure \ref{fig: Sin_Conv_Edge} shows that the upper bound of $\delta x$ for $sin(x \pm \delta x)$ varies periodically between $0.318 \pi$ and $0.416 \pi$ for Gaussian input uncertainty.
Beyond this upper bound, the result deviation is no longer positive at some expansion order.
Conceptually, the upper-bound for $\delta x$ should not be much more than $\pi/2$ to make the result meaningful, which is satisfied as shown in Figure \ref{fig: Sin_Conv_Edge}.
\ifdefined\Verbose
Similar trend holds when the input uncertainty is uniform but with larger upper bound $\delta x$, as shown in Figure \ref{fig: Sin_Conv_Edge_Uniform}.
\else
Similar trend holds when the input uncertainty is uniform but with larger upper bound $\delta x$ still less than $\pi/2$.
\fi


\subsection{Floating-Point Rounding Errors}

Variance arithmetic incorporates floating-point rounding errors as the $\delta x$ when converting a floating-point value $x$ into $x \pm \delta x$. 
Unless all the least 20 bits of the significand of $x$ are zero, $\delta x$ is assumed to be $1/\sqrt{3}$ times of the ULP (Unit in the Last Place \cite{Floating_Point_Standard}) of $x$, because rounding errors are shown to be uniformly distributed within ULP \cite{Prev_Precision_Arithmetic}.



\ifdefined\Verbose

\subsection{Comparison}

\iffalse

Statistically the less relation between two imprecise values $x \pm \delta x$ and $y \pm (\delta y)^2$ is calculated by Formula \ref{eqn: x < y}:
\begin{align}
& z \equiv \frac{\tilde{y} - y}{\delta y}; \eqspace \tilde{y} = \delta y z + y; \\
& x - \Delta x < z \delta y + y < x + \Delta x; \\ & x - \Delta x - y < z \delta y < x + \Delta x - y \\
& - \Delta y < z \delta y < \Delta y; \\
p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) & = 
  \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
    \int_{x - \Delta x}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
  \int_{-\frac{\Delta x}{\delta x}}^{\frac{\tilde{y} - x}{\delta x}} N(z) d z \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
      \frac{1}{2}(\frac{\tilde{y} - x}{\sqrt{2} \delta x}) - \zeta(\frac{-\Delta x}{\sqrt{2} \delta x})) \;d \tilde{y}; \\
& = \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x})\right) N(z) d z; \\
p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) & =     
   \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x})\right) N(z) d z;
\end{align}
Formula \eqref{eqn: x < y} seems correct because it predicts $p(x \le y) = p(y \ge x)$:
\begin{align}
& \frac{d}{d \tilde{y}} \int_{-\infty}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} = \rho(\tilde{y}, x, \delta x); \\
p(x< y) & = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \rho(\tilde{y}, x, \delta x) d \tilde{x} d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \;d \rho(\tilde{y}, x, \delta x) \\
& = 0 - \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) \;d \rho(\tilde{y}, y, \delta y) \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) 
\int_{\tilde{y}}^{+\infty} \rho(\tilde{x}, y, \delta y) 
d \tilde{x} \;d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{x}, x, \delta x) 
\int_{\tilde{x}}^{+\infty} \rho(\tilde{y}, y, \delta y) 
d \tilde{y} \;d \tilde{x}
\end{align}

Two imprecise values can be compared directly for less or greater relation when their ranges $(x - \Delta x, x + \Delta x)$ and $(y - \Delta y, y + \Delta y)$ do not overlap. 
Otherwise, statistically, Formulas \eqref{eqn: x < y} and \eqref{eqn: x > y} gives the probability for less or greater relation between $x \pm (\delta x)^2 \le y \pm (\delta y)^2$, in which $\min()$ and $\max()$ are minimal and maximal functions, respectively.
Formula \eqref{eqn: no =} shows that two imprecise variable can not be equal statistically.
Let $x = y$, $\delta x = \delta y$ and $\Delta x = \Delta y$, Formulas \eqref{eqn: x < y} and \eqref{eqn: x > y} shows that two conceptually equal imprecise values has $1/2$ chance to be one less than the other, and $1/2$ chance to be one more than the other.
Thus,
\begin{align}
\label{eqn: x < y}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) = 
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}, \kappa) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x}, \kappa)\right) N(z) d z; \\
\label{eqn: x > y}
& p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) =     
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}, \kappa) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}, \kappa)\right) N(z) d z; \\
\label{eqn: no =}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) + p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) = 1;
\end{align}

\fi

Two imprecise values can be compared statistically based on their difference.

When the value difference is zero, the two imprecise values are considered equal.  
In statistics, such two values have a $50\%$ possibility of being either less than or greater to each other but zero probability of being exactly equal \cite{Probability_Statistics}.
In variance arithmetic, however, they are treated as neither less nor greater than each other and therefore are considered equal.

Otherwise, the standard z-statistic method \cite{Probability_Statistics} is applied to determine whether two imprecise values are statistically equal, less than, or greater than each other.
For example, the difference between $1.002 \pm 0.001$ and $1.000 \pm 0.002$ is $0.002 \pm 0.00224$, yielding $z = 0.002 / 0.00224$.
The probability that they are not equal is $\xi(|z|/\sqrt{2}) = 62.8\%$, in which $\xi(z)$ is the cumulative distribution function for Normal distribution \cite{Probability_Statistics}.
If the threshold probability for inequality is set at $50\%$, then $1.000 \pm 0.002 < 1.002 \pm 0.001$.
Alternatively, an equivalent bounding range for z can be used, such as $|z| \leq 0.67448975$ for an equal probability threshold of $50\%$.

Because the result of comparison depends on threshold probability which is application specific, comparison is not part of variance arithmetic.

\fi


\section{Verification of Variance Arithmetic}
\label{sec: validation}

Analytic functions or algorithms with precisely known results are used to evaluate the outputs of variance arithmetic based on the following statistical properties: 
\begin{itemize}

\item \emph{Value error}: the difference between the numerical result and the corresponding known precise analytic result.

\item \emph{Normalized error}: the ratio of a value error to the corresponding result deviation from statistical Taylor expansion.

\item \emph{Error deviation}: the standard deviation of normalized errors.

\item \emph{Error distribution}: the histogram of the normalized errors.

\end{itemize}

Once input error from every source is accounted precisely, \emph{ideal coverage} is achieved in either context:
\begin{itemize}
\item \emph{Distribution Test}:
When comparing the calculated mean and deviation with the result data set, the error deviation is exactly $1$ and the error distribution is Normal, regardless of input uncertainty distribution.
Such convergence to Normal distribution occurs rapidly \cite{Prev_Precision_Arithmetic} because of the central limit theorem \cite{Probability_Statistics}.

\item \emph{Value Test}: 
When comparing values one-by-one between a calculated data set and the corresponding result data set, the error deviation is much less than $1$ and the error distribution is Delta.
For example, a round-trip test is a value test.

\end{itemize}

However, if the input uncertainty is known only to order of magnitude, \emph{proper coverage} is achieved when the error deviations fall within the range $[0.1, 10]$.

When an input contains unspecified errors, such as numerical errors in library functions or floating-point rounding errors, Gaussian noise with progressively increasing deviations can be added, until ideal coverage is attained.
The minimal noise deviation required provides a good estimate of the magnitude of the unspecified input uncertainty deviations.
Achieving ideal coverage serves as a necessary verification step to ensure that statistical Taylor expansion has been applied correctly within the given context.
The input noise range that yields ideal coverage defines the ideal application range for the analytic function.




\section{Polynomial}
\label{sec: polynomial}

Formula \eqref{eqn: polynomial Taylor} presents polynomial Taylor expansion:
\begin{align}
\label{eqn: polynomial Taylor}
\sum_{j=0}^{N} c_j (x + \tilde{x})^j &= \sum_{j=0}^{N} \tilde{x}^{j} P_j, \eqspace
	P_j \equiv \sum_{k=0}^{N-j} x^{k - j} c_{j + k} \begin{pmatrix} j + k \\ j \end{pmatrix};
\end{align}
Because the maximal expansion term using Formula \eqref{eqn: polynomial Taylor} is $\tilde{x}^{2N}$, $N$ in Formula \eqref{eqn: polynomial Taylor} can reach only half of the maximal expansion order of Formula \eqref{eqn: Taylor 1d variance}, for example, $224 = 448/2$ when the input uncertainty is assumed to be Gaussian, and $\hat{\kappa} = 5$.

\subsection{Residual Error}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Poly_x.pdf}
\captionof{figure}{
Residual error of $\sum_{j=0}^{224} x^j - \frac{1}{1 - x}$ vs $x$ (x-axis).
The left y-axis shows both the value and the uncertainty of the residual errors.
The right y-axis indicates the expansion order needed to reach stable value for each $x$. 
}
\label{fig: Poly_x}
\end{figure}

Figure \ref{fig: Poly_x} shows the residual error of $\sum_{j=0}^{224} x^j  - \frac{1}{1 - x}$.
It also displays the required expansion orders for $\frac{1}{1 - x}$, which are all less than $224$.
Therefore, the residual error reflects solely the rounding error between $\sum_{j=0}^{224} x^j$ and $\frac{1}{1 - x}$.
A detailed analysis indicates that the maximal residual error is four times the ULP of $\frac{1}{1 - x}$.
In all cases, the calculated uncertainty bounds the residual error effectively for all $x$, with an error deviation of $2.60$ when the expansion order is less than $224$.
Variance arithmetic can provide proper coverage for rounding errors.  


\subsection{Continuity}

In variance arithmetic, the result mean, variance and error distribution are generally continuous across parameter space.
For example, when $c$ is a natural number $n$, $(x \pm \delta x)^c$ becomes a polynomial which has no upper bound for $\delta x$, in contrast to when $c$ is not a natural number as shown in Figure \eqref{fig: Pow_Conv_Edge}.
However, the result mean, variance and error distribution of $(x \pm \delta x)^c$ remain continuous across $c = n$.


\subsection{Distributional Pole}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_at_x=0.pdf} 
\captionof{figure}{
Error distributions for $(x \pm 0.2)^n$, with $x = 0, -0.2, +0.2$, and $n = 2, 3$, as indicated in the legend.
}
\label{fig: Poly_Continuity}
\end{figure}

A statistical bounding range in variance arithmetic can include a distributional pole, such as around $(0 \pm \delta x)^c, c > 1$.
The presence of such poles does not disrupt the continuity of the result mean, variance, or error distribution.
Figure \ref{fig: Poly_Continuity} illustrates the error distributions of $(x \pm 0.2)^n$ when $x = 0, -0.2, +0.2$ and $n = 2, 3$.
\begin{itemize}
\item When the second derivative is zero, the resulting distribution is symmetric two-sided and Delta-like, such as when $n = 3, x = 0$.

\item When the second derivative is positive, the resulting distribution is right-sided Delta-like, such as the distribution when $n = 2, x = 0$, or when $n = 2, x = \pm 0.2$, or when $n = 3, x = 0.2$.

\item When the second derivative is negative, the resulting distribution is left-sided and Delta-like, such as when $n = 3, x = -0.2$, which is the mirror image of the distribution when $n = 3, x = 0.2$.

\end{itemize}
In each case, the transition from $x = 0$ to $x = \pm 0.2$ is continuous for the resulting mean, deviation and error distribution.




\section{Matrix Calculations}
\label{sec: matrix}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) of adjugate matrix $\widetilde{\mathbf{M}}^A - \mathbf{M}^A$ as a function of input noise precision (x-axis) and matrix size (y-axis).
}
\label{fig: Adjugate_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Forward_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) as a function of input noise precision (x-axis) and matrix size (y-axis) for the difference of the two sides of Formula \eqref{eqn: adjugate matrix}.
}
\label{fig: Forward_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Matrix_Determinant_Prec_vs_Condition.pdf} 
\captionof{figure}{
Linear correlation between the precision of a matrix determinant (y-axis) to its condition number (x-axis).
The legend shows the size of the matrix, as well as the type of the matrix as \textit{Random} for randomly generated matrix, and \textit{Hilbert} as the Hilbert matrix.
}
\label{fig: Matrix_Determinant_Prec_vs_Condition}
\end{figure}


\begin{align}
\label{eqn: random determinant}
|\widetilde{\mathbf{M}}| &\equiv \sum_{[p_1\dots p_n]_n} \$ [p_1\dots p_n]_n  \prod_{i=1 \dots n} (x_{i,p_i} + \tilde{x}_{i,p_{i}}) \\
\label{eqn: determinant Taylor expansion}
	&= \sum_{m=1 \dots n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_{m}]_n} 
		\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_{m}]_n} \prod_{i=1 \dots m}^{i \in \{i_1 \dots i_m\}} \tilde{x}_{i,p_{i}}; \\
\label{eqn: determinant mean}
\overline{|\mathbf{M}|} &= |\mathbf{M}|; \\
\label{eqn: determinant variance}
\delta^2 |\mathbf{M}| &= \sum_{m=1}^{n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_m]_n}
  	|\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}|^2 \prod _{k=1 \dots n}^{i_k \in \{i_1 \dots i_m\}} (\delta x_{i_k, j_k})^2; 
\end{align}

Let $[j_1, j_{2} \dots j_m]_n$ denote a permutation of length $m$ from the vector $(1,2\dots n)$, and let $<j_1, j_{2} \dots j_m>_n$ for the ordered permutation of length $m$ \cite{Linear_Algebra}. 
Let $\mathbf{M}$ be a square matrix of size $n$ with element $x_{i,j} \pm \delta x_{i,j}, i, j = 1,2,\dots n$ at row index $i$ and column index $j$. 
Fourmula \eqref{eqn: determinant Taylor expansion} presents the Taylor expansion of the determinant for matrix $\mathbf{M}$ when the uncertainties of matrix elements are all independent of each other, whereas Formulas \eqref{eqn: determinant mean} and \eqref{eqn: determinant variance} gives the statistical Taylor expansion of the determinant $|\mathbf{M}|$.
In Formula \eqref{eqn: determinant variance}, $\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}$ is a sub-matrix for $\mathbf{M}$, in which $<i_1 \dots i_m>_n$ contains the row indices, and $[j_1 \dots j_m]_n$ contains the column indices  \cite{Linear_Algebra}. 

The square matrix whose element is $(-1)^{i+j}|M_{j,i}|$ is defined as the \emph{adjugate matrix} \cite{Linear_Algebra} $\mathbf{M}^A$ to the original square matrix $\mathbf{M}$. 
By assigning a random integer between $[-2^8, +2^8]$ to each element, $\mathbf{M}$ is created such that $\mathbf{M}^A$ can be calculated precisely using floating-point arithmetic when $n \leq 7$.
By adding Gaussian noise $\tilde{x}_{i,j}$ of deviation $\delta x$ to each element of $\mathbf{M}$, $\widetilde{\mathbf{M}}$ is created and $|\widetilde{\mathbf{M}}|$ is calculated as shown in Formula \eqref{eqn: random determinant}.
The value error of $\mathbf{M}^A$ is the difference between $\widetilde{\mathbf{M}^A}$ and $\mathbf{M}^A$, whereas the result deviation is calculated using Formula \eqref{eqn: determinant variance}, such that $\widetilde{\mathbf{M}^A} - \mathbf{M}^A$ is a distribution test.
Figure \ref{fig: Adjugate_Error_vs_Size_Noise} shows that the error deviations of $\widetilde{\mathbf{M}^A} - \mathbf{M}^A$ are very close to $1$ except when the matrix size is $8$ and the input uncertainty is $\delta x = 10^{-17}$.
This abnormality is caused by floating-point rounding errors only at this point, to result in a error deviation of $3.2$. 
Figure \ref{fig: Adjugate_Error_vs_Size_Noise} demonstrates a typical distribution test of error deviation versus input noise deviation $\delta x$ and another algorithm specific dimension, with ideal coverage for all region except when $\delta x \rightarrow 0$.

\begin{align}
\label{eqn: adjugate matrix}
\mathbf{M} \times \mathbf{M}^A &= \mathbf{M}^A \times \mathbf{M} = |\mathbf{M}| \mathbf{I}; \\
\label{eqn: inverse matrix}
\mathbf{M}^{-1} &\equiv\; \mathbf{M}^A / |\mathbf{M}|; \\
\label{eqn: inverse matrix 2 variance}
\delta^2 \left( \begin{matrix} w, x \\ y, z \end{matrix} \right)^{-1} \simeq&\; \frac{
	 	\left( \begin{matrix} z^4, x^2 z^2 \\ y^2 z^2,  x^2 y^2 \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} y^2 z^2, w^2 z^2 \\ y^4, w^2 y^2 \end{matrix} \right) (\delta x)^2}{(w z - x y)^4} + \\
	&\; \frac{\left( \begin{matrix} x^2 z^2, x^4 \\ w^2 z^2, w^2 x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} x^2 y^2, w^2 x^2 \\ w^2 y^2, w^4 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^4}; \nonumber
\end{align}
Let $\mathbf{I}$ be the identity matrix for $\mathbf{M}$ \cite{Linear_Algebra}.
Formula \eqref{eqn: adjugate matrix} shows the relationship between $\mathbf{M}^A$ and $\mathbf{M}$ which leads to the definition of inverse matrix $\mathbf{M}^{-1}$ in Formula \eqref{eqn: inverse matrix} \cite{Linear_Algebra}.
$\tilde{\mathbf{M}} \times \tilde{\mathbf{M}}^A - |\tilde{\mathbf{M}}| \mathbf{I}$ tests each element value such that it is a value test.
Figure \ref{fig: Forward_Error_vs_Size_Noise} shows the error deviation of a typical value test: the error deviation decreases linearly with increasing input uncertainty deviation $\delta x$, because the values should match unless at $\delta x \rightarrow 0$ when floating-point rounding errors dominate.
The matrix size is the algorithm specific dimension in Figure \ref{fig: Forward_Error_vs_Size_Noise}.

Because an element of the original matrix $\mathbf{M}$ appears multiple times in Formula \eqref{eqn: inverse matrix}, its variance, as obtained using Formula \eqref{eqn: Taylor 2d variance}, is very complicated. 
For example, Formula \eqref{eqn: inverse matrix 2 variance} shows the simplest case for Formula \eqref{eqn: inverse matrix}: the first-order approximation of a 2x2 matrix.
Contrary to conventional approach, statistical Taylor expansion uses Formula \eqref{eqn: inverse matrix} for matrix inversion instead of Gaussian elimination \cite{Numerical_Recipes} because logically, the result should be symmetric for all matrix elements, as demonstrated by Formula \eqref{eqn: inverse matrix 2 variance}.

In Formula \eqref{eqn: inverse matrix}, $\mathbf{M}^{-1}$ is dominated by $1/|\mathbf{M}|$, suggesting that the precision of $\mathbf{M}^{-1}$ is largely determined by the precision of $|\mathbf{M}|$.
Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} shows that a strong linear correlation exists between conditional numbers \cite{Linear_Algebra} and the corresponding determinant precision of the matrices.
As a reference, Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} presents the Hilbert matrix \cite{Linear_Algebra} for each matrix size and shows that the Hilbert matrices also follow the linear relation between determinant precision and condition number.
Thus, determinant precision can replace matrix condition number to quantify overall matrix stability \cite{Linear_Algebra}, and Formula \eqref{eqn: inverse matrix 2 variance} further shows the stability of each matrix element.


\section{Mathematical Library Functions}
\label{sec: Math Library}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|} 
\hline 
Basic Function  & $e^{x \pm \delta x}$           & $\log(x \pm \delta x)$              & $(1 \pm \delta x)^c$             & $\sin(x \pm \delta x)$ \\ 
\hline 
Range             & $x \in [-100, +100]$            & $x \in [1/32, 32]$                     & $c \in [-3, +3]$                     & $x \in [-\pi, +\pi]$     \\
\hline 
Uncertainty      & $\delta x \in [10^{-15}, 1]$ & $\delta x \in [10^{-15}, 0.2 x]$ & $\delta x \in [10^{-15}, 0.2]$ & $\delta x \in [10^{-15}, 1]$     \\
\hline 
Error Deviation & $1.000 \pm 0.010$              & $0.999 \pm 0.011$                  & $0.989 \pm 0.104$                 & $0.978 \pm 0.150$ \\
\hline 
\end{tabular}
}
\captionof{table}{
The result error deviations of selected basic functions with Gaussian input noise $x \pm \delta x$ when $\delta x > 10^{-15}$, obtained by variance arithmetic.
The error deviation for $(1 \pm \delta x)^c$ can be improved to $0.998 \pm 0.034$ if diverging regions are excluded.
The error deviation for $\sin(x \pm \delta x)$ can be improved to $1.000 \pm 0.010$ if pole regions are excluded.
}
\label{tbl: basic functions}
\end{table}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_X_Dev.pdf} 
\captionof{figure}{
Error deviation for $\sin(x \pm \delta x)$ as a function of $x$ and $\delta x$.
The x-axis represents $x$ values between $-\pi$ and $+\pi$.
The y-axis represents $\delta x$ values between $-10^{-16}$ and $1$.
The z-axis shows the corresponding error deviations. 
}
\label{fig: Sin_X_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{ExpLog_Error.pdf} 
\captionof{figure}{
Values and uncertainties of $\log(e^x) - x$ and $e^{\log(x)} - x$ as functions of $x$, evaluated in increment of $0.1$.
When $x$ is 2's fractional such as $1/2$ or $1$, the resulting uncertainties are significantly smaller because of floating-point representation.
}
\label{fig: ExpLog_Error}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Error.pdf} 
\captionof{figure}{
Normalized errors of $(x^p)^{\frac{1}{p}} - x$ as functions of $x$ and $p$.
}
\label{fig: Power_Error}
\end{figure}


Table \ref{tbl: basic functions} shows that by sampling random input $\tilde{x}$ from Gaussian noise $x + \delta x$, Formulas \eqref{eqn: exp precision}, \eqref{eqn: log precision}, \eqref{eqn: sin precision}, and \eqref{eqn: power precision} provide almost perfect characterization $f(x) \pm \delta f$ for the result distributions of $f(\tilde{x})$ in which $f(\tilde{x})$ is the corresponding math library functions.
Table \ref{tbl: basic functions} shows the results from Python, which differs slightly from those from either C++ or Java.

The result error deviation is similar to that shown in Figure \ref{fig: Adjugate_Error_vs_Size_Noise} but with the specific dimension as $x$ in $e^{x \pm \delta x}$ and $\log(x \pm \delta x)$, or $c$ in $(1 + \delta x)^c$.
The coverage is proper when $ \delta x < 10^{-15}$.

Figure \ref{fig: Sin_X_Dev} shows that the error deviation for $\sin(x + \delta x)$ is $1.000 \pm 0.010$, except approaching $0$ when $x=\pm \pi/2$ and $\delta x < 10^{-8}$.
Near a distributional pole, the input uncertainty is suppressed, resulting in zero error deviation.
As expected, $\delta^2 \sin(x)$ exhibits the same periodicity as $\sin(x)$.
The numerical errors of the library functions $\sin(x)$ and $\cos(x)$ over a larger range of $x$ are examined in greater detail in Section \ref{sec: FFT}.

To test $f^{-1}(f(x)) - x = 0$ when $\delta x = 0$ for the library functions:
\begin{itemize}
\item Figure \ref{fig: ExpLog_Error} shows that the value errors in $e^{\log(x)} - x$ are much less than those in $\log(e^x) - x$. 
For $\log(e^x) - x$, the error deviation is $0.41$ when $|x| \leq 1$, or $0$ otherwise.

\item Figure \ref{fig: Power_Error} shows that the error deviation for $(x^p)^{1/p} - x$ is $0.56$,  dependent on neither $x$ nor $p$.
\end{itemize}
The reasons why $(x^p)^{1/p} - x$ has larger value errors than $\log(e^x) - x$, and $e^{\log(x)} - x$ has nearly no value error, are not clear.
It could relate to floating-point representation which stores each value $x$ as $\log_2(x)$.
Figure \ref{fig: Bounding_Factor_Leakage} also suggests that using linear approximation $e^{x \pm \delta x}$ and $\log(x \pm \delta x)$ have less error than $(x \pm \delta x)^p$.




\ifdefined\Verbose

\section{Moving-Window Linear Regression}
%\label{sec: Moving-Window Linear Regression}

\subsection{Moving-Window Linear Regression Algorithm}

\iffalse
\begin{align*}
\alpha_{j} &= \sum_{X=-H}^{H - 1} Y_{j-H+X}; \\
\beta_{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X} \\
 	&= \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} + H Y_{j - 2 H - 1} + H Y_{j} - \sum_{X=-H}^{H - 1} Y_{j-H+X} \\
 	&= \beta_{j - 1} + H Y_{j - 2 H - 1} + H Y_{j} - \alpha_{j}; \\
\delta^2 \alpha_{j} &= \sum_{X=-H}^{H - 1} (\delta Y_{j-H+X})^2; \\
\delta^2 \hat{\beta}_{j} &\equiv \sum_{X=-H}^{H} X (\delta Y_{j-H+X})^2 = \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} 
		+ H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
	&= \delta^2 \hat{\beta}_{j -1} + H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2 \\
	&= \sum_{X=-H - 1}^{H - 1} (X + 1)^2 (\delta Y_{j-H+X})^2 - H^2 (\delta Y_{j-2H-1})^2 + H^2 (\delta Y_{j})^2 
		 - \sum_{X=-H}^{H - 1} (2 X + 1) (\delta Y_{j-H+X})^2 \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2
		 - 2 \sum_{X=-H}^{H} 2 X (\delta Y_{j-H+X})^2 - \delta^2 \alpha_{j} \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2 - 2 \delta^2 \hat{\beta}_{j}  - \delta^2 \alpha_{j};
\end{align*}
\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Value.pdf} 
\captionof{figure}{ 
Result of fitting $\alpha + \beta\; Y$ to a time-series input $Y$ within a moving window of size $2*2 + 1$.
The x-axis indicates the time index.
The y-axis on the left corresponds to the value of $Y$, $\alpha$, and $\beta$, while the y-axis on the right corresponds to the uncertainty of $\alpha$ and $\beta$.
The uncertainty for $Y$ is fixed at $0.2$.
In the legend, \textit{Unadjusted} refers to results obtained by directly applying Formulas \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} using variance arithmetic, whereas \textit{Adjusted} refers to using Formulas \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} for $\alpha$ and $\beta$ values but Formulas \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} for their variances.
}
\label{fig: Moving_Linear_Fit_Value}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Error.pdf} 
\captionof{figure}{ 
Error deviations of the $\alpha + \beta\; Y$ fit vs time index. 
The x-axis represents the time index.
The y-axis on the left corresponds to the error deviation.
For reference, the input time-series signal $Y$ is also plotted, with its values indicated on the y-axis on the right.
}
\label{fig: Moving_Linear_Fit_Error}
\end{figure}



Formulas \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} provide the least-square line-fit of $Y = \alpha + \beta X$ between two set of data ${Y_j}$ and ${X_j}$, where $j$ is an integer index identifying $(X, Y)$ pairs in the sets \cite{Numerical_Recipes}.
\begin{align}
\label{eqn: linear regression 0}
\alpha &= \frac{\sum_{j} Y_{j} }{\sum_{j} 1}; \\
\label{eqn: linear regression 1}
\beta &= \frac{\sum_{j} X_{j} Y_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} Y_{j}}
    {\sum_{j} X_{j} X_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} X_{j} };
\end{align}

In many applications, data set ${Y_j}$ denotes an input data stream where $j$ represents the time index or sequence index.
${Y_j}$ is thus referred to as a time-series input, with $j$ corresponding to $X_j$.  
A moving window algorithm \cite{Numerical_Recipes} is applied within a small window centered on each $j$.  
For each calculation window, ${X_j = -H, -H+1 \dots H-1, H}$ where $H$ is an integer constant specifying the half width of the window.
This choice ensures $\sum_{j} X_{j} = 0$, which simplifies Formulas \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} into Formulas \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.

\begin{align}
\label{eqn: time-series linear regression 0}
\alpha _{j} &= \alpha \; 2 H = \sum_{X=-H+1}^{H} Y_{j-H+X}; \\
\label{eqn: time-series linear regression 1}
\beta _{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X}; 
\end{align}
The values of $(\alpha _{j}, \beta _{j})$ can be derived from the previous values $(\alpha _{j-1}, \beta _{j-1})$, allowing Formulas \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} to be reformulated into the progressive moving-window calculation given by Formulas \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.
\begin{align}
\label{eqn: moving-window linear regression 0}
\beta _{j} &= \beta _{j-1} - \alpha _{j-1} + H \left(Y_{j-2H-1} + Y_{j} \right); \\
\label{eqn: moving-window linear regression 1}
\alpha_{j} &= \alpha _{j-1} - Y_{j-2H-1} + Y_{j};
\end{align}


\subsection{Variance Adjustment} 

\begin{align}
\label{eqn: moving-window linear regression variance 0}
\delta^2 \alpha_{j} &= \sum_{X=-H+1}^{H} (\delta Y_{j-H+X})^2 = \delta^2 \alpha_{j-1} - (\delta Y_{j-2H})^2 + (\delta Y_{j})^2; \\
\label{eqn: moving-window linear regression variance 1}
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2;
\end{align}
When the time series contains uncertainty, directly applying Formulas \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} results in a loss of precision since both formulas reuse each input multiple times, thereby accumulating the variance of that input with every reuse.
To prevent this, $\alpha_j$ and $\beta_j$ should still be calculated progressively using Formulas \eqref{eqn: moving-window linear regression 1} and \eqref{eqn: moving-window linear regression 0}, respectively, while the variances should instead be computed using Formulas \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1}, respectively.
Formula \eqref{eqn: moving-window linear regression variance 1} is not progressive because the progressive form of $\delta^2 \beta_j$ is more  expensive in computation than Formula \eqref{eqn: moving-window linear regression variance 1}.

Figure \ref{fig: Moving_Linear_Fit_Value} shows that the input signal $Y_j$ consists of the following components:
\begin{enumerate}
\item An increasing slope for $j = 0 \dots 9$.

\item A decreasing slope for $j = 1 \dots 39$.

\item A sudden jump of magnitude $+10$ at $j=40$

\item A decreasing slope for $j = 41 \dots 49$.
\end{enumerate}
For each increment of $j$, the increasing and the decreasing rates are $+1$ and $-1$, respectively.

The specified input uncertainty is fixed at $0.2$.
Normal noise with a deviation of $0.2$  is added to the slopes, except for the segment $j = 10 \dots 19$ where Normal noise with a deviation of $2$ is introduced, representing actual uncertainty $10$ times larger than the specified uncertainty.

Figure \ref{fig: Moving_Linear_Fit_Value} also presents the results of the moving window fitting of $\alpha + \beta\; Y$ versus the time index $j$.
The fitted values of $\alpha$ and $\beta$ follow the expected behave, exhibiting a characteristic delay of $H$ in $j$.
When \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are applied to compute the uncertainties of $\alpha$ and $\beta$, both uncertainties increase exponentially with the time index $j$.
In contrast, when Formulas \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are used exclusively for value calculation, while Formulas \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} are applied for variance computation, the resulting uncertainties of $\alpha$ and $\beta$ are $\frac{\delta Y}{\sqrt{2H+1}}$, and $\frac{\delta Y}{\sqrt{\frac{H (H+1)(2H+1)}{3}}}$.
Both are less than the input uncertainty $\delta Y$, because of the averaging effect of the moving window.


\subsection{Unspecified Input Error}

To determine the error deviations of $\alpha$ and $\beta$, the fitting procedure is applied to multiple time-series data sets, each generated with independent noise realizations.
Figure \ref{fig: Moving_Linear_Fit_Error} illustrates the resulting error deviation as a function of the time index $j$, which remains close to $1$ except within the range $j = 10 \dots 19$ where the actual noise is ten times greater than the specified value.
This observation suggests that an error deviation exceeding 1 may indicate the presence of unspecified additional input errors beyond rounding errors, such as numerical errors in mathematical library functions.


\fi


\section{FFT (Fast Fourier Transformation)}
\label{sec: FFT}

\subsection{DFT (Discrete Fourier Transformation)}

\iffalse

\begin{align*}
h[k] &= \sin(f \frac{2 \pi}{N} k); \\
H[n] &= \sum_{k=0}^{N-1} \sin(f \frac{2 \pi}{N} k) \; e^{\frac{i 2\pi}{N} k n}
		= \sum_{k=0}^{N-1} \frac{1}{2i}\left( e^{(n + f) \frac{i 2\pi}{N} k} - e^{(n - f) \frac{i 2\pi}{N} k} \right)   \\
	&= \frac{1}{2i} \left( \frac{1 - e^{(n + f) \frac{i 2\pi}{N} N}}{1 - e^{(n + f) \frac{i 2\pi}{N}}}
		- \frac{1 - e^{(n - f) \frac{i 2\pi}{N} N}}{1 - e^{(n - f) \frac{i 2\pi}{N}}} \right) \\
&= \begin{cases}
  i N/2, & f \text{ is integer} \\
  N/\pi, & f \text{ is integer} + 1/2 \\
  \frac{1}{2} \frac{\sin(2\pi f - 2\pi \frac{f}{N}) + \sin(2\pi \frac{f}{N})-\sin(2\pi f) e^{-i 2\pi \frac{n}{N}}}{\cos(2\pi \frac{n}{N})-\cos(2\pi \frac{f}{N})} & \text{otherwise}
\end{cases}
\end{align*}

\fi

\begin{align}
\label{eqn: Fourier forward}
H[n] &=\sum_{k=0}^{N-1} h[k] \; e^{\frac{i 2\pi}{N} k n}; \\
\label{eqn: Fourier reverse}
h[k] &=\frac{1}{N} \sum_{n=0}^{N-1} H[n] \; e^{-\frac{i 2\pi}{N} n k};
\end{align}

For each signal sequence $h[k]$, where $k = 0, 1 \dots  N-1$, and $N$ is a natural number, the discrete Fourier transform (DFT) $H[n]$, for $n = 0, 1 \dots  N-1$, along with its inverse transformation, is defined by Formulas \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse}, respectively \cite{Numerical_Recipes}.
As a convention, $k$ denotes the \emph{time index} for $h[k]$ as a waveform, whereas $n$ represents the \emph{frequency index} for $H[n]$ as a spectrum.

\ifdefined\Verbose
\begin{figure}
\includegraphics[height=2.5in]{FFT_Unfaithful.pdf} 
\captionof{figure}{
The DFT spectrum $H[n]$ of signal $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, as intensity (y-axis) and phase (embedded y-axis) versus frequency index $n \in [0, 18]$ (x-axis and embedded x-axis) for different signal frequency $f$ (legend).
This result agrees with both theoretical formula \cite{Prev_Precision_Arithmetic} and numerical computation from any mathematical libraries such as \textit{SciPy}.
}
\label{fig: FFT_Unfaithful}
\end{figure}
For example, the FT spectrum of a sine function is a Delta function at the signal frequency $f$ with a phase $\pi/2$ \cite{Numerical_Recipes}.
Figure \ref{fig: FFT_Unfaithful} shows the DFT spectra of the sine function $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, where $f$ is its signal frequency. 
If DFT is regarded as the digital implementation of FT, the spectra exhibit no modeling error only when the input signal frequency $f$ is an integer, and display varying degrees of modeling errors otherwise.
Because of these modeling errors, the use of DFT as the digital implementation of FT is questionable, even though such usage is ubiquitous, and fundamental to many areas of applied mathematics 

To avoid the modeling errors inherent in DFT, only Formulas \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} are used in this study.

\else

Although mathematically self-consistent, DFT implies a periodic boundary condition in the time domain \cite{Prev_Precision_Arithmetic}.
Consequently, it is only an approximation for the mathematically defined continuous Fourier transform (FT) \cite{Prev_Precision_Arithmetic}.
To avoid the modeling errors inherent in DFT, only Formulas \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} are used in this study.

\fi



\subsection{FFT (Fast Fourier Transformation)}

\iffalse

Forward:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 + i 1 - 2 - i 3 , 0 - 1 + 2 - 3, 0 - i 1 - 2 + i 3; \\
 & [0, 1, 0, -1]:& [0, i 2, 0, - i 2]; \\
 & [1, 0, -1, 0]:& [0, 2, 0, 2];
\end{align*}

Reverse:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 - i 1 - 2 + i 3 , 0 - 1 + 2 - 3, 0 + i 1 - 2 - i 3; \\
 & [0, i 2, 0, - i 2]:& [0, 4, 0, -4]; \\
 & [0, 2, 0, 2]:& [4, 0, -4, 0];
\end{align*}

\fi


When $N = 2^{L}$, where $L$ is a natural number called the \emph{FFT order}, the generalized Danielson-Lanczos lemma can be applied to DFT to produce  FFT \cite{Numerical_Recipes}. 
\begin{itemize}

\item For each output, each input is used only once, therefore no dependency problem arises when decomposing FFT into arithmetic operations such as Formulas \eqref{eqn: addition mean}, \eqref{eqn: addition variance}, \eqref{eqn: multiplication mean}, and \eqref{eqn: multiplication variance}.

\item When $L$ is large, the substantial volume of input and output data enables high-quality statistical analysis.

\item The computational complexity is proportional to $L$, because increasing $L$ by 1 adds an additional step involving a sum of two multiplications.

\item Each step in the forward transformation doubles the variance; hence the uncertainty deviation increases with the FFT order $L$ as $\sqrt{2}^L$.
Because the reverse transformation divides the result by $2^L$, its uncertainty deviation decreases with $L$ as $\sqrt{1/2}^L$.
Consequently, the uncertainty deviation for the round-trip transformation is therefore $\sqrt{2}^L \times \sqrt{1/2}^L = 1$.

\item The forward and reverse transformations are identical except for a sign difference, implying that they are essentially the same algorithm, and any observed difference arises solely from the input data.  

\end{itemize}

 

\subsection{Testing Signals}

\iffalse

The Fourier transformation of a linear signal $h[n] = n$. 
Let $y \equiv i 2\pi n /N$:
\begin{align*}
& G(y) = \sum_{k=0}^{N-1}  e^{y k} = \sum_{k=0}^{N-1}  (e^y)^k = \frac{e^{N y} - 1}{e^y - 1}
 = \begin{cases} y = 0: \eqspace N \\ y \neq 0: \eqspace 0 \end{cases}; \\
H[n] &= \sum_{k=0}^{N-1} k e^{\frac{i 2\pi n}{N} k} = \sum_{k=0}^{N-1} k e^{y k} 
 = \frac{d G}{y} = \frac{N e^{N y}}{e^y - 1} - \frac{e^{N y} - 1}{(e^y - 1)^2} e^y = \frac{N}{e^y - 1} \\
 &= \frac{N}{\cos(y) - 1 + i \sin(y)} = \frac{N}{2} \frac{\cos(y) - 1 -  i \sin(y)}{1 - \cos(y)} 
  = - \frac{N}{2}(1 + i \frac{2 \sin(\frac{y}{2}) \cos(\frac{y}{2})}{2 \sin^2(\frac{y}{2})}) \\
 &= \begin{cases} y = 0: \eqspace \frac{N^2}{2} \\ y \neq 0: \eqspace - \frac{N}{2}(1 + i \frac{1}{\tan(\frac{n}{N} \pi)}) \end{cases};
\end{align*}

\fi


The following signals are used for testing:
\begin{itemize}
\item \emph{Sin}: $h[k] = \sin(2\pi k f/N), f = 1, 2, ... \frac{N}{2} -1$.

\item \emph{Cos}: $h[k] = \cos(2\pi k f/N), f = 1, 2, ... \frac{N}{2} -1$.

\item \emph{Linear}: $h[k] = k$, whose DFT is given by Formula \eqref{eqn: Fourier spec for linear}.
\begin{align}
& y \equiv i 2\pi \frac{n}{N}: \eqspace G(y) = \sum_{k=0}^{N-1}  e^{y k} = \frac{e^{N y} - 1}{e^y - 1}; \nonumber \\
\label{eqn: Fourier spec for linear}
H[n] &= \frac{d G}{d y} = \begin{cases} n = 0: \eqspace \frac{N (N-1)}{2} \\ n \neq 0: \eqspace
 - \frac{N}{2}(1 + i \frac{\cos(n \frac{\pi}{N})}{\sin(n \frac{\pi}{N})}) \end{cases};
\end{align}

\end{itemize}
For Sin or Cos signals, forward and reverse DFT transforms differ in their data prospective of time domain versus frequency domain:
\begin{itemize}
\item The forward transformation converts a time-domain sine or cosine signal into a frequency-domain spectrum in which most values are zero, causing its uncertainties to grow more rapidly during cancellation. 

\item In contrast, the reverse transformation spreads the precise frequency-domain spectrum (where most values are zero) back into a time-domain sine or cosine signal, causing its uncertainties to grow more slowly. 
\end{itemize}
The question is whether variance arithmetic can work properly in these two contrary cases.



\subsection{Trigonometric Library Errors}

\begin{figure}[p]
\includegraphics[height=2.5in]{Sin_Diff.pdf} 
\captionof{figure}{
Difference between Library and Quart $\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
The uncertainty of the Quart $\sin(x)$ is $\sin(x)$ ULP, which shows a periodicity of $\pi$.
}
\label{fig: Sin_Diff}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{Cot_Diff.pdf} 
\captionof{figure}{
Difference between the Library and the Quart $\cos(x)/\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
}
\label{fig: Cot_Diff}
\end{figure}


Formulas \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} restrict the use of $\sin(x)$ and $\cos(x)$ to $x = 2\pi j /2^L$, where $L$ is the FFT order.
To minimize numerical errors in computing $\sin(x)$, the following \emph{indexed sine} can be used in place of  standard library sine functions:
\begin{enumerate}
\item Instead of a floating-point value $x$ as input for $\sin(x)$, an integer index $j$ defines the input as $\sin(\pi j/2^L)$, thereby eliminating the floating-point rounding error of $x$.

\item The values of $\sin(\pi j/2^L), j \in [0, 2^{L-2}]$ are the library sine directly, whereas the values of $\sin(\pi j/2^L), j \in [2^{L-2}, 2^{L-1}]$ are computed from the library $\cos(\pi (2^{L - 1} - j)/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended from $j \in [0, 2^{L-1}]$ to $j \in [0, 2^{L + 1}]$ by exploiting the symmetry of $\sin(\pi j/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended to all the integer value of $j$ by leveraging the periodicity of $\sin(2\pi j/2^L)$.

\end{enumerate}
The constructed indexed $\sin(x)$ is referred to as the \emph{Quart} sine function.
In contrast, the direct use of the standard library $\sin(x)$ is referred to as the \emph{Library} sine function.

Because the Quart sine function strictly preserves the symmetry and periodicity of the sine function, it provides better numerical accuracy than the Library sine function.
\begin{itemize}
\item Figure \ref{fig: Sin_Diff} shows that the absolute value difference between the Library $\sin(x)$ and the Quart $\sin(x)$ increases approximately linearly with $|x|$.

\item Figure \ref{fig: Cot_Diff} shows the value difference between the Quart and Library $\cos(x)/\sin(x)$ also increases roughly linearly with $|x|$, but is $10^2$ times larger than that observed for $\sin(x)$.
Therefore, the linear spectrum in Formula \eqref{eqn: Fourier spec for linear} contains significantly larger numerical errors when computed using the Library sine functions.
\end{itemize}


\subsection{Using Quart Sine for Sin/Cos Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Error distributions of normalized errors of Sin/Cos signals for forward, reverse and round-trip transformations (legend) using the Quart sine function.
The FFT order is $18$.  
}
\label{fig: FFT_SinCos_Clean_Histo_Indexed}
\end{figure}


With the FFT order as the specific dimension, the error deviations obtained using the Quart sine function for forward and reverse transformations resemble those shown in Figure \ref{fig: Adjugate_Error_vs_Size_Noise}, whereas those for round-trip transformation are nearly identical to those shown in Figure \ref{fig: Forward_Error_vs_Size_Noise}, independent of the signal frequency or whether a Sin or Cos signals is used.
Therefore, the results for Sin and Cos signals across all frequencies are pooled together for statistical analysis, under the unified category \emph{Sin/Cos} signals.
When the FFT order $L$ is less than 8, the error deviations oscillate around $1$ because of an insufficient sample count of $2^L$.
Even data for forward and reverse transformations are drastically different, variance arithmetic works effectively in all cases.

When $L=18$ and $\delta x = 0$, Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed} shows that the error distributions of Sin/Cos signal resemble Normal distributions, with an additional Delta-like distribution at $\tilde{z} = 0$ for the forward transformation.
The error distribution of the reverse transformation is structured on top of the Normal distribution, suggesting that the reverse transformation is more sensitive to numerical errors in the sine function.


\subsection{Using Library Sine for Sin/Cos Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Error distributions of normalized errors of Sin/Cos signal for forward, reverse and round-trip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_SinCos_Clean_Histo_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Spec_Lib.pdf} 
\captionof{figure}{
FFT value error spectrum of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the forward transformation.
The legend distinguishes between uncertainty and value error.
The x-axis represents the frequency index, and the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Spec_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Wave_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the reverse transformation.
The legend distinguishes between uncertainty and value error.
The x-axis represents the time index, and the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Wave_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Roundtrip_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the round-trip transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, and the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Lib_Reverse_Error_vs_Freq_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) of the FFT reverse transformation of $sin(f \frac{2 \pi}{2^L} j)$ versus frequency $f$ (x-axis) and FFT order $L$ (y-axis).
}
\label{fig: Lib_Reverse_Error_vs_Freq_Order}
\end{figure}


With FFT order as the specific dimension, the error deviations obtained using the Library sine for the forward and reverse transformations resemble those shown in Figure \ref{fig: Adjugate_Error_vs_Size_Noise}, whereas those for the round-trip transformation are nearly identical to those shown in Figure \ref{fig: Forward_Error_vs_Size_Noise}.
In addition, the error deviations are larger than $1$ when $\delta x < 10^{-15}$ for the forward transformation and $\delta x < 10^{-14}$ for the reverse transformation.

When $\delta x = 0$, the error deviations for the reverse transformation increase with the FFT order, to $6.2$ at FFT order $18$.
As shown in Figure \ref{fig: Sin_Diff}, the Library sine function contains more numerical errors, as a result, the error distribution for the reverse transformation obtained using the Library sine function in Figure \ref{fig: FFT_SinCos_Clean_Histo_Lib} is more structured and broader than that shown in Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed} obtained using the Quart sine function, whereas the error distributions for the forward transformations are more similar.
This difference is consistent with a larger error deviation $6.2 > 1$ for the reverse transformation, compared with the comparable error deviation $1.1 > 1$ for the forward transformation when $\delta x = 0$.

When $\delta x = 0$, using the Library sine function, for a sine wave with a frequency of $3$, Figures \ref{fig: FFT_Sin_Clean_6_3_Spec_Lib}, \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}, and \ref{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib} present the value errors for the forward, reverse, and round-trip transformations, respectively. 
In the reverse transformation, the value errors exhibit a clear trend of increasing with the time index. 
These large value errors appear systematic rather than random and visually resemble a resonant pattern.
Similar increases are observed at other frequencies and FFT orders, as well as in computational results obtained using mathematical libraries such as \textit{SciPy}.
In contrast, such resonance is absent from the round-trip transformation shown in Figure \ref{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}, as well as when using the Quart sine function.
Figure \ref{fig: Lib_Reverse_Error_vs_Freq_Order} demonstrates that the error deviations increase with sine or cosine frequency, regardless of FFT order $L$ when $L > 8$.
Figure \ref{fig: Sin_Diff} indicates that the numerical errors of $cos(x)/sin(x)$ obtained using the Library sine function increase with a periodicity of $\pi$, which can resonate with a signal whose periodicity is an integer multiply of $\pi$, producing the resonant pattern shown in Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}.
At higher frequency, the resonant beats between the signal and the numerical errors in the Library sine function become stronger.
To suppress this numerical error resonance, an input noise of $\delta x = 10^{-14}$ must be added to the sine or cosine signals.
Such \emph{resonance of numerical errors} can easily and mistakenly be taken as signals.



\subsection{Using Quart Sine for Linear Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Error distributions of normalized errors of Linear signals for forward, reverse and round-trip transformations (legend) computed using the Quart sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Indexed}
\end{figure}

Figures \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order} show the error deviations for the forward and the reverse transformations, respectively. 
The forward transformation exhibits a larger ideal coverage area than the reverse transformations: $\delta x > 10^{-12}$ for the forward transformation, and $\delta x > 10^{-8}$ for the reverse transformation.
In other areas, both transformations achieve proper coverage with error values around $1$.

When $L = 18$ and $\delta x = 0$, the error distribution of the reverse transformation in Figure \ref{fig: FFT_Linear_Clean_Histo_Indexed} is narrower than that shown in Figure \ref{fig: FFT_SinCos_Clean_Histo_Lib}.
The corresponding error deviations are $1.5 < 6.2$, respectively.

The error deviations for the round-trip transformation resemble those in Figure \ref{fig: Forward_Error_vs_Size_Noise} but with the FFT order as the specific dimension.


\subsection{Using Library Sine for Linear Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Error distributions of normalized errors of Linear signals for forward, reverse and round-trip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Roundtrip_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the round-trip transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order}
\end{figure}


Figure \ref{fig: FFT_Linear_Clean_Histo_Lib} shows that the reverse error distribution when $\delta x = 0$ is no longer bound.
The difference between Figures \ref{fig: FFT_Linear_Clean_Histo_Lib} and \ref{fig: FFT_Linear_Clean_Histo_Indexed} is consistent with the large numerical errors as demonstrated in Figure \ref{fig: Cot_Diff}.
Variance arithmetic fails because of the large amount of unspecified numerical errors from the Library sine function.

Figures \ref{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order} show much smaller ideal coverage areas than those shown in Figures \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}.
Because uncertainty deviations grow more slowly in the reverse transformation than in the forward transformation, the former exhibits a smaller ideal coverage region.
Outside of the ideal coverage region, proper coverage cannot be achieved for the reverse transformation.
Furthermore, the range of input noise that produces ideal coverage decreases with increasing FFT order.
At sufficiently high FFT orders (visually beyond FFT order $25$ for the reverse transformation), ideal coverage may no longer be achievable.
Although FFT is widely regarded as one of the most robust numerical algorithms \cite{Numerical_Recipes}\cite{Precise_Numerical_Methods}, and is generally insensitive to input errors, it can still fail because heavy calculation can amplify numerical errors in the Library sine function to completely obscure true signals.
Such deterioration in calculation is not easily detectable when using conventional floating-point arithmetic.

Figure \ref{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order} shows that, even when variance arithmetic can no longer effectively track the value errors for either the forward or the reverse transformations, it can still effectively track the value errors for the round-trip transformation, as shown by the plateau region at high $L$ and low $\delta x$.
Such error cancellation arises from dependence tracing in statistical Taylor expansion.



\subsection{Ideal Coverage}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_1e-3_vs_Order_Lib.pdf} 
\captionof{figure}{
Error deviation (left y-axis) and uncertainty deviation (right y-axis) of Linear signals versus FFT order (x-axis) and transformation types (legend) computed using the Library sine function.
}
\label{fig: FFT_Linear_1e-3_vs_Order_Lib}
\end{figure}

Adding noise to the input can suppress unspecified input errors.
After adding a Gaussian input noise of $\delta x = 10^{-3}$ to a Linear signal when using the Library sine function, the error distributions for both the forward and the reverse transformations become Normal, whereas the error distribution for the round-trip transformation becomes Delta.
Figure \ref{fig:  FFT_Linear_1e-3_vs_Order_Lib} illustrates the corresponding error deviations and uncertainty deviations versus FFT order:
\begin{itemize}
\item As expected, the resulting uncertainty deviations for the forward transformations increase with FFT order $L$ as $\sqrt{2}^L$.

\item As expected, the resulting uncertainty deviations for the reverse transformations decrease with FFT order $L$ as $1/\sqrt{2}^L$.

\item As expected, the resulting uncertainty deviations for the round-trip transformations remain equal to the corresponding input uncertainties of $10^{-3}$.

\item As expected, the resulting error deviations for the forward and the reverse transformations remain constant at $1$.

\item As expected, the resulting error deviations for the round-trip transformations are far less than $1$ but increase exponentially with FFT order $L$ because of increasing calculation.
\end{itemize}

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline 
Signal     & Sine function & Forward       & Reverse       & Roundtrip    \\ 
\hline 
Sin/Cos   & Quart           & $10^{-16}$  & $10^{-12}$ & $10^{-14}$ \\ 
\hline 
Sin/Cos   & Library         & $10^{-16}$  & $10^{-11}$ & $10^{-12}$ \\ 
\hline 
Linear     & Quart           & $10^{-11}$  & $10^{-7}$   & $10^{-8}$ \\
\hline 
Linear     & Library         & $10^{-11}$  & $10^{-3}$   & $10^{-8}$  \\
\hline 
\end{tabular}
\captionof{table}{
The measured minimal required noise to achieve ideal coverage for FFT transformations at FFT order $18$ for different signals and sine functions.
}
\label{tbl: ideal coverage}
\end{table}

Table \ref{tbl: ideal coverage} shows the minimal required noise to achieve ideal coverage for FFT transformations at FFT order $L=18$ for different signals and sine functions, which is consistent with the corresponding error distributions shown in Figures \ref{fig: FFT_SinCos_Clean_Histo_Indexed}, \ref{fig: FFT_SinCos_Clean_Histo_Lib}, \ref{fig: FFT_Linear_Clean_Histo_Indexed}, and \ref{fig: FFT_Linear_Clean_Histo_Lib}.
An error distribution can reflect its input uncertainty coverage.
Without knowing the precise result, similar histogram can be constructed from the result data set for $f$ using the calculated mean $\overline{f}$ and deviation $\delta f$.
It is worth investigating if such an empirical histogram has similar power to reveal input uncertainty coverage.



\section{Regressive Generation of Sine}
\label{sec: recursion}

\begin{figure}
\centering
\includegraphics[height=2.5in]{Regression_Sin.pdf}
\captionof{figure}{
The resulting precision (left y-axis) and uncertainties (right y-axis) of $\sin(\pi j/2^{18})$ versus index $j$ (x-axis) using either Quart or regressive sine function (legend).
}
\label{fig: Regression_Sin}
\end{figure}


\begin{align}
\label{eqn: phase boundary}
& \sin(0) = \cos(\frac{\pi}{2}) = 0;\eqspace \sin(\frac{\pi}{2}) = \cos(0) = 1; \\
\label{eqn: phase sin}
& \sin(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 - \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 - \cos(\alpha) \cos(\beta) + \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: phase cos}
& \cos(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 + \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 + \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: regr error}
& \sin(\alpha + \beta) = 2 \sin(\frac{\alpha + \beta}{2}) \cos(\frac{\alpha + \beta}{2}) = \sqrt{1 - \cos(\frac{\alpha + \beta}{2})^2};
\end{align}
Formulas \eqref{eqn: phase sin} and \eqref{eqn: phase cos} calculate $\sin(\pi j/2^L), \cos(\pi j/2^L), j = 0 \dots 2^{L - 2}$ repressively for regression order $L = 0 \dots 17$ starting from Formula \eqref{eqn: phase boundary}.
Formula \eqref{eqn: regr error} shows that such regression guarantees both $\sin(x)^2 + \cos(x)^2 = 1$ and $\sin(2x) = 2\sin(x) \cos(x)$, such that value errors will not accumulate when the regression order $L$ increases.


Formula \eqref{eqn: phase sin} is not suitable for computing $\sin(x)$ as $x \rightarrow 0$ because it suffers from behavior analogous to catastrophic cancellation \cite{Rounding_Error}\cite{Precise_Numerical_Methods}.
As shown in Figure \ref{fig: Regression_Sin}, the Quart sine function exhibits a constant precision around $10^{-16}$, whereas the regression sine function shows increasing precision up to $10^{-7}$ as $x \rightarrow 0$.
Unlike the sneaky catastrophic cancellation in floating-point arithmetic, variance arithmetic uses coarser precision to demonstrate where and by how much the regression algorithm becomes unfit to compute $\sin(x)$.




\section{Conclusion and Discussion}
\label{sec: conclusion and discussion}

\subsection{Summary}

When the uncorrelated uncertainty condition is satisfied, statistical Taylor expansion produces the mean, deviation, and reliability of an analytic expression.
It tracks the variable dependencies in intermediate steps and rejects invalid calculations.
Unlike conventional approaches, it explicitly incorporates the sample counts and uncertainty distributions into its result.
Although statistical Taylor expansion eliminates the dependency problem, it also reduces execution flexibility.

The presence of ideal coverage is a necessary condition for a numerical algorithm based on statistical Taylor expansion to be considered correct. 
Ideal coverage defines the optimal range of applicability for an algorithm. 

Variance arithmetic simplifies statistical Taylor expansion by introducing numerical rules that eliminate invalid results, such as divergent, negative variance, unstable, infinite, or unreliable results.
It also provides proper coverage for floating-point rounding errors.
The applicability of variance arithmetic has been demonstrated across a wide range of computational scenarios.

The code and analysis framework for variance arithmetic are available as an open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}.
A more detailed description of this study is presented at  \url{https://arxiv.org/abs/2410.01223}.



\subsection{Improvements Needed}

This study presents statistical Taylor expansion and variance arithmetic, which are still in early stages of development.
Accordingly, several important questions remain.

Library mathematical functions should be recalculated using variance arithmetic, to ensure that each output value is accompanied by its corresponding uncertainty.
Without this refinement, the value errors in the library functions can produce unpredictable and potentially significant result errors.

Bound momentum $\zeta(n, \kappa)$ should be extended to all probability distributions.
The choice of ideal bounding range $\hat{\kappa}$ should be extended to other distributions, perhaps by using the same value of linear leakage $\varepsilon \equiv 1 - \zeta(2, \kappa)$.
The procedure to find the bound moments $\kappa$ for each sample count $N$ should be developed for discrete distributions.
\ifdefined\Verbose
The statistical meaning for ideal leakage $\varepsilon \in [0, 1]$ in the context of its choice of ideal bounding range $\hat{\kappa}$ needs further investigation: Is $\varepsilon$ the possibility for the result to be not bound by $\hat{\kappa}$?
\fi

The performance of variance arithmetic must be improved for broader practical adoption.
The fundamental formulas of statistical Taylor expansion, Formulas \eqref{eqn: Taylor 1d mean}, \eqref{eqn: Taylor 1d variance}, \eqref{eqn: Taylor 2d mean}, and \eqref{eqn: Taylor 2d variance}, contain many independent summations, making them excellent candidates for parallel processing.
Moreover, the inherently procedural nature of these formulas allows statistical Taylor expansion to be implemented efficiently at the hardware level.

A key open question is whether variance arithmetic can be adapted to achieve ideal coverage for floating-point rounding errors, because many theoretical calculations lack explicit input uncertainties.
Variance arithmetic does not adjust uncertainty characterization when floating-point rounding error occurs during calculation, which leads to error deviations larger than 1.
Detecting floating-point rounding error and adjusting uncertainty characterization in real time needs hardware implementation for efficiency.

In variance arithmetic, deviations are comparable to values; however variances are used in calculation.
This approach effectively limits the range of deviations to the square root of that of the values.
If the sign bit of the floating type can be re-purposed as an exponent bit in a new unsigned floating-point representation, the range of the deviations will be identical to that of the values.

When an analytic expression undergoes statistical Taylor expansion, the resulting expression can become highly complex, as in the case of matrix inversion.
Modern symbolic computation tools such as \textit{SymPy} and \textit{Mathematica} can significantly facilitate such calculations.
This observation suggests that it may be time to shift from purely numerical programming toward analytic programming, particularly for problems that possess  inherently analytic formulations.

As an enhancement to dependency tracing, source tracing identifies each inputs contribution to the overall result uncertainty.
This capability enables engineers to pinpoint the primary sources of measurement inaccuracy and guides them to targeted improvements in data acquisition and processing strategies.
For example, Formula \eqref{eqn: sum leakage} can provide guidance on improving the ideal leakage of $x \pm y$.

Because conventional numerical approaches are based on floating-point arithmetic and path dependent in general, they must be reexamined or even reinvented within the framework of variance arithmetic.
For instance, most conventional numerical algorithms aim to identify optimal computational paths, whereas statistical Taylor expansion conceptually rejects all path-dependent calculations.
Reconciling these two paradigms may present a significant and ongoing challenge.

Establishing theoretical foundation for applying statistical Taylor expansion in the absence of a closed-form analytic solution, or when only limited low-order numerical derivatives are available, as in solving differential equations, remains an important  direction for future research.


\ifdefined\Verbose
\subsection{Possible Connections to Quantum Physics}

For input distribution without bounding, Formula \eqref{eqn: Taylor 1d variance} may not converge at all, for example, when the input uncertainty is Laplace \cite{Probability_Statistics}, $\zeta(2n, \infty) = (2n)!$.
Formula \eqref{eqn: Taylor 1d variance} converges in all cases only when the input uncertainty is bounded because of sampling, which resembles the need for re-normalization in quantum field theory \cite{Quantum Field}.
Furthermore, the quantitative dependence of convergence on the choice of ideal bounding range $\hat{\kappa}$ is similar to the principle of quantum physics: What it is depends on how it is measured.
For example, it is possible that an experimental measurement reduces $\hat{\kappa}$ for a wave function, to cause the wave function to collapse.
Both resemblances are worthy of further investigation.
\fi


\section{Statements and Declarations}

\subsection{Acknowledgments}

As an independent researcher without institutional affiliation, the author expresses sincere gratitude to Dr. Zhong Zhong (Brookhaven National Laboratory) and Prof Weigang Qiu (Hunter College) for their encouragement and valuable discussions.
Special thanks are extended to the organizers of \emph{AMCS 2005}, particularly Prof. Hamid R. Arabnia (University of Georgia), and to the organizers of the \emph{NKS Mathematica Forum 2007}. 
The author also gratefully acknowledges Prof Dongfeng Wu (Louisville University) for her insightful guidance on statistical topics.
Finally, heartfelt appreciation is extended to the editors and reviewers of \emph{Reliable Computing} for their substantial assistance in shaping and accepting an earlier version of this work, with special recognition to Managing Editor Prof. Rolph Baker Kearfott.


\subsection{Data  Availability Statement}

The data set used in this study are all generated in the open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}. 
The execution assistance and explanation of the above code are available from the author upon request.


\subsection{Competing Interests}

The author has no competing interests to declare that are relevant to the content of this article.

\subsection{Founding}

No funding was received from any organization or agency in support of this research.


\ifdefined\ManualReference

\begin{thebibliography}{10}

\bibitem{Statistical_Methods}
Sylvain Ehrenfeld and Sebastian~B. Littauer.
\newblock {\em Introduction to Statistical Methods}.
\newblock McGraw-Hill, 1965.

\bibitem{Precisions_Physical_Measurements}
John~R. Taylor.
\newblock {\em Introduction to Error Analysis: The Study of Output Precisions
  in Physical Measurements}.
\newblock University Science Books, 1997.

\bibitem{Lower_Order_Variance_Expansion}
Fredrik Gustafsson and Gustaf Hendeby.
\newblock Some relations between extended and unscented kalman filters.
\newblock {\em IEEE Transactions on Signal Processing}, 60-2:545--555, 2012.

\bibitem{Probability_Statistics}
Michael~J. Evans and Jeffrey~S. Rosenthal.
\newblock {\em Probability and Statistics: The Science of Uncertainty}.
\newblock W. H. Freeman, 2003.

\bibitem{Numerical_Recipes}
William~H. Press, Saul~A Teukolsky, William~T. Vetterling, and Brian~P.
  Flannery.
\newblock {\em Numerical Recipes in C}.
\newblock Cambridge University Press, 1992.

\bibitem{Computer_Architecture}
John~P Hayes.
\newblock {\em Computer Architecture}.
\newblock McGraw-Hill, 1988.

\bibitem{Floating_Point_Arithmetic}
David Goldberg.
\newblock What every computer scientist should know about floating-point
  arithmetic.
\newblock {\em ACM Computing Surveys}, March 1991.

\bibitem{Floating_Point_Standard}
Institute of Electrical and Electronics Engineers.
\newblock {\em ANSI/IEEE 754-2008 Standard for Binary Floating-Point
  Arithmetic}, 2008.

\bibitem{Rounding_Error}
J.~H. Wilkinson.
\newblock {\em Rounding Errors in Algebraic Processes}.
\newblock SIAM, 1961.

\bibitem{Precise_Numerical_Methods}
Oliver Aberth.
\newblock {\em Precise Numerical Methods Using C++}.
\newblock Academic Press, 1998.

\bibitem{Algorithms_Accuracy}
Nicholas~J. Higham.
\newblock {\em Accuracy and Stability of Numerical Algorithms}.
\newblock SIAM, 2002.

\bibitem{Interval_Analysis}
R.E. Moore.
\newblock {\em Interval Analysis}.
\newblock Prentice Hall, 1966.

\bibitem{Worst_Case_Error_Bounds}
W.~Kramer.
\newblock A prior worst case error bounds for floating-point computations.
\newblock {\em IEEE Trans. Computers}, 47:750--756, 1998.

\bibitem{Interval_Analysis_Theory_Applications}
G.~Alefeld and G.~Mayer.
\newblock Interval analysis: Theory and applications.
\newblock {\em Journal of Computational and Applied Mathematics}, 121:421--464,
  2000.

\bibitem{Interval_Arithmetic}
W.~Kramer.
\newblock Generalized intervals and the dependency problem.
\newblock {\em Proceedings in Applied Mathematics and Mechanics}, 6:685--686,
  2006.

\bibitem{Interval_Analysis_Notations}
A.~Neumaier S.M. Rump S.P.~Shary B.~Kearfott, M. T.~Nakao and P.~Van
  Hentenryck.
\newblock Standardized notation in interval analysis.
\newblock {\em Computational Technologies}, 15:7--13, 2010.

\bibitem{Prev_Precision_Arithmetic}
C.~P. Wang.
\newblock A new uncertainty-bearing floating-point arithmetic.
\newblock {\em Reliable Computing}, 16:308--361, 2012.

\bibitem{Stochastic_Arithmetic}
J.~Vignes.
\newblock A stochastic arithmetic for reliable scientific computation.
\newblock {\em Mathematics and Computers in Simulation}, 35:233--261, 1993.

\bibitem{CADNA_library}
C.~Denis N.~S.~Scott, F.~Jezequel and J.~M. Chesneaux.
\newblock Numerical 'health' check for scientific codes: the cadna approach.
\newblock {\em Computer Physics Communications}, 176(8):501--527, 2007.

\bibitem{Linear_Algebra}
J.~Hefferon.
\newblock Linear algebra.
\newblock \url{http://joshua.smcvt.edu/linearalgebra/}, 2011.

\bibitem{Quantum Field}
Michel Le Bellac, G Barton.
\newblock Quantum and statistical Field Theory.
\newblock Oxford University Press, 1992, ISBN 9781383026535.


\end{thebibliography}

\else
\bibliographystyle{unsrt}
\bibliography{VarianceArithmetic}
\fi






\end{document}
