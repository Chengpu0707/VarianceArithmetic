\documentclass[twoside]{article}
%Latex for arxiv needs figures in pdf format.  To convert png to pdf, go to https://pngpdf.com/#google_vignette

\pagestyle{myheadings}
\setlength{\oddsidemargin}{44pt}
\setlength{\evensidemargin}{44pt}
\setcounter{page}{1}

\usepackage{url,intmacros,graphicx}
\usepackage{amssymb,amsmath}
\usepackage{capt-of}
\usepackage{float}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\numberwithin{equation}{section}

\newcommand{\email}[1]{\\ \small{\url{#1}} \\}
\newcommand{\institution}[1]{\\ \parbox{3.0in}{\small{#1}}}
\newcommand{\keywords}[1]{\small\textbf{Keywords: }#1}
\newcommand{\AMSsubj}[1]{\noindent\textbf{AMS subject classifications: }#1}
\newcommand\whenaccepted{}

\newcommand{\eqspace}{\;\;\;}
\newcommand{\largespace}{\;\;\;\;\;\;\;\;\;\;\;\;}
%\newcommand{\Verbose}{}
\newcommand{\ManualReference}{}



%-------------------------------------------------------


%  You may add additional packages here.  However, if they
%  are not available with the usual LaTeX distribution,
%  they must be supplied with the final, accepted LaTeX.

% Fill in your title here. (Retain the footnote.)
\title{Statistical Taylor Expansion: A New and Path-Independent Method for Uncertainty Analysis \footnote{\whenaccepted}}

% Delete the "\and" or add more as needed
\author{Chengpu Wang
\institution{40 Grossman Street, Melville, NY 11747, USA}
\email{Chengpu@gmail.com}}

% Put a short running title within the first argument to
% this command.  Do not alter the second argument
\markboth{CP Wang, \textit{Statistical Taylor Expansion}}
         {\textit{https://github.com/Chengpu0707/VarianceArithmetic}}




\begin{document}
\maketitle
\begin{abstract}

As a rigorous statistical approach, statistical Taylor expansion extends the conventional Taylor expansion by replacing precise input variables with random variables of known distributions and sample counts to compute the mean, the standard deviation, and the reliable factor of each result.
It tracks the propagation of the input uncertainties through intermediate steps, so that the final analytic result becomes path independent.
Therefore, it differs fundamentally from common approaches in applied mathematics that optimize computational path for each calculation.
Statistical Taylor expansion may standardize numerical computations for analytic expressions.
This study also introduces the implementation of statistical Taylor expansion termed variance arithmetic and presents corresponding test results across a wide range of mathematical applications.

Another important conclusion of this study is that numerical errors in library functions can significantly affect results. 
It is desirable that each value from library functions be accomplished by an uncertainty deviation.  
\end{abstract}

% Put keywords appropriate to your paper here, as shown
\keywords{computer arithmetic, error analysis, interval arithmetic, uncertainty, numerical algorithms.}

% Put your AMS subject classifications into the argument of
% the following command.
\AMSsubj{G.1.0}

Copyright \copyright{2024}


\section{Introduction}
\label{sec: introduction}

Let $x$ and  $\delta x$ be a value and its uncertainty deviation, respectively.
As an input, $x$ and $\delta x$ are the mean and standard deviation of a measurement \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}.
If $\delta x = 0$,  $x$ is a \emph{precise value}, otherwise, the pair specifies an \emph{imprecise value} $x \pm \delta x$.

Let $P(x) \equiv \delta x / |x|$ be the \emph{statistical precision} (hereafter referred to as precision) of $x \pm \delta x$.
A smaller $P(x)$ indicate a higher measurement quality of $x \pm \delta x$, and when $P(x) \ll 1$, it is unclear whether $\delta x$ is too poor to measure $x$, or it is a measurement of $x = 0$.

Statistical Taylor expansion determines the result $f \pm \delta f$ of a general analytic expression $f(x, \dots)$, and the reliability $\alpha \in [0, 1]$ of $f \pm \delta f$ based on the inputs $x \pm \delta x, \dots$ and their corresponding sample counts. 
\begin{itemize}
\item 
Previous studies have examined the effect of input uncertainties on output values for specific cases \cite{Lower_Order_Variance_Expansion}.
Statistical Taylor expansion generalizes these effects as uncertainty bias, as shown in Formula \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 2d mean} in this paper.

\item
The traditional variance-covariance framework accounts only for linear interactions between random variables through an analytic function \cite{Lower_Order_Variance_Expansion}\cite{Probability_Statistics}\cite{Numerical_Recipes}, whereas statistical Taylor expansion extends this framework to include higher-order interactions as expressed in Formula \eqref{eqn: Taylor 2d variance} in this paper.

\item Calculating the reliability of $f \pm \delta f$ seems completely new.
\end{itemize}

Statistical Taylor expansion is superior to all existing numerical arithmetic.

Conventional floating-point arithmetic \cite{Computer_Architecture}\cite{Floating_Point_Arithmetic}\cite{Floating_Point_Standard} can only calculate the result $f$ which contains unknown amount of rounding error \cite{Rounding_Error}\cite{Precise_Numerical_Methods}\cite{Algorithms_Accuracy}.
As a consequence, a floating-point representation with $10^{-16}$ resolution may not be good enough for inputs with $10^{-2}$ to $10^{-6}$ precision.
While statistical Taylor expansion can track rounding errors.

The bounding range in interval arithmetic \cite{Interval_Analysis}\cite{Worst_Case_Error_Bounds}\cite{Interval_Analysis_Theory_Applications}\cite{Interval_Arithmetic}\cite{Interval_Analysis_Notations} is inconsistent with the statistical nature of an $x \pm \delta x$ pair and it tends to over-estimate the result uncertainty due to its worst-case assumption \cite{Prev_Precision_Arithmetic}.
On the other hand, statistical Taylor expansion is precise statistically.

Both conventional floating-point arithmetic and interval arithmetic depend strongly on the specific algebraic form of an analytic function, a phenomenon known as the \emph{dependency problem} \cite{Precise_Numerical_Methods}\cite{Algorithms_Accuracy}\cite{Interval_Analysis}\cite{Interval_Analysis_Theory_Applications}, which makes the numerical calculations of analytic problems sometimes more like an art than science.
In contrast, statistical Taylor expansion is path independent.

To be rigorous in mathematics and statistics, statistical Taylor expansion abandons the significance arithmetic nature of its processor \cite{Prev_Precision_Arithmetic}.

As a statistical sampling process, stochastic arithmetic \cite{Stochastic_Arithmetic}\cite{CADNA_library} is computationally expensive, while statistical Taylor expansion provides a direct characterization without sampling.

In this paper:

\begin{enumerate}
\item Section \ref{sec: introduction} compares briefly statistical Taylor expansion with all other numerical arithmetic.

\item Section \ref{sec: statistical Taylor expansion} develops the theoretical foundation of statistical Taylor expansion.

\item Section \ref{sec: variance arithmetic} describes variance arithmetic as a numerical implementation of statistical Taylor expansion.

\item Section \ref{sec: validation} lays out standard to validate variance arithmetic.

\item Section \ref{sec: polynomial} illustrates variance arithmetic in computing polynomial, demonstrating its capability in tracing floating-point rounding errors, and its continuity in parameter space.

\item Section \ref{sec: matrix} applies variance arithmetic to adjugate matrix and matrix inversion, showing that the uncertainty calculation for a linear calculation is no longer linear.
It also distinguishes between a distribution test and a value test.

\item Section \ref{sec: Math Library} evaluates variance arithmetic on common mathematical library functions.
It also shows that the error deviation should be close to $1$ except near a distributional pole.

\ifdefined\Verbose
\item Section \ref{sec: Moving-Window Linear Regression} reveals the accumulation of numerical errors by reusing an input multiple time in a moving-window progressive algorithm.

\item Section \ref{sec: recursion} showcases variance arithmetic in catching catastrophic cancellation in a regression algorithm.
\fi

\item Section \ref{sec: FFT} examines the impact of numerical library errors and shows that these errors can be significant.

\item Section \ref{sec: conclusion and discussion} concludes with a summary and a discussion.

\end{enumerate}




\section{Statistical Taylor Expansion}
\label{sec: statistical Taylor expansion}

\subsection{The Uncorrelated Uncertainty Condition}

\iffalse

\begin{align*}
& \frac{1}{\gamma_{P}} - 1 = \left(\frac{1}{\gamma} -1\right) \frac{1}{P^2}; \\
& P^2 \left( \frac{1}{\gamma_{P}} - 1 \right) + 1 = \frac{1}{\gamma};
\end{align*}

\fi

When there is no systematic error \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements} among the inputs, the uncertainties of the inputs are uncorrelated to each other, even though the inputs may have significant correlations among them.
This condition can be characterized quantitatively in statistics as the \emph{uncorrelated uncertainty condition} \cite{Prev_Precision_Arithmetic}.


\subsection{Distributional Zero and Distributional Pole}

\iffalse

To solve for mode:
\begin{align*}
& \rho(\tilde{y}, y, \delta y) = \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z});
\eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x}; \\
& 0 = \frac{d \rho(\tilde{y}, y, \delta y)}{d \tilde{y}} = \frac{d^2 \tilde{z}}{d \tilde{y}^2} N(\tilde{z}) - \frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z}) \tilde{z}; \\
& \frac{d^2 \tilde{z}}{d \tilde{y}^2} = \frac{d \tilde{z}}{d \tilde{y}} \tilde{z}; \eqspace \tilde{z} = \frac{f^{-1}(\tilde{y}) - x}{\delta x};
\end{align*}

The exponential function:
\begin{align*}
f(x) = e^x:& \eqspace 
\tilde{z} = \frac{\log(\tilde{y}) - x}{\delta x}, \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\tilde{y} \delta x} = \frac{1}{\frac{d}{d \tilde{z}} e^{x + \tilde{z} \delta x}}; \eqspace \\
\frac{d \tilde{z}}{d \tilde{y}} N(\tilde{z})
&= e^{-\log(\tilde{y})} \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2}{2 \delta^2 x}}
 = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - x)^2 + 2 \log(\tilde{y}) \delta^2 x }{2 \delta^2 x}} \\
& = \frac{1}{\sqrt{2\pi} \delta x} e^{-\frac{(\log(\tilde{y}) - (x - \delta^2 x))^2 + 2 x \delta^2 x - (\delta^2 x)^2 }{2 \delta^2 x}}
 = N(\frac{\log(\tilde{y}) - (x - \delta^2 x)}{\delta x}) e^{-x + \frac{\delta^2 x}{2}}; \\
& 0 = \tilde{z} e^{x + \tilde{z} \delta x} + (\delta x) e^{x + \tilde{z} \delta x}; \eqspace
 \tilde{z} = -(\delta x) = \frac{1}{\delta x}(x - (\delta x)^2 - x);
\end{align*}

The log function $f(x) = \log(x)$:
\begin{align*}
& \tilde{z} = \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} e^{\tilde{y}} = \frac{1}{\delta x} (x + \tilde{z} \delta x)
 = \frac{1}{\frac{d}{d \tilde{z}} \log(x + \tilde{z} \delta x)}; \\
& \frac{1}{\delta x} e^{\tilde{y}} = (\frac{1}{\delta x} e^{\tilde{y}})^2 \frac{e^{\tilde{y}} - x}{\delta x}; \eqspace 
(e^{\tilde{y}})^2 - x e^{\tilde{y}} - \delta^2 x = 0; \\
e^{\tilde{y}_m} &= \frac{x + \sqrt{x^2 + 4 \delta^2 x}}{2}; \\
& \eqspace 0 = \frac{\tilde{z}}{x + \tilde{z} \delta x} - (\delta x) \frac{1}{(x + \tilde{z} \delta x)^2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} - (\delta x); \\
& \tilde{z} = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 + 4 (\delta x)^2}}{2} - x);
\end{align*}

The power mode $f(x) = x^{\frac{1}{p}}$:
\begin{align*}
& \tilde{z} = \frac{\tilde{y}^p - x}{\delta x}; \eqspace 
\frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} p \tilde{y}^{p-1}; \\
& \frac{1}{\delta x} p (p - 1) \tilde{y}^{p-2} = (\frac{1}{\delta x} p \tilde{y}^{p-1})^2 \frac{\tilde{y}^p - x}{\delta x}; \\
& \tilde{y}^{2p} - x \tilde{y}^{p} - \frac{p - 1}{p} \delta^2 x = 0; \\
\tilde{y}_m^p &= \frac{1}{2} (x + \sqrt{x^2 + 4 \frac{p - 1}{p} \delta^2 x}); \\
& 0 = c (x + \tilde{z} \delta x)^{c-1} \tilde{z} + c (c-1) \delta x (x + \tilde{z} \delta x)^{c-2}; \eqspace
0 = (\delta x) \tilde{z}^2 + x \tilde{z} + (c - 1) (\delta x); \\
& \tilde{z}_m = \frac{1}{\delta x}(\frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2} - x); \eqspace 
f^{-1}(\tilde{y}_m) = \frac{x + \sqrt{x^2 - 4(c-1)(\delta x)^2}}{2}; \\
p = 2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 2 \tilde{y}; \eqspace 
  \tilde{y}_m^{\frac{1}{2}} = \frac{1}{2} \left( x + \sqrt{x^2 + 2 \delta^2 x} \right); \\
p = 3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} 3 \tilde{y}^2; \eqspace  
  \tilde{y}_m^{\frac{1}{3}} = \frac{1}{2} \left( x \pm \sqrt{x^2 + \frac{8}{3} \delta^2 x} \right); \\
p = 1/2:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{2} \tilde{y}^{-\frac{1}{2}}; \eqspace 
  \tilde{y}_m^2 = \frac{1}{2} \left( x + \sqrt{x^2 - 4 \delta^2 x} \right); \\
p = 1/3:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = \frac{1}{\delta x} \frac{1}{3} \tilde{y}^{-\frac{2}{3}}; \eqspace
  \tilde{y}_m^2 = \frac{1}{2} \left( x \pm \sqrt{x^2 - 8 \delta^2 x} \right); \\
p = -1:& \eqspace \frac{d \tilde{z}}{d \tilde{y}} = - \frac{1}{\delta x} \tilde{y}^{-2}; \eqspace 
  \tilde{y}_m^{-1} = \frac{1}{2} \left( x + \sqrt{x^2 + 8 \delta^2 x} \right); 
\end{align*}


The distribution difference:
\begin{align*}
&\nu = \frac{x - f^{-1}(\tilde{y}_m)}{\delta x}: \eqspace 
 N(\frac{f^{-1}(\tilde{y}) - f^{-1}(\tilde{y}_m)}{\delta x} )
 = N(\tilde{z} + \nu) = N(\tilde{z}) e^{-\tilde{z} \nu} e^{-\frac{1}{2} \nu^2}; \\
\eta &\equiv \int |\varrho(\tilde{y}, y, \delta y) - \rho(\tilde{y}, y, \delta y)| d \tilde{y}
 =  \int |\rho(f^{-1}(\tilde{y}), f^{-1}(\tilde{y_m}), \delta x) - \rho(f^{-1}(\tilde{y}), x, \delta x)| d f^{-1}(\tilde{y}) \\
&= \int |N(\tilde{z} + \nu) - N(\tilde{z})| d \tilde{z} 
 = \int |e^{-\frac{1}{2} \nu^2} e^{-\tilde{z} \nu} - 1| N(\tilde{z}) d \tilde{z}; \\
&= | \int |\sum_{m=0}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=0}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n - 1| 
   N(\tilde{z}) d \tilde{z} | \\
&= | \int \left(\sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{(-\nu)^n}{n!} \tilde{z}^n
  - \frac{1}{2} \nu^2 - \nu \tilde{z} \right) N(\tilde{z}) d \tilde{z} | \\
&= \frac{1}{2} \nu^2 - \sum_{m=1}^{\infty} \frac{(-\nu^2)^m}{2^m m!} \sum_{n=1}^{\infty} \frac{\nu^{2n}}{2^n n!}
 = \frac{1}{2} \nu^2 - \sum_{m=2}^{\infty} \sum_{n=1}^{m-1} \frac{(-1)^m}{2^m (m - n)! n!} \nu^{2m} \\
&\simeq \frac{1}{2} \nu^2 - \frac{1}{4} \nu^4 + \frac{1}{8} \nu^6 - \frac{7}{192} \nu^8;
\end{align*}
When $f(x)=x^c$:
\begin{align*}
\nu &= \frac{x - \frac{1}{2} (x + \sqrt{x^2 + (1 - c) 4 \delta^2 x})}{\delta x} = \frac{1 - \sqrt{1 + (1 - c) 4 P(x)^2}}{2 P(x)} \\
&= - \sum_{m=1} \frac{(1 - c)^m (2P(x))^{2m - 1}}{m!} \prod_{n=1}^{m} \frac{\frac{3}{2} -n}{n} \\
&\simeq -(1 - c) P(x) + \frac{1}{4} (1 - c)^2 P(x)^3 - \frac{1}{4} (1 - c)^3 P(x)^5; \\
\eta &= \frac{1}{2} (1-c)^2 P(x)^2 - \frac{1}{4} (1-c)^3 (2-c) P(x)^4 + 1/8 (1-c)^4 (c^2-4c+7) P(x)^6
\end{align*}

\begin{align}
\label{eqn: power distribution}
&y = x^c: \eqspace \rho(\tilde{y}, \mu_y, \sigma_y) = c \tilde{y}^{\frac{1}{c}-1} \frac{1}{\sigma} N(\frac{\tilde{y}^\frac{1}{c} - \mu}{\sigma}); 
\end{align}

\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Distribution.pdf} 
\captionof{figure}{
Probability density function of $\tilde{y} = \tilde{x}^2$, for various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\sqrt{\tilde{y}}$.
}
\label{fig: Square_Distribution}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Square_Root_Distribution.pdf} 
\captionof{figure}{
Probability density function for $\tilde{y} = \sqrt{\tilde{x}}$, various values of $\mu$ as indicated in the legend. 
The variable $\tilde{x}$ follows a Gaussian distribution with the distributional mean $\mu$ and deviation $1$.
The horizontal axis is scaled as $\tilde{y}^2$.
}
\label{fig: Square_Root_Distribution}
\end{figure}

Let $\rho(\tilde{x}, \mu, \sigma)$ denote the probability density function of a random variable $\tilde{x}$ with the distribution mean $\mu$ and distribution deviation $\sigma$.
Let $\tilde{y} = f(\tilde{x})$ be a strictly monotonic function, so that its inverse function $\tilde{x} = f^{-1}(\tilde{y})$ exists.
Formula \eqref{eqn: function distribution} shows the probability density function of $\tilde{y}$ \cite{Statistical_Methods}\cite{Probability_Statistics}.
In Formula \eqref{eqn: function distribution}, the same distribution can be expressed in terms of either $\tilde{x}$ or $\tilde{y}$, which are simply different representations of the same underlying random variable.
\begin{align}
\label{eqn: function distribution}
\rho(\tilde{x}, \mu, \sigma) d\tilde{x} &= \rho(f^{-1}(\tilde{y}), \mu, \sigma) \frac{d\tilde{x}}{d\tilde{y}} d\tilde{y} 
= \rho(\tilde{y}, \mu_y, \sigma_y) d\tilde{y};
\end{align}

Viewed in the $f^{-1}(\tilde{y})$ coordinate, $\rho(\tilde{y}, \mu_y, \sigma_y)$ is given by $\rho(\tilde{x}, \mu, \sigma)$ modulated by $1/f^{(1)}_x$, in which $f^{(1)}_x$ is the first derivative of $f(x)$ with respect to $x$.
\begin{itemize}
\item 
A \emph{distributional pole} occurs when $f^{(1)}_x=0 \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = \infty$.
For example, $(\mu \pm 1)^2$ has a distributional pole at $\tilde{x} = 0$, as shown in Figure \ref{fig: Square_Distribution}.

\item 
A \emph{distributional zero} occurs when $f^{(1)}_x=\infty \rightarrow \rho(\tilde{y}, \mu_y, \sigma_y) = 0$.
For example  $\sqrt{\mu \pm 1}$ has a distributional zero at $\tilde{x} = 0$, as shown in Figure \ref{fig: Square_Root_Distribution}.

\end{itemize}

In both Figure \ref{fig: Square_Distribution} and \ref{fig: Square_Root_Distribution}, $\rho(\tilde{y}, \mu_y, \sigma_y)$ closely representation resembles $\rho(\tilde{x}, \mu, \sigma)$ when the mode of $\rho(\tilde{x}, \mu, \sigma)$ lies sufficiently far away from either a distributional pole or a distributional zero, such as  $(5 \pm 1)^2$ in Figure \ref{fig: Square_Distribution} and $\sqrt{5 \pm 1}$ in Figure \ref{fig: Square_Root_Distribution}, thereby allowing for characterization of the output $f(x)$ using mean $\overline{f(x)}$ and deviation $\delta f(x)$ only. 



\subsection{Statistical Taylor Expansion}

Define $\tilde{z} \equiv (\tilde{x} - \mu)/\sigma$ and let $\rho(\tilde{z})$ be the normalized form of $\rho(\tilde{x}, \mu, \sigma)$ such that $\tilde{z}$ has distribution mean $0$ and distribution deviation $1$.

\begin{align}
\label{eqn: bound moment}
\zeta(n) &\equiv \int_{\varrho}^{\kappa} \tilde{z}^n \rho(\tilde{z}) d \tilde{z};\\
\label{eqn: mean-reverting bounding}
\zeta(1) &= 0;
\end{align}
Let $\tilde{z} \in [\varrho, \kappa]$ where $\varrho, \kappa$ specify the \emph{bounding ranges}. 
Formula \eqref{eqn: bound moment} defines the corresponding \emph{bound moment} $\zeta(n)$, which further satisfies the \emph{mean-reverting condition} of Formula \eqref{eqn: mean-reverting bounding} so that $\kappa$ determines $\varrho$.
For any symmetric $\rho(-\tilde{z}) = \rho(\tilde{z}): \varrho = -\kappa, \zeta(2n+1) = 0$.

\begin{align}
\label{eqn: Taylor 1d} 
f(x + \tilde{x}) &= f(x + \tilde{z} \sigma) = f(x) + \sum_{n=1}^{\infty} \frac{f^{(n)}_x}{n!} \tilde{z}^n \sigma^n; \\
\label{eqn: Taylor 1d mean}
\overline{f(x)} &= \int_{-\varrho}^{+\kappa} f(x + \tilde{x}) \rho(\tilde{x}, \mu, \sigma) d \tilde{x}
  = f(x) + \sum_{n=1}^{\infty}\sigma^n \frac{f^{(n)}_x}{n!} \zeta(n); \\
\label{eqn: Taylor 1d variance}
\delta^2 f(x) &= \overline{(f(x) - \overline{f(x)})^2} = \overline{f(x)^2} - \overline{f(x)}^2 \nonumber \\
	&= \sum_{n=1}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{f^{(j)}_x}{j!} \frac{f^{(n-j)}_x}{(n-j)!} \big(\zeta(n) - \zeta(j) \zeta(n -j) \big);
\end{align}
An analytic function $f(x)$ can be accurately evaluated over in a range using the Taylor series as shown in Formula \eqref{eqn: Taylor 1d}.
Formula \eqref{eqn: Taylor 1d mean} and Formula \eqref{eqn: Taylor 1d variance} yield the mean $\overline{f(x)}$ and the variance $\delta^2 f(x)$ of $f(x)$, respectively.
The difference $\overline{f(x)} - f(x)$ is defined as the \emph{uncertainty bias}, representing the effect of input uncertainty on the resulting value.

\begin{align}
\label{eqn: Taylor 2d}
f(x + \tilde{x}, y + \tilde{y}) &= \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{f^{(m,n)}_{(x,y)}}{m! n!} \tilde{x}^m \tilde{y}^n; \\
\label{eqn: Taylor 2d mean}
\overline{f(x,y)} &=  \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} (\sigma_x)^m (\sigma_y)^n \frac{f^{(m,n)}_{(x,y)}}{m!\;n!} \zeta_x(m) \zeta_y(n);  \\
\label{eqn: Taylor 2d variance}
\delta^2 f(x, y) &= \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} (\sigma_x)^m (\sigma_y)^n \sum_{i=0}^{m} \sum_{j=0}^{n} 
		\frac{f^{(i,j)}_{(x,y)}}{i!\;j!}\frac{f^{(m-i, n-j)}_{(x,y)}}{(m-i)!\;(n-j)!} \nonumber \\
	&\largespace\largespace \big( \zeta_x(m) \zeta_y(n) - \zeta_x(i)\zeta_x(m-i)\; \zeta_y(j)\zeta_y(n-j) \big);
\end{align}

Under the uncorrelated uncertainty condition, Formula \eqref{eqn: Taylor 2d mean} and \eqref{eqn: Taylor 2d variance} compute the mean and variance of the Taylor expansion given in Formula \eqref{eqn: Taylor 2d}, where $\zeta_x(m)$ and $\zeta_y(n)$ denote the variance moments for $x$ and $y$, respectively.
Although Formula \eqref{eqn: Taylor 2d variance} is only for 2-dimensional, it can be extended easily to any dimension.

With the mean reverting condition of Formula \eqref{eqn: mean-reverting bounding}:
\begin{align}
\label{eqn: addition mean}
\overline{x \pm y} &= \zeta(0, \kappa_x) x \pm \zeta(0, \kappa_x) y; \\
\label{eqn: addition variance}
\delta^2 (x \pm y) &= \zeta(2, \kappa_x) (\sigma_x)^2 + \zeta(2, \kappa_y) (\sigma_y)^2; \\
\label{eqn: multiplication mean}
\overline{x y} &= \zeta(0, \kappa_x) x \; \zeta(0, \kappa_y) y; \\
\label{eqn: multiplication variance}
\delta^2 (x y) &= \zeta(2, \kappa_x) (\sigma_x)^2 y^2 + x^2 \zeta(2, \kappa_y) (\sigma_y)^2 + \zeta(2, \kappa_x) (\sigma_x)^2 \; \zeta(2, \kappa_y) (\sigma_y)^2;
\end{align}
When $N \rightarrow \infty$, $\zeta(0, \kappa) \rightarrow 1$ and $\zeta(2, \kappa) \rightarrow 1$, making Formula \eqref{eqn: addition mean} and \eqref{eqn: addition variance} the convolution results for $x \pm y$ \cite{Probability_Statistics}, and Formula \eqref{eqn: multiplication mean} and \eqref{eqn: multiplication variance} the corresponding results of the product distribution for $x y$ \cite{Probability_Statistics}.





\subsection{One-Dimensional Examples}

\iffalse
\begin{align}
\label{eqn: exp Taylor}
& e^{x + \tilde{x}} = e^x \sum_{n=0}^{\infty} \frac{\tilde{x}^n}{n!}; \\
\label{eqn: log Taylor}
& \log(x + \tilde{x}) - \log(x) = \log(1 + \frac{\tilde{x}}{x}) = \sum_{j=1}^{\infty} \frac{(-1)^{j+1}}{j} \frac{\tilde{x}^j}{x^j}; \\
\label{eqn: sin Taylor}
\sin(x + \tilde{x}) &= \sum_{n=0}^{\infty} \eta(n, x) \frac{\tilde{x}^{n}}{n!};
	\eqspace \eta(n, x) \equiv \begin{cases} 
		n = 4j: \eqspace  sin(x); \\ n = 4j + 1: \eqspace  cos(x); \\ n = 4j + 2: \eqspace  -sin(x); \\ n = 4j +3: \eqspace  -cos(x); 
	\end{cases} \\
\label{eqn: power Taylor}
&(x + \tilde{x})^c = x^c (1 + \frac{\tilde{x}}{x})^c = x^c + x^c \sum_{n=1}^{\infty} \frac{\tilde{x}^n}{x^n} \begin{pmatrix} c \\ n \end{pmatrix};
	\eqspace \begin{pmatrix} c \\ n \end{pmatrix} \equiv \frac{\prod_{j=0}^{n-1} (c -j)}{n!};
\end{align}

\begin{align*}
\frac{x + \tilde{x}}{y + \tilde{y}} &\simeq \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y}
  		 - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2
		 + \frac{1}{y^3} \tilde{x} \tilde{y}^2 - \frac{x}{y^4} \tilde{y}^3; \\
(x + \tilde{x}) \frac{1}{y + \tilde{y}} &\simeq (x + \tilde{x}) \left( \frac{1}{y} - \frac{1}{y^2} \tilde{y} + \frac{1}{y^3} \tilde{y}^2 - \frac{1}{y^4} \tilde{y}^3 \right) \\
	&= \frac{x}{y} + \frac{1}{y} \tilde{x} - \frac{x}{y^2} \tilde{y} - \frac{1}{y^2} \tilde{x} \tilde{y} + \frac{x}{y^3} \tilde{y}^2  + \frac{1}{y^3} \tilde{x} \tilde{y}^2
		 -  \frac{x}{y^4} \tilde{y}^3 - \frac{1}{y^4} \tilde{x} \tilde{y}^3;
\end{align*}


\fi

Formula \eqref{eqn: exp mean} and \eqref{eqn: exp precision} give the mean and variance for $e^x$, respectively:
\begin{align}
\label{eqn: exp mean}
\frac{\overline{e^x}}{e^x}  &= 1 + \sum_{n=1}^{\infty} \sigma^n \zeta(n) \frac{1}{n!}; \\
\label{eqn: exp precision}
\frac{\delta^2 e^x}{(e^x)^2} &= \sum_{n=2}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{\zeta(n) - \zeta(j) \zeta(n - j)}{j!\;(n - j)!};
\end{align}

Formula \eqref{eqn: log mean} and \eqref{eqn: log precision} give the mean and variance for $\log(x)$, respectively:
\begin{align}
\label{eqn: log mean}
\overline{\log(x)}  &= \log(x) + \sum_{n=1}^{+\infty} P(x)^{n} \frac{(-1)^{n+1} \zeta(n)}{n}; \\
\label{eqn: log precision}
\delta^2 \log(x) &= \sum_{n=2}^{+\infty} P(x)^{n} \sum_{j=1}^{n-1} \frac{\zeta(n) - \zeta(j) \zeta(n - j)}{j (n-j)};
\end{align}

Formula \eqref{eqn: sin mean} and \eqref{eqn: sin precision} give the mean and variance for $\sin(x)$, respectively:
\begin{align}
\label{eqn: sin Taylor}
\sin(x + \tilde{x}) &= \sum_{n=0}^{\infty} \eta(n, x) \frac{\tilde{x}^{n}}{n!};
	\eqspace \eta(n, x) \equiv \begin{cases} 
		n = 4j: \eqspace  sin(x); \\ n = 4j + 1: \eqspace  cos(x); \\ n = 4j + 2: \eqspace  -sin(x); \\ n = 4j +3: \eqspace  -cos(x); 
	\end{cases} \\
\label{eqn: sin mean}
\overline{\sin(x)} =& \sum_{n=0}^{\infty} \sigma^n \eta(n, x) \frac{\zeta(n)}{n!}; \\
\label{eqn: sin precision}
\delta^2 \sin(x) =& \sum_{n=2}^{\infty} \sigma^n \sum_{j=1}^{n-1} \frac{\eta(j, x)\eta(n - j, x)}{j! (n-j)!}
      	\big(\zeta(n) - \zeta(j) \zeta(n - j)\big); 
\end{align}

Formula \eqref{eqn: power mean} and \eqref{eqn: power precision} give the mean and variance for $x^c$, respectively:
\begin{align}
\label{eqn: power mean}
\frac{\overline{x^c}}{x^c}  &= 1 + \sum_{n=1}^{\infty} P(x)^{n} \zeta(n) \begin{pmatrix} c \\ n \end{pmatrix}; \\
\label{eqn: power precision}
\frac{\delta^2 x^c}{(x^c)^2} &= \sum_{n=2}^{\infty} P(x)^{n} \sum_{j=1}^{n-1}
  \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ n - j \end{pmatrix} \big( \zeta(n) - \zeta(j) \zeta(n - j) \big);
\end{align}

The input and output in statistical Taylor expansion reflects the inherent characteristics of the calculation, such as $\sigma \rightarrow P(e^x)$, $P(x) \rightarrow \delta \log(x)$, $\sigma \rightarrow \delta \sin(x)$, and $P(x) \rightarrow P(x^c)$.



\subsection{Convergence of Variance}

\iffalse

\begin{align*}
\int_{0}^{\kappa}& \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}
		= - \int_{0}^{\kappa} \tilde{z}^{2n - 1} d\; e^{-\frac{1}{2} \tilde{z}^2}
		= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \tilde{z}^{2n - 1} e^{-\frac{1}{2} \tilde{z}^2} \Big |_{0}^{\kappa} \\
		&= (2n - 1) \int_{0}^{\kappa} \tilde{z}^{2n - 2}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1) (2n - 3) \int_{0}^{\kappa} \tilde{z}^{2n - 4}  e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} 
			- (2n - 1) \kappa^{2n - 3} e^{-\frac{1}{2} \kappa^2} - \kappa^{2n - 1} e^{-\frac{1}{2} \kappa^2} \\
		&= (2n - 1)!! \left( \int_{0}^{\kappa} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z} - e^{-\frac{1}{2} \kappa^2} \sum_{j=1}^{n} \frac{\kappa^{2n - 2j + 1}}{(2j - 1)!!}  \right) \\
		&= (2n - 1)!! \left(\sqrt{2} \int_{0}^{\frac{\kappa}{\sqrt{2}}} e^{-(\frac{\tilde{z}}{\sqrt{2}})^2} d \frac{\tilde{z}}{\sqrt{2}} 
			- e^{-\frac{1}{2} \kappa^2} \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!} \right) \\
		& = (2n - 1)!! e^{-\frac{1}{2} \kappa^2} \left( \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}})  e^{\frac{1}{2} \kappa^2} 
			- \sum_{j=0}^{n - 1} \frac{\kappa^{2j + 1}}{(2j + 1)!!}  \right)
\end{align*}
According to Eq (21) in https://mathworld.wolfram.com/DoubleFactorial.html:
\begin{align*}
\sum_{j=0}^{+\infty} \frac{\kappa^{2j + 1}}{(2j + 1)!!} = \sqrt{\frac{\pi}{2}} \xi (\frac{\kappa}{\sqrt{2}}) e^{\frac{1}{2} \kappa^2};
\end{align*}
Thus:
\begin{align*}
\int_{0}^{\kappa} \tilde{z}^{2n} e^{-\frac{1}{2} \tilde{z}^2} d \tilde{z}
		=  e^{-\frac{1}{2} \kappa^2} \sum_{j=n}^{+\infty} \frac{(2n - 1)!!}{(2j + 1)!!} \kappa^{2j + 1}
		= e^{-\frac{1}{2} \kappa^2} \kappa^{2n} \sum_{j=1}^{+\infty} \frac{(2n - 1)!!}{(2n - 1 + 2j)!!} \kappa^{2j - 1};
\end{align*}


\begin{align*}
\zeta(2n) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} \rho(\tilde{z}) d \tilde{z} = 2 N(\kappa) \kappa^{2n} \sum_{j=0}^{\infty} \kappa^{2j+1} \frac{(2n-1)!!}{(2j + 2n+1)!!}; \\
\zeta(2n) &= 2 N(\kappa) \frac{\kappa^{2n+1}}{2n + 1} + 2 N(\kappa) \frac{\kappa^{2n+2}}{2n+1} \sum_{j=1}^{\infty} \kappa^{2(j-1)+1} \frac{(2(n+1)- 1)!!}{(2(j-1)+2(n+1)+1)!!} \\
 &= 2 N(\kappa) \frac{\kappa^{2n + 1}}{2n + 1} + \frac{\zeta(\kappa, 2n + 2)}{2n + 1}; \\
\zeta(\kappa, 2n + 2) &= (2n + 1) \zeta(\kappa, 2n) - 2 N(\kappa) \kappa^{2n + 1};
\end{align*}

\begin{align*}
\int_{-\kappa}^{+\kappa} \frac{1}{2 \sqrt{3}} \tilde{z}^{2n} d \tilde{z} = \frac{1}{\sqrt{3}} \frac{\kappa^{2n+1}}{2n + 1};
\end{align*}

\fi

\iffalse

\begin{align*}
\frac{\delta^2 x^c}{(x^c)^2} &\simeq \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \\
	&\lim_{2n \rightarrow +\infty}  \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	n c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} = \prod_{j=0}^{2n-1} \frac{c -j }{1 + j}; \\
 			c > 0:	n \begin{pmatrix} c \\ n \end{pmatrix}^2 = n \left( \prod_{j=0}^{n-1} \frac{c -j }{1 + j} \right)^2 < n;
 		\end{cases}\\
&\begin{pmatrix} c \\ j + 1 \end{pmatrix} \begin{pmatrix} c \\ 2n - j - 1 \end{pmatrix} 
		= \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} \frac{c - j}{j + 1} \frac{2n - j}{c - 2n + j + 1}; \\
&\sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j + 2 \end{pmatrix} = 
		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix}  \begin{pmatrix} c \\ 2 n - j \end{pmatrix}
		\frac{c - (2 n - j)}{2 n - j + 1} \frac{c - (2 n - j + 1)}{2 n - j + 2} \\
		&\largespace + \left(\frac{c^2}{2} + \frac{c - 2n}{2n + 1} c \right) \begin{pmatrix} c \\ 2n \end{pmatrix}; \\
&\lim_{2n \rightarrow +\infty} \sum_{j=1}^{2n+2-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n + 2 - j \end{pmatrix}  =
	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 
		+ \prod_{j=0}^{2 n - 1} \frac{c - j}{j + 1} \frac{1}{2} ((c - 1)^2 - 1)
\end{align*}

\begin{align}
\label{eqn: pow convergence}
\frac{\delta^2 x^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 		\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
 		\simeq \sum_{n=1}^{\infty} P(x)^{2n} \kappa^{2n} \frac{1}{2n}\sum_{j=1}^{2n-1} 
 				\begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}; \nonumber \\
	&\frac{1}{2n} \sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix} 	< 
 		\begin{cases}		
 			c < 0:	\left| c  \begin{pmatrix} c \\ 2n - 1 \end{pmatrix} \right| < \frac{\left| c \Gamma(c) \right|}{(2n - 1) !}; \\
 			c > 0:	\begin{pmatrix} c \\ n \end{pmatrix}^2 < 1;
 		\end{cases}
\end{align}


\begin{align}
\label{eqn: variance asymptotic}
\lim_{n \rightarrow +\infty}& \zeta(n) - \zeta(j) \zeta(n-j) = \frac{\kappa^n}{n}; \\
\label{eqn: log convergence}
\delta^2 \log(x \pm \delta x) \simeq& \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n) \sum_{j=1}^{2n-1} \frac{1}{j} \frac{1}{2n-j} 
		= \sum_{n = 1}^{+\infty} P(x)^{2n} \zeta(2n) \frac{1}{n} \sum_{j=1}^{2n-1} \frac{1}{j}  \nonumber \\
	\simeq&\; 2\nu(\kappa) \log(2) \sum_{n = 1}^{+\infty} \frac{(P(x) \kappa)^{2n}}{(2n)^2},
		\begin{cases}		
 			\text{Gaussian}:	\nu(\kappa) = N(\kappa) \kappa\\
 			\text{Uniform}:	\nu(\kappa) = 1,\eqspace \kappa = \sqrt{3}
		\end{cases}; \\
\label{eqn: pow convergence}
\frac{\delta^2 (x \pm \delta x)^c}{(x^c)^2} \simeq& \sum_{n=1}^{\infty} P(x)^{2n} \zeta(2n) 
 	\sum_{j=1}^{2n-1} \begin{pmatrix} c \\ j \end{pmatrix} \begin{pmatrix} c \\ 2n - j \end{pmatrix}
	\simeq \nu(\kappa) \sum_{n = 1}^{+\infty} (P(x) \kappa)^{2n} \frac{\begin{pmatrix} 2c \\ 2n \end{pmatrix}}{2n};
\end{align}

\fi



\ifdefined\Verbose
\begin{figure}
\centering
\includegraphics[height=2.5in]{Gaussian_5_Moment.pdf} 
\captionof{figure}{
When $\kappa = 5$, the value of bound moment  $\zeta(2n, \kappa)$ (left y-axis) for Gaussian distribution with order $2n$ (x-axis) versus $2 N(\kappa) \frac{\kappa^{2n+1}}{2n+1}$ (left y-axis), and their ratio (right y-axis).
}
\label{fig: Gaussian_5_Moment}
\end{figure}

\begin{align}
\label{eqn: uniform moment}
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} \frac{1}{2 \sqrt{3}} d \tilde{z} = 2 \rho(\kappa) \frac{\kappa^{2n+1}}{2n + 1}, \;\; 0 \leq \kappa \leq \sqrt{3}; \\
\label{eqn: symetric asymptotic moment} 
\lim_{n \rightarrow +\infty}& \zeta(2n, \kappa) = 2 \rho(\kappa) \frac{\kappa^{2n+1}}{2n+1};
\end{align}
\fi

\begin{align}
\label{eqn: variance asymptotic}
\forall j&: \lim_{n \rightarrow +\infty} \zeta(n) - \zeta(j) \zeta(n-j) = \frac{\kappa^n}{n}; \\
\label{eqn: log convergence}
\rho(-\tilde{z}) =\rho(\tilde{z})&: \delta^2 \log(x \pm \delta x) \sim \sum_{n = 1}^{+\infty} \frac{(P(x) \kappa)^{2n}}{(2n)^2}; \\
\label{eqn: pow convergence}
\rho(-\tilde{z}) =\rho(\tilde{z})&: \frac{\delta^2 (x \pm \delta x)^c}{(x^c)^2} \sim \sum_{n = 1}^{+\infty} (P(x) \kappa)^{2n} \frac{\begin{pmatrix} 2c \\ 2n \end{pmatrix}}{2n};
\end{align}

Formula \eqref{eqn: variance asymptotic} shows that the asymptotic behavior of $\zeta(n, \kappa)$ when $n \rightarrow \infty$ determines if Formula \eqref{eqn: Taylor 1d variance} will converge.
\ifdefined\Verbose
For example, Formula \eqref{eqn: uniform moment} gives $\zeta(2n, \kappa)$ for Uniform distributions, Formula \eqref{eqn: symetric asymptotic moment} shows the asymptoticity for any symmetric distribution $\rho(\tilde{z})$, and Figure \ref{fig: Gaussian_5_Moment} shows $\zeta(2n, 5)$ for Normal distribution when $2n \rightarrow +\infty$.
\fi
\begin{itemize}
\item Formula \eqref{eqn: exp precision} for $e^{x \pm \delta x}$ and Formula \eqref{eqn: sin precision} for $\sin(x \pm \delta x)$ both converge unconditionally.

\item Formula \eqref{eqn: log precision} for $\log(x \pm \delta x)$ can be approximated by Formula \eqref{eqn: log convergence},
which converges when $P(x) < 1/\kappa$.
Statistical Taylor expansion rejects the distributional zero of $\log(x)$ or $(x \pm \delta x)^c$ in the range of $P(x) \gtrapprox 1/\hat{\kappa}$ statistically due to the divergence of Formula \eqref{eqn: log convergence} and \eqref{eqn: pow convergence} mathematically, with $\zeta(2n, \hat{\kappa})$ providing the connection between these two perspectives.

\item Formula \eqref{eqn: power precision} for $(x \pm \delta x)^c$ can be approximated by Formula \eqref{eqn: pow convergence}. which converges when $P(x) \lesssim 1/\kappa$ although the precise upper bound for $P(x)$ varies with $c$.
\end{itemize}





\subsection{Statistical Bounding}

\iffalse
\begin{align}
\label{eqn: Uniform moment} 
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \frac{1}{2 \sqrt{3}} \tilde{z}^{2n} d \tilde{z} = \frac{1}{\sqrt{3}} \frac{\kappa^{2n+1}}{2n + 1}; \\
\label{eqn: Gaussian moment} 
\zeta(2n, \kappa) &= \int_{-\kappa}^{+\kappa} \tilde{z}^{2n} N(\tilde{z}) d \tilde{z}
		= 2 N(\kappa) \kappa^{2n} \sum_{j=1}^{\infty} \kappa^{2j-1} \frac{(2n - 1)!!}{(2n-1 + 2j)!!} \\
	& \zeta(2, \kappa) = \zeta(0, \kappa) - 2 N(\kappa) \kappa;
\end{align}
\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Normal_Bounding_Leakage.pdf} 
\captionof{figure}{
Measured result leakage $\varepsilon(\kappa_s, N)$ (y-axis) for varying measuring range $\kappa_s$ (x-axis) and sample count $N$ (legend).
$\zeta(2, \kappa)$ decreases when $\kappa < 1.5$, which causes the cutoff in the figure.
}
\label{fig: Normal_Bounding_Leakage}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Bounding_Factor_Leakage.pdf} 
\captionof{figure}{
Measured result range $\kappa$ (left y-axis) and corresponding measured result leakage $\varepsilon(\kappa)$ (right y-axis) for varying sample count $N$ (x-axis) when the underlying distribution is uniform or normal (legend), with different measuring bounding range $\kappa_s$ for the normal distribution.
The similar trend for uniform distribution is due to a common artifact in statistics: the $N - 1$ denominator is only unbiased for Gaussian distribution, but it underestimates variances for uniform distribution.
}
\label{fig: Bounding_Factor_Leakage}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=2.5in]{Normal_Function.pdf} 
\captionof{figure}{
The ideal ratio (y-axis) for varying measuring bounding leakage $\kappa_s$ (x-axis) for the selected function $f(x=1 \pm 0.1)$ (legend) when $\hat{\kappa} = 6$.
}
\label{fig: Normal_Function}
\end{figure}




\begin{align}
\label{eqn: offset approximation}
f(x) - \overline{f(x)} &\simeq (\delta x)^2 \zeta(2, \kappa) f_x^{(2)}; \\
\label{eqn: variance approximation}
\delta^2 f(x) &\simeq (\delta x)^2 \zeta(2, \kappa) (f_x^{(1)})^2;
\end{align}

Figure \eqref{eqn: offset approximation} and \eqref{eqn: variance approximation} show the first-order approximation of Formula \eqref{eqn: Taylor 1d mean} and \eqref{eqn: Taylor 1d variance}, respectively.
As $\kappa \rightarrow +\infty$, $\zeta(2, \kappa) \rightarrow 1$ so that $\delta^2 f(x)$ reaches a stable value but the convergence range of $\delta^2 f(x)$ reduces toward zero.
As a trade-off, the choice of $\kappa$ to calculate the stable $f(x) - \overline{f(x)}$ and $\delta^2 f(x)$ is defined as the \emph{ideal range} $\hat{\kappa}$, and the result is the \emph{ideal variance} $\hat{\delta^2} f(x)$.
For Uniform distribution, by definition $\hat{\kappa} = \sqrt{3}$.
For Gaussian distribution, according to the 5-$\sigma$ rule for determining the statistical significance of experimental result \cite{Statistical_Methods}\cite{Precisions_Physical_Measurements}\cite{Probability_Statistics},  $\hat{\kappa} = 5$ by default.

Define \emph{result leakage} $\varepsilon(\kappa) \equiv 1 - \zeta(2, \kappa)$.
When sampling from a distribution, the sample mean $\overline{x}$ and sample deviation $\delta x$ approach the distribution mean $\mu$ and distribution deviation $\sigma$ respectively as the sample count $N$ increases \cite{Probability_Statistics}.
This yields the \emph{sample result leakage} $\varepsilon(\kappa, N)$ for the interval $[\overline{x} - \varrho \delta x, \overline{x} + \kappa \delta x]$, in contrast to the \emph{distributional result leakage} $\varepsilon(\kappa)$ for the interval $[\mu - \varrho \sigma, \mu + \kappa \sigma]$.
Because $\varepsilon(\kappa) \neq \varepsilon(\kappa, N)$ for finite $N$, let $\varepsilon(\kappa) = \varepsilon(\kappa_s, N)$, where $\kappa_s$ is the \emph{measuring range}, and $\kappa(\kappa_s, N)$ is the \emph{measured range}.

\begin{align}
\label{eqn: Gaussian result bounding leakage} 
\epsilon(\kappa) &= \varepsilon(\kappa) + 2 N(\kappa) \kappa; \\
\label{eqn: Gaussian theoretical bounding leakage} 
\epsilon(\kappa) &= 1 - \xi(\frac{\kappa}{\sqrt{2}}); \\
\label{eqn: Gaussian experimental bounding leakage}
\epsilon(\kappa_s, N) &= 1 - \frac{1}{2} \xi(\frac{|\kappa_s \delta x - \overline{x}|}{\sqrt{2}}) - \frac{1}{2} \xi(\frac{|\kappa_s \delta x + \overline{x}|}{\sqrt{2}});
\end{align}

Define \emph{bounding leakage} $\epsilon(\kappa) \equiv 1 - \zeta(0, \kappa)$.
When the underlying distribution is Normal, Formula \eqref{eqn: Gaussian result bounding leakage} presents the difference between the result leakage $\varepsilon(\kappa)$ and the bounding leakage $\epsilon(\kappa)$, while Formula \eqref{eqn: Gaussian theoretical bounding leakage} and \eqref{eqn: Gaussian experimental bounding leakage} give the distributional bounding leakage $\epsilon(\kappa)$ and the sample bounding leakage $\epsilon(\kappa_s, N)$ respectively, where $\xi()$ is the Normal error function \cite{Probability_Statistics}.
Figure \ref{fig: Normal_Bounding_Leakage} shows that $\epsilon(\kappa_s) < \epsilon(\kappa_s, N)$, and $\lim_{N \rightarrow \infty} \epsilon(\kappa_s, N) = \epsilon(\kappa_s)$ along the y-axis direction.
It also shows that $\kappa(\kappa_s, N) < \kappa_s$ and $\lim_{N \rightarrow \infty} \kappa(\kappa_s, N) = \kappa_s$ along the x-axis direction. 
Figure \ref{fig: Bounding_Factor_Leakage} sections Figure \ref{fig: Normal_Bounding_Leakage} along the y-axis direction for $\kappa_s=4,5,6$, and shows the $\kappa(\kappa_s, N)$ which is used in $\zeta(n, \kappa)$.
It shows that to reach the stable variance for a $\kappa_s$, the required sample count is $N \gtrsim 10^3$.


\begin{align}
\label{ideal ratio}
\alpha &\equiv \frac{\delta^2 f(x)}{\hat{\delta^2} f} = \frac{\zeta(2, \kappa)}{\zeta(2, \hat{\kappa})}; \\
\label{eqn: sum leakage}
\alpha(x \pm y) &= \frac{\zeta_x(2, \kappa_x) (\delta x)^2 + \zeta_y(2, \kappa_y) (\delta y)^2}{(\delta x)^2 + (\delta y)^2};
\end{align}

Define \emph{ideal ratio} as $\alpha \equiv \delta^2 f/\widehat{\delta^2} f$.
The ideal ratio $\alpha$ quantifies the reliability of $\hat{\delta^2} f$ from the underlying distributions and the corresponding sample counts of all inputs, with Formula \eqref{eqn: sum leakage} as  an example for $\alpha(x \pm y)$ .
Figure \ref{fig: Bounding_Factor_Leakage} shows that the ideal ratio converges to $1$ with increasing sample count $N$ faster when $\hat{\kappa}$ is larger.
Figure \ref{fig: Normal_Function} demonstrates that for Gaussian distribution, ideal variance $\hat{\delta^2} f(x)$ stabilizes with increasing $\hat{\kappa}$, for example, with absolute value changes less than $10^{-4}$ when $\hat{\kappa} \geq 5$.



\subsection{Dependency Tracing}

\begin{align}
\label{eqn: sum variance}
\delta^2 (f + g) =&\; \delta^2 f + \delta^2 g + 2 (\overline{fg} - \overline{f}\overline{g}); \\
\label{eqn: linear variance}
\delta^2 (c_1 f + c_0) =&\; c_1^2 \delta^2f;
\end{align}


When all inputs satisfy the uncorrelated uncertainty assumption, statistical Taylor expansion traces dependencies through the intermediate steps.
For example:
\begin{itemize}
\item Formula \eqref{eqn: sum variance} expresses $\delta^2 (f + g)$, The dependency tracing of is illustrated by $\delta^2 (f - f) = 0$, and $\delta^2 (f(x) + g(y)) = \delta^2 f + \delta^2 g$, with the latter corresponding to Formula \eqref{eqn: addition variance}.   

\item The dependence tracing of $\delta^2 (f g)$ is illustrated by $\delta^2 (f/f) = 0$, $\delta^2 (f f) = \delta^2 f^2$, and $\delta^2 (f(x) g(y)) = \overline{f}^2 (\delta^2 g) + (\delta^2 f) \overline{g}^2 +  (\delta^2 f) (\delta^2 g)$, with the latter corresponding to Formula \eqref{eqn: multiplication variance}.  

\item The dependency tracing of $\delta^2 f(g(x))$ is demonstrated by $\delta^2 (f^{-1}(f(x))) = (\delta x)^2$.  
For a reversible transformation such as matrix reversion or FFT, after a \emph{round-trip transformation} which is a forward transformation followed by a reverse transformation, the original input should be restored.

\item Formula \eqref{eqn: linear variance} gives the variance of the linear transformation of a function, which can be applied to other formulas for more general dependency tracing.

\end{itemize}
Statistical Taylor expansion employs dependency tracing to ensure that the calculated mean and variance satisfy statistics rigorously.
Dependency tracing also implies that the results of statistical Taylor expansion must remain path independent.
However, dependency tracing comes at a cost: variance calculations are generally more complex than value calculations and exhibits a narrower convergence range for input variables.


\subsection{Traditional Execution and Dependency Problem}

Dependency tracing requires an analytic form of the function to apply statistical Taylor expansion for the result mean and variance, as in Formula \eqref{eqn: Taylor 1d mean}, \eqref{eqn: Taylor 1d variance}, \eqref{eqn: Taylor 2d mean}, and \eqref{eqn: Taylor 2d variance}.
This requirement often conflicts with conventional numerical methods for analytic functions:
\begin{itemize}

\item
Traditionally, intermediate variables are widely used in computations; however, this practice disrupts dependency tracing by obscuring the relationships among the original input variables.

\item
Similarly, conditional executions are often employed to optimize performance and minimize rounding errors, for example, using Gaussian elimination to minimize floating-point rounding errors in matrix inversion \cite{Linear_Algebra}.  
For dependency tracing, such conditional executions should instead be replaced by direct matrix inversion as described in Section \ref{sec: matrix}.

\item 
Furthermore, traditional approaches frequently apply approximations to result values during execution.
Under the statistical Taylor expansion, Formula \eqref{eqn: Taylor 1d variance} shows that the variance converges more slowly than value in statistical Taylor expansion.
Consequently, approximation strategies should prioritize variances than values.

\item
Traditionally, results from mathematical library functions are accepted without scrutiny, with accuracy assumed down to the last digit.
As demonstrated in Section \ref{sec: FFT}, statistical Taylor expansion enables the detection of numerical errors within these functions and requires that they be recalculated with uncertainty deviations explicitly incorporated into the output.

\item 
In conventional practice, an analytic expression is often decomposed into simpler, ostensibly and independent arithmetic operations such as negation, addition, multiplication, division, square root, and library calls.
However, this decomposition introduces dependency problems.
For example, if $x^2 - x$ is calculated as $x^2 - x$, $x(x - 1)$, and $(x - \frac{1}{2})^2 - \frac{1}{4}$, only $(x - \frac{1}{2})^2 - \frac{1}{4}$ gives the correct result, while the other two give wrong results for wrong independence assumptions between $x^2$ and $x$, or between $x -1$ and $x$, respectively.

\item
Similarly, large calculations are often divided into sequential steps, such as computing $f(g(x))$ as $f(y)|_{y = g(x)}$.
This approach also introduces the dependency problem by ignoring dependency tracing within $g(x)$ affecting $f(g(x))$, such as $\overline{(\sqrt{x})^2} > \overline{\sqrt{x^2}}$ and $\delta^2 (\sqrt{x})^2 > \delta^2 \sqrt{x^2}$.

\end{itemize}
Dependency tracing therefore removes nearly all flexibility from traditional numerical executions, effectively eliminating the associated dependency problems.
Consequently, all conventional numerical algorithms must be reevaluated or redesigned to align with the principles of statistical Taylor expansion.








\section{Variance Arithmetic}
\label{sec: variance arithmetic}

Variance arithmetic implements statistical Taylor expansion.
It represents an imprecise value $x \pm \delta x$ using a pair of 64-bit standard floating-point numbers and uses floating-point arithmetic for computation.

Because of the finite precision and limited range of conventional floating-point representation, $\zeta(n)$ can only be computed to limited terms.
Consequently, the following numerical rules are introduced:
\begin{itemize}
\item \emph{finite}: The resulting value and variance must remain finite.

\item \emph{monotonic}: As a necessary condition for convergence, the last $20$ terms of the expansion must decrease monotonically in absolute value, ensuring that the probability of the expansion exhibiting an absolute increase is no more than $2^{-20} \simeq 9.53\; 10^{-7}$.

\item \emph{stable}: To avoid truncation error \cite{Numerical_Recipes}, the absolute value of the last expansion term must be less than $\epsilon$ times of both the result deviation and the result absolute value, in which $\epsilon \simeq 5.73\;10^{-7}$ is the bounding leakage for Gaussian distribution with $\hat{\kappa} = 5$.
This rule ensures sufficiently fast convergence in the context of monotonic convergence.

\item \emph{positive}: At every expansion order, the expansion variance must be positive.

\item \emph{reliable}: At every order, the deviation of the variance must be less than $1/5$ times the value of the variance.

\end{itemize}

For simplicity of discussion, the Taylor coefficients in Formula \eqref{eqn: Taylor 1d} and \eqref{eqn: Taylor 2d} are assumed to be precise.


\subsection{Monotonic}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for Gaussian uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
}
\label{fig: Pow_Conv_Edge}
\end{figure}

\ifdefined\Verbose

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $(1 \pm \delta x)^c$ across different values of $c$ (x-axis) for uniform uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
When $c$ is a natural number, $\delta x$ has no upper bound; however, such cases are omitted in the figure.
When $c \in [0, 1]$, uncertainty bias is negative so it does not show up in the plot.
}
\label{fig: Pow_Conv_Edge_Uniform}
\end{figure}

\fi

Beyond an upper bound $\delta x$, the expansion is no longer monotonic for $e^{x \pm \delta x}$, $\log(x \pm \delta x)$, and $(x \pm \delta x)^c$.

For Gaussian input uncertainty with $\hat{\kappa}=5$:
\begin{itemize}
\item 
For $e^{x \pm \delta x}$, $\delta x \lesssim 19.864$ and $P(e^{x \pm \delta x}) \lesssim 1681.767$ regardless of $x$.
These limits follow directly from the relationship $\delta x \rightarrow P(e^x)$, as indicated in Formula \eqref{eqn: exp precision}.

\item 
For $\log(x \pm \delta x)$, $P(x) \lesssim 0.20086$ and $\delta \log(x \pm \delta x) \lesssim 0.213$ regardless of $x$.
These limits follow directly from the relationship $P(x) \rightarrow \delta \log(x)$, as indicated in Formula \eqref{eqn: log precision}.

\item
For $(x \pm \delta x)^c$, except when $c$ is a natural number, the upper bound $P(x)$ is close to $1/5$ but increasing with $c$. 
This trend is displayed in Figure \ref{fig: Pow_Conv_Edge}.
\end{itemize}
Similar trends holds when the input uncertainty is uniform but with different values.
For example, after the respective upper bound $P(x)$ for $(x \pm \delta x)^c$ are normalized by the corresponding $\hat{\kappa}$, they show almost the same trends of increasing with $c$.
\ifdefined\Verbose
Figure \ref{fig: Pow_Conv_Edge_Uniform} shows the upper bound $P(x)$ for $(x \pm \delta x)^c$.
\fi 



\subsection{Positive}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for Gaussian uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge}
\end{figure}

\ifdefined\Verbose
\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_Conv_Edge.Uniform.pdf} 
\captionof{figure}{
Measured upper bound $\delta x$ (left y-axis) for $\sin(x \pm \delta x)$ across different values of $x$ (x-axis) for uniform uncertainty.
The corresponding uncertainty bias and uncertainty are also shown (right y-axis).
The x-axis is expressed in units of $\pi$. 
}
\label{fig: Sin_Conv_Edge_Uniform}
\end{figure}
\fi

Besides convergence, the variance expansion may yield negative results, as in Formula \eqref{eqn: sin precision} for $sin(x \pm \delta x)$.
Figure \ref{fig: Sin_Conv_Edge} shows that the upper bound of $\delta x$ for $sin(x \pm \delta x)$ varies periodically between $0.318 \pi$ and $0.416 \pi$ for Gaussian input uncertainty.
Beyond this upper bound, the result deviation is no longer positive at some expansion order.
Conceptually, the upper-bound for $\delta x$ should not be much more than $\pi/2$ to make the result meaningful, which Figure \ref{fig: Sin_Conv_Edge} satisfies.
\ifdefined\Verbose
Similar trend holds when the input uncertainty is uniform but with larger upper bound $\delta x$, as shown in Figure \ref{fig: Sin_Conv_Edge_Uniform}.
\else
Similar trend holds when the input uncertainty is uniform but with larger upper bound $\delta x$ still less than $\pi/2$.
\fi


\subsection{Floating-Point Rounding Errors}

Variance arithmetic incorporates floating-point rounding errors as the $\delta x$ when converting a floating-point value $x$ into $x \pm \delta x$. 
Unless all the least 20 bits of the significand of $x$ are zero, $\delta x$ is assumed to be $1/\sqrt{3}$ times of the ULP of $x$, where ULP refers to the \emph{Unit in the Last Place} in conventional floating-point representation \cite{Floating_Point_Standard}, because rounding errors are shown to be uniformly distributed within ULP \cite{Prev_Precision_Arithmetic}.



\ifdefined\Verbose

\subsection{Comparison}

\iffalse

Statistically the less relation between two imprecise values $x \pm \delta x$ and $y \pm (\delta y)^2$ is calculated by Formula \ref{eqn: x < y}:
\begin{align}
& z \equiv \frac{\tilde{y} - y}{\delta y}; \eqspace \tilde{y} = \delta y z + y; \\
& x - \Delta x < z \delta y + y < x + \Delta x; \\ & x - \Delta x - y < z \delta y < x + \Delta x - y \\
& - \Delta y < z \delta y < \Delta y; \\
p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) & = 
  \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
    \int_{x - \Delta x}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
  \int_{-\frac{\Delta x}{\delta x}}^{\frac{\tilde{y} - x}{\delta x}} N(z) d z \;d \tilde{y}; \\
& = \int_{y - \Delta y}^{y + \Delta y} \rho(\tilde{y}, y, \delta y) 
      \frac{1}{2}(\frac{\tilde{y} - x}{\sqrt{2} \delta x}) - \zeta(\frac{-\Delta x}{\sqrt{2} \delta x})) \;d \tilde{y}; \\
& = \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x})\right) N(z) d z; \\
p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) & =     
   \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x})\right) N(z) d z;
\end{align}
Formula \eqref{eqn: x < y} seems correct because it predicts $p(x \le y) = p(y \ge x)$:
\begin{align}
& \frac{d}{d \tilde{y}} \int_{-\infty}^{\tilde{y}} \rho(\tilde{x}, x, \delta x) d \tilde{x} = \rho(\tilde{y}, x, \delta x); \\
p(x< y) & = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \rho(\tilde{y}, x, \delta x) d \tilde{x} d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, y, \delta y) \;d \rho(\tilde{y}, x, \delta x) \\
& = 0 - \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) \;d \rho(\tilde{y}, y, \delta y) \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{y}, x, \delta x) 
\int_{\tilde{y}}^{+\infty} \rho(\tilde{x}, y, \delta y) 
d \tilde{x} \;d \tilde{y} \\
& = \int_{-\infty}^{+\infty} \rho(\tilde{x}, x, \delta x) 
\int_{\tilde{x}}^{+\infty} \rho(\tilde{y}, y, \delta y) 
d \tilde{y} \;d \tilde{x}
\end{align}

Two imprecise values can be compared directly for less or greater relation when their ranges $(x - \Delta x, x + \Delta x)$ and $(y - \Delta y, y + \Delta y)$ do not overlap. 
Otherwise, statistically, Formula \eqref{eqn: x < y} and \eqref{eqn: x > y} gives the probability for less or greater relation between $x \pm (\delta x)^2 \le y \pm (\delta y)^2$, in which $\min()$ and $\max()$ are minimal and maximal functions, respectively.
Formula \eqref{eqn: no =} shows that two imprecise variable can not be equal statistically.
Let $x = y$, $\delta x = \delta y$ and $\Delta x = \Delta y$, Formula \eqref{eqn: x < y} and \eqref{eqn: x > y} shows that two conceptually equal imprecise values has $1/2$ chance to be one less than the other, and $1/2$ chance to be one more than the other.
Thus,
\begin{align}
\label{eqn: x < y}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) = 
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x}) - \zeta(-\frac{\Delta x}{\sqrt{2} \delta x})\right) N(z) d z; \\
\label{eqn: x > y}
& p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) =     
  \int_{\frac{\max(-\Delta y, x - \Delta x - y)}{\delta y}}^{\frac{\min(+\Delta y, x + \Delta x - y)}{\delta y}} 
      \frac{1}{2} \left(\zeta(+\frac{\Delta x}{\sqrt{2} \delta x}) - \zeta(\frac{z \delta y + y - x}{\sqrt{2} \delta x})\right) N(z) d z; \\
\label{eqn: no =}
& p\left( x \pm (\delta x)^2 < y \pm (\delta y)^2 \right) + p\left( x \pm (\delta x)^2 > y \pm (\delta y)^2 \right) = 1;
\end{align}

\fi

Two imprecise values can be compared statistically based on their difference.

When the value difference is zero, the two imprecise values are considered equal.  
In statistics, such two values have a $50\%$ possibility of being either less than or greater to each other but zero probability of being exactly equal \cite{Probability_Statistics}.
In variance arithmetic, however, they are treated as neither less nor greater than each other and therefore are considered equal.

Otherwise, the standard z-statistic method \cite{Probability_Statistics} is applied to determine whether two imprecise values are statistically equal, less than, or greater than each other.
For example, the difference between $1.002 \pm 0.001$ and $1.000 \pm 0.002$ is $0.002 \pm 0.00224$, yielding $z = 0.002 / 0.00224$.
The probability that they are not equal is $\xi(|z|/\sqrt{2}) = 62.8\%$, in which $\xi(z)$ is the cumulative distribution function for Normal distribution \cite{Probability_Statistics}.
If the threshold probability for inequality is set at $50\%$, then $1.000 \pm 0.002 < 1.002 \pm 0.001$.
Alternatively, an equivalent bounding range for z can be used, such as $|z| \leq 0.67448975$ for an equal probability threshold of $50\%$.

Because the result of comparison depends on threshold probability which is application specific, comparison is not part of variance arithmetic.

\fi


\section{Verification of Variance Arithmetic}
\label{sec: validation}

Analytic functions or algorithms with precisely known results are used to evaluate the outputs of variance arithmetic based on the following statistical properties: 
\begin{itemize}

\item \emph{Value error}: the difference between the numerical result and the corresponding known precise analytic result.

\item \emph{Normalized error}: the ratio of a value error to the corresponding result deviation from statistical Taylor expansion.

\item \emph{Error deviation}: the standard deviation of normalized errors.

\item \emph{Error distribution}: the histogram of the normalized errors.

\end{itemize}

Once input error from every source is accounted precisely, \emph{ideal coverage} is achieved in either contexts:
\begin{itemize}
\item \emph{Distribution Test}:
When comparing the calculated mean and deviation with the result data set, the error deviation is exactly $1$ and the error distribution is Normal.
Empirically, error distributions are close to Normal when error deviation are close to 1, regardless of input uncertainty distribution, and such convergence occurs rapidly \cite{Prev_Precision_Arithmetic}.
This is due to central limit theorem \cite{Probability_Statistics}.

\item \emph{Value Test}: 
When comparing the calculated value and the result value in a corresponding data set, the error deviation is much less than $1$ and the error distribution is Delta.
For example, a round-trip test is a value test.

\end{itemize}

However, if the input uncertainty is known only to order of magnitude, \emph{proper coverage} is achieved when the error deviations fall within the range $[0.1, 10]$.

When an input contains unspecified errors, such as numerical errors in library functions or floating-point rounding errors, Gaussian noise with progressively increasing deviations can be added, until ideal coverage is attained.
The minimal noise deviation required provides a good estimate of the magnitude of the unspecified input uncertainty deviations.
Achieving ideal coverage serves as a necessary verification step to ensure that statistical Taylor expansion has been applied correctly within the given context.
The input noise range that yields ideal coverage defines the ideal application range for the analytic function.




\section{Polynomial}
\label{sec: polynomial}

Formula \eqref{eqn: polynomial Taylor} presents polynomial Taylor expansion:
\begin{align}
\label{eqn: polynomial Taylor}
\sum_{j=0}^{N} c_j (x + \tilde{x})^j &= \sum_{j=0}^{N} \tilde{x}^{j} P_j, \eqspace
	P_j \equiv \sum_{k=0}^{N-j} x^{k - j} c_{j + k} \begin{pmatrix} j + k \\ j \end{pmatrix};
\end{align}
Because the maximal expansion term using Formula \eqref{eqn: polynomial Taylor} is $\tilde{x}^{2N}$, $N$ in Formula \eqref{eqn: polynomial Taylor} can only reach half of the maximal expansion order of Formula \eqref{eqn: Taylor 1d variance}, for example, $224 = 448/2$ when the input uncertainty is assumed to be Gaussian.

\subsection{Residual Error}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Poly_x.pdf}
\captionof{figure}{
Residual error of $\sum_{j=0}^{224} x^j - \frac{1}{1 - x}$ vs $x$ (x-axis).
The y-axis to the left shows both the value and the uncertainty of the residual errors.
The y-axis to the right indicates the expansion order needed to reach stable value for each $x$. 
}
\label{fig: Poly_x}
\end{figure}

Figure \ref{fig: Poly_x} shows the residual error of $\sum_{j=0}^{224} x^j  - 1(1 - x)$.
It also displays the required expansion orders for $1/(1 - x)$, which are all less than $224$.
Therefore, the residual error reflects solely the rounding error between $\sum_{j=0}^{224} x^j$ and $\frac{1}{1 - x}$.
A detailed analysis indicates that the maximal residual error is $4$ times the ULP of $1/(1 - x)$.
In all cases, the calculated uncertainty bounds the residual error effectively for all $x$, with an error deviation of $2.60$ when the expansion order is less than $224$.
Variance arithmetic can provide proper coverage for rounding errors.  


\subsection{Continuity}

In variance arithmetic, the result mean, variance and histogram are generally continuous across parameter space.
For example, $\delta x$ has an upper bound for $(x \pm \delta x)^c$ to converge except when $c$ is a natural number $n$.
The result mean, variance and histogram of $(x \pm \delta x)^c$ remain continuous around $c = n$.


\subsection{Distributional Hole}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_at_x=0.pdf} 
\captionof{figure}{
Histograms of normalized errors for $(x \pm 0.2)^n$, with $x = 0, -0.2, +0.2$, and $n = 2, 3$, as indicated in the legend.
}
\label{fig: Poly_Continuity}
\end{figure}

Unlike a distributional zero, a statistical bounding range in variance arithmetic can include a distributional pole, such as around $(0 \pm \delta x)^c, c > 1$.
The presence of such poles does not disrupt the continuity of the result mean, variance, or histogram.
Figure \ref{fig: Poly_Continuity} illustrates the error distributions of $(x \pm 0.2)^n$ when $x = 0, -0.2, +0.2$ and $n = 2, 3$.
\begin{itemize}
\item When the second derivative is zero, the resulting distribution is symmetric two-sided and Delta-like, such as when $n = 3, x = 0$.

\item When the second derivative is positive, the resulting distribution is right-sided Delta-like, such as the distribution when $n = 2, x = 0$, or when $n = 2, x = \pm 0.2$, or when $n = 3, x = 0.2$.

\item When the second derivative is negative, the resulted distribution is left-sided and Delta-like, such as when $n = 3, x = -0.2$, which is the mirror image of the distribution when $n = 3, x = 0.2$.

\end{itemize}
In each case, the transition from $x = 0$ to $x = 0.2$ is continuous for the resulting mean and deviation.




\section{Matrix Calculations}
\label{sec: matrix}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Adjugate_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) of adjugate matrix $\widetilde{\mathbf{M}}^A - \mathbf{M}^A$ as a function of matrix size (x-axis) and input noise precision (y-axis).
}
\label{fig: Adjugate_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Forward_Error_vs_Size_Noise.pdf} 
\captionof{figure}{
Error deviations (z-axis) as a function of matrix size (x-axis) and input noise precision (y-axis) for the difference of the two sides of Formula \eqref{eqn: adjugate matrix}.
}
\label{fig: Forward_Error_vs_Size_Noise}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Matrix_Determinant_Prec_vs_Condition.pdf} 
\captionof{figure}{
Linear correlation between the precision of a matrix determinant (y-axis) to its condition number (x-axis).
The legend shows the size of the matrix, as well as the type of the matrix as \textit{Random} for randomly generated matrix, and \textit{Hilbert} as the Hilbert matrix.
}
\label{fig: Matrix_Determinant_Prec_vs_Condition}
\end{figure}


\begin{align}
\label{eqn: determinant mean}
\overline{|\mathbf{M}|} &= |\mathbf{M}|; \\
\label{eqn: determinant variance}
\delta^2 |\mathbf{M}| &= \sum_{m=1}^{n} \sum_{<i_1 \dots i_m>_n} \sum_{[j_1 \dots j_m]_n}
  	|\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}|^2 \prod _{k=1 \dots n}^{i_k \in \{i_1 \dots i_m\}} (\delta x_{i_k, j_k})^2; 
\end{align}

Let $[j_1, j_{2} \dots j_m]_n$ denote a permutation of length $m$ from the vector $(1,2\dots n)$, and let $<j_1, j_{2} \dots j_m>_n$ for the ordered permutation of length $m$ \cite{Linear_Algebra}.  
Formula \eqref{eqn: determinant mean} and \eqref{eqn: determinant variance} gives the statistical Taylor expansion of the determinant of a square matrix $\mathbf{M}$ of size $n$, when the uncertainties of matrix elements are all independent of each other.
In Formula \eqref{eqn: determinant variance}, $\mathbf{M}_{<i_1 \dots i_m>_n, [j_1 \dots j_m]_n}$ is a sub-matrix for $\mathbf{M}$, in which $<i_1 \dots i_m>_n$ contains the row indices, and $[j_1 \dots j_m]_n$ contains the column indices. 

The square matrix whose element is $(-1)^{i+j}|m_{j,i}|$ is defined as the \emph{adjugate matrix} \cite{Linear_Algebra} $\mathbf{M}^A$ to the original square matrix $\mathbf{M}$, in which $m_{j,i}$ is the element of $\mathbf{M}$ at row index $j$ and column index $i$. 
Each $m_{j,i}$ is an integer between $[-2^8, +2^8]$ so that  $\mathbf{M}^A$ can be calculated precisely using integer arithmetic.
By adding Gaussian noise of deviation $\delta x$ to each $m_{j,i}$, $\widetilde{\mathbf{M}}$ is created.
The value error of $\mathbf{M}^A$ is the difference between $\widetilde{\mathbf{M}^A}$ and $\mathbf{M}^A$, while the result deviation is calculated by Formula \eqref{eqn: determinant variance}, so that $\widetilde{\mathbf{M}^A} - \mathbf{M}^A$ is a distribution test.
Figure \ref{fig: Adjugate_Error_vs_Size_Noise} shows that the error deviations of $\widetilde{\mathbf{M}^A} - \mathbf{M}^A$ are very close to $1$ except when the matrix size is $8$ and the input uncertainty is $\delta x = 10^{-17}$.
This abnormality is caused by floating-point rounding errors only at this point, when the element values exceed the significand range of 64-bit floating-point representation, to result in a error deviation of $3.2$. 
Figure \ref{fig: Adjugate_Error_vs_Size_Noise} demonstrates a typical distribution test of error deviation versus input noise deviation $\delta x$ and another specific dimension, with ideal coverage for all region except when $\delta x \rightarrow 0$.

\begin{align}
\label{eqn: adjugate matrix}
\mathbf{M} \times \mathbf{M}^A &= \mathbf{M}^A \times \mathbf{M} = |\mathbf{M}| \mathbf{I}; \\
\label{eqn: inverse matrix}
\mathbf{M}^{-1} &\equiv\; \mathbf{M}^A / |\mathbf{M}|; \\
\label{eqn: inverse matrix 2 variance}
\delta^2 \left( \begin{matrix} w, x \\ y, z \end{matrix} \right)^{-1} \simeq&\; \frac{
	 	\left( \begin{matrix} z^4, x^2 z^2 \\ y^2 z^2,  x^2 y^2 \end{matrix} \right) (\delta w)^2 +
	 	\left( \begin{matrix} y^2 z^2, w^2 z^2 \\ y^4, w^2 y^2 \end{matrix} \right) (\delta x)^2}{(w z - x y)^4} + \\
	&\; \frac{\left( \begin{matrix} x^2 z^2, x^4 \\ w^2 z^2, w^2 x^2 \end{matrix} \right) (\delta y)^2 +
	 	\left( \begin{matrix} x^2 y^2, w^2 x^2 \\ w^2 y^2, w^4 \end{matrix} \right) (\delta z)^2
	}{(w z - x y)^4}; \nonumber
\end{align}
Let $\mathbf{I}$ be the identity matrix for $\mathbf{M}$ \cite{Linear_Algebra}.
Formula \eqref{eqn: adjugate matrix} show the relation of $\mathbf{M}^A$ and $\mathbf{M}$ which leads to the definition of inverse matrix $\mathbf{M}^{-1}$ in Formula \eqref{eqn: inverse matrix} \cite{Linear_Algebra}.
$\tilde{\mathbf{M}} \times \tilde{\mathbf{M}}^A - |\tilde{\mathbf{M}}| \mathbf{I}$ tests each element value so that it is a value test.
Figure \ref{fig: Forward_Error_vs_Size_Noise} shows the error deviation of a typical value test: the error deviation decreases linearly with increasing input uncertainty deviation $\delta x$, because the value should match unless at $\delta x \rightarrow 0$ when floating-point rounding errors dominate.
The matrix size is the specific dimension in Figure \ref{fig: Forward_Error_vs_Size_Noise}.

Because an element of the original matrix $\mathbf{M}$ appears multiple times in Formula \eqref{eqn: inverse matrix}, its result variance is very complicated using Formula \eqref{eqn: Taylor 2d variance}. 
For example, Formula \eqref{eqn: inverse matrix 2 variance} shows the simplest case for Formula \eqref{eqn: inverse matrix}: the first-order approximation of a 2x2 matrix.
Contrary to traditional approach, statistical Taylor expansion uses Formula \eqref{eqn: inverse matrix} for matrix inversion instead of Gaussian elimination \cite{Numerical_Recipes}, because logically, the result should be symmetric for all matrix elements, as what Formula \eqref{eqn: inverse matrix 2 variance} has demonstrated.

In Formula \eqref{eqn: inverse matrix}, $\mathbf{M}^{-1}$ is dominated by $1/|\mathbf{M}|$, suggesting that the precision of $\mathbf{M}^{-1}$ is largely determined by the precision of $|\mathbf{M}|$.
Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} shows that there is a strong linear correlation between conditional numbers and the corresponding determinant precision of matrices.
As a reference, Figure \ref{fig: Matrix_Determinant_Prec_vs_Condition} presents the Hilbert matrix \cite{Linear_Algebra} for each matrix size and shows that the Hilbert matrices also follow the linear relation between determinant precision and condition number.
Thus, determinant precision can replace matrix condition number.


\section{Mathematical Library Functions}
\label{sec: Math Library}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|} 
\hline 
Basic Function   & $e^{x \pm \delta x}$  & $\log(x \pm \delta x)$  & $(1 \pm \delta x)^c$   & $\sin(x \pm \delta x)$ \\ 
\hline 
Range               & $x \in [-100, +100]$  & $x \in [1/32, 32]$        & $c \in [-3, +3]$           & $x \in [-\pi, +\pi]$     \\
\hline 
Error Deviation  & $1.000 \pm 0.010$    & $0.999 \pm 0.011$       & $0.989 \pm 0.104$      & $0.978 \pm 0.150$ \\
\hline 
\end{tabular}
}
\captionof{table}{
The result error deviation of selected basic functions using variance arithmetic when $\delta x > 10^{-15}$.
The error deviation for $(1 \pm \delta x)^c$ can be improved to $0.998 \pm 0.034$ if diverge regions are excluded.
The error deviation for $\sin(x \pm \delta x)$ can be improved to $1.000 \pm 0.010$ if pole regions are excluded.
}
\label{tbl: basic functions}
\end{table}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Sin_X_Dev.pdf} 
\captionof{figure}{
Error deviation for $\sin(x \pm \delta x)$ as a function of $x$ and $\delta x$.
The x-axis represents $x$ value between $-\pi$ and $+\pi$.
The y-axis represents $\delta x$ value between $-10^{-16}$ and $1$.
The z-axis shows the corresponding error deviation. 
}
\label{fig: Sin_X_Dev}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{ExpLog_Error.pdf} 
\captionof{figure}{
Values and uncertainties of $\log(e^x) - x$ and $e^{\log(x)} - x$ as functions of $x$, evaluated at $0.1$ increment.
When $x$ is 2's fractional such as $1/2$ or $1$, the result uncertainties are significantly smaller.
}
\label{fig: ExpLog_Error}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Pow_Error.pdf} 
\captionof{figure}{
Normalized errors of $(x^p)^{\frac{1}{p}} - x$ as functions of $x$ and $p$.
}
\label{fig: Power_Error}
\end{figure}


Table \ref{tbl: basic functions} shows that Formula \eqref{eqn: exp precision}, \eqref{eqn: log precision}, \eqref{eqn: sin precision}, and \eqref{eqn: power precision} provide almost perfect deviation for the result values which are calculated using corresponding math library functions by sampling from $x \pm \delta x$ in which $\delta x$ is Gaussian input noise deviation.
Table \ref{tbl: basic functions} shows the results from Python, which differs slightly from those from either C++ or Java.

The result error deviation is similar to Figure \ref{fig: Adjugate_Error_vs_Size_Noise} but with the specific dimension as $x$ in $e^{x \pm \delta x}$ and $\log(x \pm \delta x)$, or $c$ in $(1 + \delta x)^c$.
The coverage is only proper when $ \delta x < 10^{-15}$.

Figure \ref{fig: Sin_X_Dev} shows that the error deviation for $\sin(x + \delta x)$ is $1.000 \pm 0.010$, except approaching $0$ when $x=\pm \pi/2$ and $\delta x < 10^{-8}$.
Near a distributional pole, the input uncertainty is suppressed, resulting in zero error deviation.
As expected, $\delta^2 \sin(x)$ exhibits the same periodicity as $\sin(x)$.
The numerical errors of the library functions $\sin(x)$ and $\cos(x)$ over a larger range of $x$ will be examined in greater detail in Section \ref{sec: FFT}.

To test $f^{-1}(f(x)) - x = 0$ when $\delta x = 0$ for the library functions:
\begin{itemize}
\item Figure \ref{fig: ExpLog_Error} shows that the value errors in $e^{\log(x)} - x$ are much less than those in $\log(e^x) - x$. 
For $\log(e^x) - x$, the error deviation is $0.41$ when $|x| \leq 1$, or $0$ otherwise.

\item Figure \ref{fig: Power_Error} shows that the error deviation for $(x^p)^{1/p} - x$ is $0.56$,  independent on neither $x$ nor $p$.
\end{itemize}
The reasons why $(x^p)^{1/p} - x$ has larger value errors than $\log(e^x) - x$, and $e^{\log(x)} - x$ has almost no value error, are not clear.


\ifdefined\Verbose

\section{Moving-Window Linear Regression}
\label{sec: Moving-Window Linear Regression}

\subsection{Moving-Window Linear Regression Algorithm}

\iffalse
\begin{align*}
\alpha_{j} &= \sum_{X=-H}^{H - 1} Y_{j-H+X}; \\
\beta_{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X} \\
 	&= \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} + H Y_{j - 2 H - 1} + H Y_{j} - \sum_{X=-H}^{H - 1} Y_{j-H+X} \\
 	&= \beta_{j - 1} + H Y_{j - 2 H - 1} + H Y_{j} - \alpha_{j}; \\
\delta^2 \alpha_{j} &= \sum_{X=-H}^{H - 1} (\delta Y_{j-H+X})^2; \\
\delta^2 \hat{\beta}_{j} &\equiv \sum_{X=-H}^{H} X (\delta Y_{j-H+X})^2 = \sum_{X=-H-1}^{H-1} (X + 1) Y_{j-H+X} 
		+ H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
	&= \delta^2 \hat{\beta}_{j -1} + H (\delta Y_{j - 2 H - 1})^2 + H (\delta Y_{j})^2 - \delta^2 \alpha_{j}; \\
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2 \\
	&= \sum_{X=-H - 1}^{H - 1} (X + 1)^2 (\delta Y_{j-H+X})^2 - H^2 (\delta Y_{j-2H-1})^2 + H^2 (\delta Y_{j})^2 
		 - \sum_{X=-H}^{H - 1} (2 X + 1) (\delta Y_{j-H+X})^2 \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2
		 - 2 \sum_{X=-H}^{H} 2 X (\delta Y_{j-H+X})^2 - \delta^2 \alpha_{j} \\
	&= \delta^2 \beta_{j - 1} - H^2 (\delta Y_{j-2H-1})^2 + (H^2 + 2H) (\delta Y_{j})^2 - 2 \delta^2 \hat{\beta}_{j}  - \delta^2 \alpha_{j};
\end{align*}
\fi

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Value.pdf} 
\captionof{figure}{ 
Result of fitting $\alpha + \beta\; Y$ to a time-series input $Y$ within a moving window of size $2*2 + 1$.
The x-axis indicates the time index.
The y-axis on the left corresponds to the value of $Y$, $\alpha$, and $\beta$, while the y-axis on the right corresponds to the uncertainty of $\alpha$ and $\beta$.
The uncertainty for $Y$ is fixed at $0.2$.
In the legend, \textit{Unadjusted} refers to results obtained by directly applying Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} using variance arithmetic, whereas \textit{Adjusted} refers to using Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} for $\alpha$ and $\beta$ values but Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} for their variances.
}
\label{fig: Moving_Linear_Fit_Value}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Moving_Linear_Fit_Error.pdf} 
\captionof{figure}{ 
Error deviations of the $\alpha + \beta\; Y$ fit vs time index. 
The x-axis represents the time index.
The y-axis on the left corresponds to the error deviation.
For reference, the input time-series signal $Y$ is also plotted, with its values indicated on the y-axis on the right.
}
\label{fig: Moving_Linear_Fit_Error}
\end{figure}



Formula \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} provide the least-square line-fit of $Y = \alpha + \beta X$ between two set of data ${Y_j}$ and ${X_j}$, where $j$ is an integer index identifying $(X, Y)$ pairs in the sets \cite{Numerical_Recipes}.
\begin{align}
\label{eqn: linear regression 0}
\alpha &= \frac{\sum_{j} Y_{j} }{\sum_{j} 1}; \\
\label{eqn: linear regression 1}
\beta &= \frac{\sum_{j} X_{j} Y_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} Y_{j}}
    {\sum_{j} X_{j} X_{j} \; \sum_{j} 1 - \sum_{j} X_{j} \; \sum_{j} X_{j} };
\end{align}

In many applications, data set ${Y_j}$ denotes an input data stream where $j$ represents the time index or sequence index.
${Y_j}$ is thus referred to as a time-series input, with $j$ corresponding to $X_j$.  
A moving window algorithm \cite{Numerical_Recipes} is applied within a small window centered on each $j$.  
For each calculation window, ${X_j = -H, -H+1 \dots H-1, H}$ where $H$ is an integer constant specifying the half width of the window.
This choice ensures $\sum_{j} X_{j} = 0$, which simplifies Formula \eqref{eqn: linear regression 0} and \eqref{eqn: linear regression 1} into Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.

\begin{align}
\label{eqn: time-series linear regression 0}
\alpha _{j} &= \alpha \; 2 H = \sum_{X=-H+1}^{H} Y_{j-H+X}; \\
\label{eqn: time-series linear regression 1}
\beta _{j} &= \beta \; \frac{H (H+1)(2H+1)}{3} = \sum_{X=-H}^{H} X Y_{j-H+X}; 
\end{align}
The values of $(\alpha _{j}, \beta _{j})$ can be derived from the previous values $(\alpha _{j-1}, \beta _{j-1})$, allowing Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} to be reformulated into the progressive moving-window calculation given by Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1}, respectively \cite{Prev_Precision_Arithmetic}.
\begin{align}
\label{eqn: moving-window linear regression 0}
\beta _{j} &= \beta _{j-1} - \alpha _{j-1} + H \left(Y_{j-2H-1} + Y_{j} \right); \\
\label{eqn: moving-window linear regression 1}
\alpha_{j} &= \alpha _{j-1} - Y_{j-2H-1} + Y_{j};
\end{align}


\subsection{Variance Adjustment} 

\begin{align}
\label{eqn: moving-window linear regression variance 0}
\delta^2 \alpha_{j} &= \sum_{X=-H+1}^{H} (\delta Y_{j-H+X})^2 = \delta^2 \alpha_{j-1} - (\delta Y_{j-2H})^2 + (\delta Y_{j})^2; \\
\label{eqn: moving-window linear regression variance 1}
\delta^2 \beta_{j} &= \sum_{X=-H}^{H} X^2 (\delta Y_{j-H+X})^2;
\end{align}
When the time series contains uncertainty, directly applying Formula \eqref{eqn: moving-window linear regression 0} and \eqref{eqn: moving-window linear regression 1} results in a loss of precision since both formulas reuse each input multiple times, thereby accumulating the variance of that input with every reuse.
To prevent this, $\alpha_j$ and $\beta_j$ should still be calculated progressively using Formula \eqref{eqn: moving-window linear regression 1} and \eqref{eqn: moving-window linear regression 0}, respectively, while the variances should instead be computed using Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1}, respectively.
Formula \eqref{eqn: moving-window linear regression variance 1} is not progressive because the progressive form of $\delta^2 \beta_j$ is more  expensive in computation than Formula \eqref{eqn: moving-window linear regression variance 1}.

Figure \ref{fig: Moving_Linear_Fit_Value} shows that the input signal $Y_j$ consists of the following components:
\begin{enumerate}
\item An increasing slope for $j = 0 \dots 9$.

\item A decreasing slope for $j = 1 \dots 39$.

\item A sudden jump of magnitude $+10$ at $j=40$

\item A decreasing slope for $j = 41 \dots 49$.
\end{enumerate}
For each increment of $j$, the increasing and the decreasing rates are $+1$ and $-1$, respectively.

The specified input uncertainty is fixed at $0.2$.
Normal noise with a deviation of $0.2$  is added to the slopes, except for the segment $j = 10 \dots 19$ where Normal noise with a deviation of $2$ is introduced, representing actual uncertainty $10$ times larger than the specified uncertainty.

Figure \ref{fig: Moving_Linear_Fit_Value} also presents the results of the moving window fitting of $\alpha + \beta\; Y$ versus the time index $j$.
The fitted values of $\alpha$ and $\beta$ follow the expected behave, exhibiting a characteristic delay of $H$ in $j$.
When \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are applied to compute the uncertainties of $\alpha$ and $\beta$, both uncertainties increase exponentially with the time index $j$.
In contrast, when Formula \eqref{eqn: time-series linear regression 0} and \eqref{eqn: time-series linear regression 1} are used exclusively for value calculation, while Formula \eqref{eqn: moving-window linear regression variance 0} and \eqref{eqn: moving-window linear regression variance 1} are applied for variance computation, the resulting uncertainties of $\alpha$ and $\beta$ are $\frac{\delta Y}{\sqrt{2H+1}}$, and $\frac{\delta Y}{\sqrt{\frac{H (H+1)(2H+1)}{3}}}$.
Both are less than the input uncertainty $\delta Y$, due to the averaging effect of the moving window.


\subsection{Unspecified Input Error}

To determine the error deviations of $\alpha$ and $\beta$, the fitting procedure is applied to multiple time-series data sets, each generated with independent noise realizations.
Figure \ref{fig: Moving_Linear_Fit_Error} illustrates the resulting error deviation as a function of the time index $j$, which remains close to $1$ except within the range $j = 10 \dots 19$ where the actual noise is ten times greater than the specified value.
This observation suggests that an error deviation exceeding 1 may indicate the presence of unspecified additional input errors beyond rounding errors, such as numerical errors in mathematical library functions.


\section{Regressive Generation of Sin and Cos}
\label{sec: recursion}

\begin{figure}
\centering
\includegraphics[height=2.5in]{Regression_Sin.pdf}
\captionof{figure}{
The uncertainties (y-axis) of $\sin(\pi j/2^{18})$ versus $j$ (x-axis) and $L$ using either Quart or regressive sine function (legend).
}
\label{fig: Regression_Sin}
\end{figure}



Formula \eqref{eqn: phase sin} and Formula \eqref{eqn: phase cos} calculate $\sin(\pi j/2^L), \cos(\pi j/2^L), j = 0 \dots 2^{L - 2}$ repressively for regression order $L = 0 \dots 17$ starting from Formula \eqref{eqn: phase boundary}.
Formula \eqref{eqn: regr error} shows that such regression guarantees both $\sin(x)^2 + \cos(x)^2 = 1$ and $\sin(2x) = 2\sin(x) \cos(x)$, so that value errors will not accumulate when the regression order increases.
\begin{align}
\label{eqn: phase boundary}
& \sin(0) = \cos(\frac{\pi}{2}) = 0;\eqspace \sin(\frac{\pi}{2}) = \cos(0) = 1; \\
\label{eqn: phase sin}
& \sin(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 - \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 - \cos(\alpha) \cos(\beta) + \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: phase cos}
& \cos(\frac{\alpha + \beta}{2}) = \sqrt{\frac{1 + \cos(\alpha + \beta)}{2}} = \sqrt{\frac{1 + \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta)}{2}}; \\
\label{eqn: regr error}
& \sin(\alpha + \beta) = 2 \sin(\frac{\alpha + \beta}{2}) \cos(\frac{\alpha + \beta}{2}) = \sqrt{1 - \cos(\frac{\alpha + \beta}{2})^2};
\end{align}

Formula \eqref{eqn: phase sin} is not suitable for computing $\sin(x)$ as $x \rightarrow 0$ because it suffers from behavior analogous to catastrophic cancellation, resulting in excessive uncertainty.
In Figure \ref{fig: Regression_Sin}, the Quart sine function exhibits uncertainties proportional to $\sin(\frac{\pi}{2^L})$, whereas the regression sine function shows increasing uncertainties as $x \rightarrow 0$.
Unlike catastrophic cancellation in floating-point arithmetic, variance arithmetic uses very coarse precision to signal that the regression algorithm is unfit to compute $\sin(x \rightarrow 0)$.



\fi



\section{FFT (Fast Fourier Transformation)}
\label{sec: FFT}

\subsection{DFT (Discrete Fourier Transformation)}

\iffalse

\begin{align*}
h[k] &= \sin(f \frac{2 \pi}{N} k); \\
H[n] &= \sum_{k=0}^{N-1} \sin(f \frac{2 \pi}{N} k) \; e^{\frac{i 2\pi}{N} k n}
		= \sum_{k=0}^{N-1} \frac{1}{2i}\left( e^{(n + f) \frac{i 2\pi}{N} k} - e^{(n - f) \frac{i 2\pi}{N} k} \right)   \\
	&= \frac{1}{2i} \left( \frac{1 - e^{(n + f) \frac{i 2\pi}{N} N}}{1 - e^{(n + f) \frac{i 2\pi}{N}}}
		- \frac{1 - e^{(n - f) \frac{i 2\pi}{N} N}}{1 - e^{(n - f) \frac{i 2\pi}{N}}} \right) \\
&= \begin{cases}
  i N/2, & f \text{ is integer} \\
  N/\pi, & f \text{ is integer} + 1/2 \\
  \frac{1}{2} \frac{\sin(2\pi f - 2\pi \frac{f}{N}) + \sin(2\pi \frac{f}{N})-\sin(2\pi f) e^{-i 2\pi \frac{n}{N}}}{\cos(2\pi \frac{n}{N})-\cos(2\pi \frac{f}{N})} & \text{otherwise}
\end{cases}
\end{align*}

\fi

\begin{align}
\label{eqn: Fourier forward}
H[n] &=\sum_{k=0}^{N-1} h[k] \; e^{\frac{i 2\pi}{N} k n}; \\
\label{eqn: Fourier reverse}
h[k] &=\frac{1}{N} \sum_{n=0}^{N-1} H[n] \; e^{-\frac{i 2\pi}{N} n k};
\end{align}

For each signal sequence $h[k]$, where $k = 0, 1 \dots  N-1$, and $N$ is a natural number, the discrete Fourier transform (DFT) $H[n]$, for $n = 0, 1 \dots  N-1$, along with its inverse transformation, is defined by Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse}, respectively \cite{Numerical_Recipes}.
As a convention, $k$ denotes \emph{time index} for $h[k]$ as a waveform, while $n$ represents \emph{frequency index} for $H[n]$ as a spectrum.

\ifdefined\Verbose
\begin{figure}
\includegraphics[height=2.5in]{FFT_Unfaithful.pdf} 
\captionof{figure}{
The DFT spectrum $H[n]$ of signal $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, as intensity (y-axis) and phase (embedded y-axis) versus frequency index $n \in [0, 18]$ (x-axis and embedded x-axis) for different signal frequency $f$ (legend).
This result agrees with both theoretical formula \cite{Prev_Precision_Arithmetic} and numerical computation from any mathematical libraries such as \textit{SciPy}.
}
\label{fig: FFT_Unfaithful}
\end{figure}
For example, the FT spectrum of a sine function is a Delta function at the signal frequency $f$ with a phase $\pi/2$ \cite{Numerical_Recipes}.
Figure \ref{fig: FFT_Unfaithful} shows the DFT spectra of the sine function $h[k] = \sin(f \frac{2 \pi}{128} k), k \in [0, 127]$, where $f$ is its signal frequency. 
If DFT is regarded as the digital implementation of FT, the spectra exhibit no modeling error only when the input signal frequency $f$ is an integer, and display varying degrees of modeling errors otherwise.
Because of these modeling errors, the use of DFT as the digital implementation of FT is questionable, even though such usage is ubiquitous, and fundamental to many areas of applied mathematics 

To avoid the modeling errors inherent in DFT, only Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} are used in this paper.

\else

Although mathematically self-consistent, DFT implies a periodic boundary condition in the time domain \cite{Prev_Precision_Arithmetic}.
Consequently, it is only an approximation for the mathematically defined continuous Fourier transform (FT) \cite{Prev_Precision_Arithmetic}.
To avoid the modeling errors inherent in DFT, only Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} are used in this paper.

\fi



\subsection{FFT (Fast Fourier Transformation)}

\iffalse

Forward:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 + i 1 - 2 - i 3 , 0 - 1 + 2 - 3, 0 - i 1 - 2 + i 3; \\
 & [0, 1, 0, -1]:& [0, i 2, 0, - i 2]; \\
 & [1, 0, -1, 0]:& [0, 2, 0, 2];
\end{align*}

Reverse:
\begin{align*}
L = 1:\;& o=0: & [0, 1]; \\
 & o = 1: & F = [0 + 1, 0 - 1]; \\
L = 2:\;& o=0: & [0, 2, 1, 3]; \\
 & o = 1: & [0 + 2, 0 - 2, 1 + 3, 1 - 3]; \\
 & o = 2: & [0 + 1 + 2 + 3, 0 - i 1 - 2 + i 3 , 0 - 1 + 2 - 3, 0 + i 1 - 2 - i 3; \\
 & [0, i 2, 0, - i 2]:& [0, 4, 0, -4]; \\
 & [0, 2, 0, 2]:& [4, 0, -4, 0];
\end{align*}

\fi


When $N = 2^{L}$, where $L$ is a natural number named \emph{FFT order}, the generalized Danielson-Lanczos lemma can be applied to DFT to produce  FFT \cite{Numerical_Recipes}. 
\begin{itemize}

\item For each output, each input is used only once, therefore no dependency problem arises when decomposing FFT into arithmetic operations such as Formula \eqref{eqn: addition mean}, \eqref{eqn: addition variance}, \eqref{eqn: multiplication mean}, and \eqref{eqn: multiplication variance}.

\item When $L$ is large, the substantial volume of input and output data enables high-quality statistical analysis.

\item The computational complexity is proportional to $L$, since increasing $L$ by 1 adds an additional step involving a sum of two multiplications.

\item Each step in the forward transformation doubles the variance, so the uncertainty deviation increases with the FFT order $L$ as $\sqrt{2}^L$.
Because the reverse transformation divides the result by $2^L$, its uncertainty deviation decreases with $L$ as $\sqrt{1/2}^L$.
Consequently, the uncertainty deviation for the roundtrip transformation is therefore $\sqrt{2}^L \times \sqrt{1/2}^L = 1$.

\item The forward and reverse transformations are identical except for a sign difference, meaning that they are essentially the same algorithm, and any observed difference arises solely from the input data.  

\end{itemize}

In normal usage, forward and reverse FFT transforms differ in their data prospective of time domain versus frequency domain:
\begin{itemize}
\item The forward transformation converts a time-domain sine or cosine signal into a frequency-domain spectrum in which most values are zero, causing its uncertainties to grow more rapidly by cancellation. 

\item In contrast, the reverse transformation spreads the precise frequency-domain spectrum (where most values are zero) back into a time-domain sine or cosine signal, causing its uncertainties to grow more slowly by spreading. 
\end{itemize}
The question is whether variance arithmetic can work properly in these two contrary cases.
 

\subsection{Testing Signals}

\iffalse

The Fourier transformation of a linear signal $h[n] = n$. 
Let $y \equiv i 2\pi n /N$:
\begin{align*}
& G(y) = \sum_{k=0}^{N-1}  e^{y k} = \sum_{k=0}^{N-1}  (e^y)^k = \frac{e^{N y} - 1}{e^y - 1}
 = \begin{cases} y = 0: \eqspace N \\ y \neq 0: \eqspace 0 \end{cases}; \\
H[n] &= \sum_{k=0}^{N-1} k e^{\frac{i 2\pi n}{N} k} = \sum_{k=0}^{N-1} k e^{y k} 
 = \frac{d G}{y} = \frac{N e^{N y}}{e^y - 1} - \frac{e^{N y} - 1}{(e^y - 1)^2} e^y = \frac{N}{e^y - 1} \\
 &= \frac{N}{\cos(y) - 1 + i \sin(y)} = \frac{N}{2} \frac{\cos(y) - 1 -  i \sin(y)}{1 - \cos(y)} 
  = - \frac{N}{2}(1 + i \frac{2 \sin(\frac{y}{2}) \cos(\frac{y}{2})}{2 \sin^2(\frac{y}{2})}) \\
 &= \begin{cases} y = 0: \eqspace \frac{N^2}{2} \\ y \neq 0: \eqspace - \frac{N}{2}(1 + i \frac{1}{\tan(\frac{n}{N} \pi)}) \end{cases};
\end{align*}

\fi


The following signals are used for testing:
\begin{itemize}
\item \emph{Sin}: $h[k] = \sin(2\pi k f/N), f = 1, 2, ... N/2 -1$.

\item \emph{Cos}: $h[k] = \cos(2\pi k f/N), f = 1, 2, ... N/2 -1$.

\item \emph{Linear}: $h[k] = k$, whose DFT is given by Formula \eqref{eqn: Fourier spec for linear}.
\begin{align}
& y \equiv i 2\pi \frac{n}{N}: \eqspace G(y) = \sum_{k=0}^{N-1}  e^{y k} = \frac{e^{N y} - 1}{e^y - 1}; \nonumber \\
\label{eqn: Fourier spec for linear}
H[n] &= \frac{d G}{d y} = \begin{cases} n = 0: \eqspace \frac{N (N-1)}{2} \\ n \neq 0: \eqspace
 - \frac{N}{2}(1 + i \frac{\cos(n \frac{\pi}{N})}{\sin(n \frac{\pi}{N})}) \end{cases};
\end{align}

\end{itemize}



\subsection{Trigonometric Library Errors}

\begin{figure}[p]
\includegraphics[height=2.5in]{Sin_Diff.pdf} 
\captionof{figure}{
Difference between library and Quart $\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
The uncertainties of the Quart $\sin(x)$ is $\sin(x)$ ULP, which shows a periodicity of $\pi$.
}
\label{fig: Sin_Diff}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{Cot_Diff.pdf} 
\captionof{figure}{
Difference between the library and the Quart $\cos(x)/\sin(x)$ (y-axis) for $x = 2\pi j /2^L, j =0, 1 \dots 2^{L + 2}$ (x-axis), and $L = 5,6$ (legend).
}
\label{fig: Cot_Diff}
\end{figure}


Formula \eqref{eqn: Fourier forward} and \eqref{eqn: Fourier reverse} restrict the use of $\sin(x)$ and $\cos(x)$ to $x = 2\pi j /2^L$, where $L$ is the FFT order.
To minimize numerical errors in computing $\sin(x)$, the following \emph{indexed sine} can be used in place of  standard library sine functions:
\begin{enumerate}
\item Instead of a floating-point value $x$ as input for $\sin(x)$, an integer index $j$ defines the input as $\sin(\pi j/2^L)$, thereby eliminating the floating-point rounding error of $x$.

\item The values of $\sin(\pi j/2^L), j \in [0, 2^{L-2}]$ are library sine directly, while the values of $\sin(\pi j/2^L), j \in [2^{L-2}, 2^{L-1}]$ are computed from library $\cos(\pi (2^{L - 1} - j)/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended from $j \in [0, 2^{L-1}]$ to $j \in [0, 2^{L + 1}]$ by exploiting the symmetry of $\sin(\pi j/2^L)$.

\item The values of $\sin(\pi j/2^L)$ are extended to all the integer value of $j$ by leveraging the periodicity of $\sin(2\pi j/2^L)$.

\end{enumerate}
The constructed indexed $\sin(x)$ is referred to as the \emph{Quart} sine function.
In contrast, the direct use of the standard library $\sin(x)$ is referred to as the \emph{Library} sine function.

Because the Quart sine function strictly preserves the symmetry and periodicity of sine function, it provides superior numerical accuracy compared to the Library sine function.
\begin{itemize}
\item Figure \ref{fig: Sin_Diff} shows that the absolute value difference between the Library $\sin(x)$ and the Quart $\sin(x)$ increases approximately linearly with $|x|$.

\item Figure \ref{fig: Cot_Diff} shows the value difference between the Quart and Library $\cos(x)/\sin(x)$ also increases roughly linearly with $|x|$ but are $10^2$ times larger than those observed for $\sin(x)$.
Therefore, the linear spectrum in Formula \eqref{eqn: Fourier spec for linear} contains significantly larger numerical errors when computed using library sine functions.
\end{itemize}


\subsection{Using Quart Sine for Sin/Cos Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Histograms of normalized errors of Sin/Cos signals for forward, reverse and roundtrip transformations (legend) using the Quart sine function.
The FFT order is $18$.  
}
\label{fig: FFT_SinCos_Clean_Histo_Indexed}
\end{figure}


With FFT order as the specific dimension, the error deviations for forward and reverse transformations resemble Figure \ref{fig: Adjugate_Error_vs_Size_Noise}, while those for round-trip transformation is almost identical to Figure \ref{fig: Forward_Error_vs_Size_Noise}, independent of Sin or Cos signals, or frequency of the signals.
Therefore, the results for Sin and Cos signals across all frequencies are pooled together for statistical analysis, under the unified category \emph{Sin/Cos} signals.
When the FFT order $L$ is less than 8, the error deviations oscillate around $1$ due to insufficient sample count of $2^L$.
Even data for forward and reverse transformations are drastically different, variance arithmetic works effectively in all cases.

When $L=18$ and $\delta x = 0$, Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed} shows that the error distributions of Sin/Cos signal resemble Normal distribution, with an additional Delta-like distribution at $\tilde{z} = 0$.
The error distribution of the reverse transformation is structured on top of the Normal distribution, suggesting that the reverse transformation is more sensitive to numerical errors in sine function.


\subsection{Using Library Sine for Sin/Cos Signals}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_SinCos_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Histograms of normalized errors of Sin/Cos signal for forward, reverse and roundtrip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_SinCos_Clean_Histo_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Spec_Lib.pdf} 
\captionof{figure}{
FFT value error spectrum of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the forward transformation.
The legend distinguishes between uncertainty and value error.
The x-axis represents the frequency index, while the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Spec_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Wave_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the reverse transformation.
The legend distinguishes between uncertainty and value error.
The x-axis represents the time index, while the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Wave_Lib}
\end{figure}

\begin{figure}[p]
\includegraphics[height=2.5in]{FFT_Sin_Clean_6_3_Roundtrip_Lib.pdf} 
\captionof{figure}{
FFT value error waveform of $\sin(3 \frac{2 \pi}{2^6} j)$ computed using either the Library sine function or \textit{SciPy} after the roundtrip transformation.
The legend distinguishes between the uncertainty and the value error.
The x-axis represents the frequency index, while the y-axis represents both uncertainty and value error.
}
\label{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{Lib_Reverse_Error_vs_Freq_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) of FFT reverse transformation of $sin(f \frac{2 \pi}{2^L} j)$ versus frequency $f$ (x-axis) and FFT Order $L$ (y-axis).
}
\label{fig: Lib_Reverse_Error_vs_Freq_Order}
\end{figure}


With FFT order as the specific dimension, the error deviations for the forward and the reverse transformations resemble Figure \ref{fig: Adjugate_Error_vs_Size_Noise}, while those for the round-trip transformation is almost identical to Figure \ref{fig: Forward_Error_vs_Size_Noise}.
In addition, the error deviations are larger than $1$ when $\delta x < 10^{-15}$ for the forward transformation and $\delta x < 10^{-14}$ for the reverse transformation.

When $\delta x = 0$, the error deviations for the reverse transformation increase with FFT order, to $6.2$ at FFT order $18$.
As shown in Figure \ref{fig: Sin_Diff}, the Library sine function contains more numerical errors, so the error distribution for the reverse transformation using Library sine function in Figure \ref{fig: FFT_SinCos_Clean_Histo_Lib} is more structured and broader than that in Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed} using Quart sine function, while the error distribution for the forward transformations is more similar.
This difference is consistent with a larger error deviation $6.2 > 1$ for the reverse transformation, but a comparable error deviation $1.1 > 1$ for the forward transformation when $\delta x = 0$.

Using the Library sine function, for a sine wave with a frequency of $3$, Figure \ref{fig: FFT_Sin_Clean_6_3_Spec_Lib}, \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}, and \ref{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib} presents the value errors for the forward, the reverse, and the round-trip transformations, respectively. 
In the reverse transformation, value errors exhibit a clear trend of increasing with the time index. 
These large value errors appear systematic rather than random and visually resemble a resonant pattern.
Similar increases are observed at other frequencies and FFT orders, as well as in computational results obtained using mathematical libraries such as \textit{SciPy}.
In contrast, such resonance is absent for the roundtrip transformation as shown in Figure \ref{fig: FFT_Sin_Clean_6_3_Roundtrip_Lib}, as well as when using the Quart sine function.
Figure \ref{fig: Lib_Reverse_Error_vs_Freq_Order} demonstrates that the error deviations increase with sine or cosine frequency.
Figure \ref{fig: Sin_Diff} indicates that the numerical errors using the Library sine function increase with a periodicity of $\pi$, which may resonate with a signal whose periodicity is an integer multiply of $\pi$, producing the resonant pattern in Figure \ref{fig: FFT_Sin_Clean_6_3_Wave_Lib}.
At higher frequency, the resonant beats between the signal and the numerical errors in the Library sine function become stronger.
To suppress these numerical error resonances, an input noise of $\delta x = 10^{-14}$ must be added to the sine or cosine signals.
Such \emph{resonance of numerical errors} can easily and mistakenly be taken as signals.



\subsection{Using Quart Sine for Linear Signal}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Quart sine function.
}
\label{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Indexed.pdf} 
\captionof{figure}{
Histograms of normalized errors of Linear signals for forward, reverse and roundtrip transformations (legend) computed using the Quart sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Indexed}
\end{figure}

Figure \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order} show the error deviation for the forward and the reverse transformations, respectively. 
The forward transformation exhibits a larger ideal coverage area than the reverse transformations: $\delta x > 10^{-12}$ for the forward transformation, and $\delta x > 10^{-8}$ for the reverse transformation.
In other areas, both transformations achieve proper coverage with error values around $1$.

When $L = 18$ and $\delta x = 0$, the error distribution of the reverse transformation in Figure \ref{fig: FFT_Linear_Clean_Histo_Indexed} is narrower than that in Figure \ref{fig: FFT_SinCos_Clean_Histo_Lib}.
The corresponding error deviations are $1.5 < 6.2$, respectively.




\subsection{Using Library Sine for Linear Signal}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Clean_Histo_Lib.pdf} 
\captionof{figure}{
Histograms of normalized errors of Linear signals for forward, reverse and roundtrip transformations (legend) computed using the Library sine function.
The FFT order is $18$.
}
\label{fig: FFT_Linear_Clean_Histo_Lib}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the forward transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the reverse transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_Lib_Roundtrip_ErrorDev_vs_Noise_Order.pdf} 
\captionof{figure}{
Error deviation (z-axis) versus input uncertainty (x-axis) and FFT order (y-axis) for the roundtrip transformations of Linear signals computed using the Library sine function.
}
\label{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order}
\end{figure}


Figure \ref{fig: FFT_Linear_Clean_Histo_Lib} shows that the reverse error distribution when $\delta x = 0$ is no longer bound.
The difference between Figure \ref{fig: FFT_Linear_Clean_Histo_Lib} and Figure \ref{fig: FFT_Linear_Clean_Histo_Indexed} is consistent with the large numerical errors as demonstrated in Figure \ref{fig: Cot_Diff}.
Variance arithmetic fails because of the large amount of unspecified numerical errors from the Library sine.

Figure \ref{fig: FFT_Linear_Lib_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Lib_Reverse_ErrorDev_vs_Noise_Order} shows a much smaller ideal coverage areas than those in Figure \ref{fig: FFT_Linear_Indexed_Forward_ErrorDev_vs_Noise_Order} and \ref{fig: FFT_Linear_Indexed_Reverse_ErrorDev_vs_Noise_Order}.
Because result deviations grow more slowly in the reverse transformation than in the forward transformation, the reverse transformation exhibits a smaller ideal coverage region.
Outside the ideal coverage region, proper coverage cannot be achieved for the reverse transformation.
Furthermore, the range of input noise that produces ideal coverage decreases with increasing FFT order.
At sufficiently high FFT orders, visually beyond FFT order $25$ for the reverse transformation, ideal coverage may no longer be achievable.
Although FFT is widely regarded as one of the most robust numerical algorithms \cite{Numerical_Recipes}\cite{Precise_Numerical_Methods}, and generally insensitive to input errors, it can still fail due to numerical errors in the Library sine function.
Such deterioration in calculation accuracy is not easily detectable when using conventional floating-point arithmetic.

Figure \ref{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order} shows that, even when variance arithmetic can no longer effectively track the value errors for the forward and the reverse transformations, it can still effectively track the value errors for the round-trip transformation, as the plateau region at high $L$ and low $\delta x$ in Figure \ref{fig: FFT_Linear_Lib_Roudtrip_ErrorDev_vs_Noise_Order}.
Such error cancellation is due to dependence tracing in statistical Taylor expansion.



\subsection{Ideal Coverage}

\begin{figure}[p]
\centering
\includegraphics[height=2.5in]{FFT_Linear_1e-3_vs_Order_Lib.pdf} 
\captionof{figure}{
Result error deviation (left y-axis) and uncertainty deviation (right y-axis) of Linear signals versus FFT order (x-axis) and transformation types (legend) computed using the Library sine function.
}
\label{fig: FFT_Linear_1e-3_vs_Order_Lib}
\end{figure}

Adding noise to the input can suppress unspecified input errors.
After adding a Gaussian input noise of $\delta x = 10^{-3}$ to a Linear signal when using the Library sine function, the error distributions for both the forward and the reverse transformations become Normal, while the error distribution for the round-trip transformation becomes Delta.
Figure \ref{fig:  FFT_Linear_1e-3_vs_Order_Lib} illustrates the corresponding error deviations and uncertainty deviations versus FFT order:
\begin{itemize}
\item As expected, the resulting uncertainty deviations for the forward transformations increase with FFT order $L$ as $\sqrt{2}^L$.

\item As expected, the resulting uncertainty deviations for the reverse transformations decrease with FFT order $L$ as $1/\sqrt{2}^L$.

\item As expected, the resulting uncertainty deviations for the round-trip transformations remains equal to the corresponding input uncertainties of $10^{-3}$.

\item As expected, the resulting error deviations for the forward and the reverse transformations remain constant at $1$.

\item As expected, the resulting error deviations for the round-trip transformations are far less than $1$ but increase with FFT order $L$ exponentially due to increasing calculation.
\end{itemize}

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline 
Signal     & Sine function & Forward       & Reverse       & Roundtrip    \\ 
\hline 
Sin/Cos   & Quart           & $10^{-16}$  & $10^{-12}$ & $10^{-14}$ \\ 
\hline 
Sin/Cos   & Library         & $10^{-16}$  & $10^{-11}$ & $10^{-12}$ \\ 
\hline 
Linear     & Quart           & $10^{-11}$  & $10^{-7}$   & $10^{-8}$ \\
\hline 
Linear     & Library         & $10^{-11}$  & $10^{-3}$   & $10^{-8}$  \\
\hline 
\end{tabular}
\captionof{table}{
The measured minimal required noise to achieve ideal coverage for FFT transformations at FFT order $18$ for different signals and sine functions.
}
\label{tbl: ideal coverage}
\end{table}

Table \ref{tbl: ideal coverage} shows the minimal required noise to achieve ideal coverage for FFT transformations at FFT order $L=18$ for different signals and sine functions, which is consistent with the corresponding error distributions in Figure \ref{fig: FFT_SinCos_Clean_Histo_Indexed}, \ref{fig: FFT_SinCos_Clean_Histo_Lib}, \ref{fig: FFT_Linear_Clean_Histo_Indexed}, and \ref{fig: FFT_Linear_Clean_Histo_Lib}.
This shows that an error distribution can qualify its input uncertainty quantification.
Without knowing precise result, similar histogram can be constructed from the calculated mean $\overline{f}$ and deviation $\delta f$, and the result data set for $f$.
It is worth investigating if such empirical histogram has similar power to reveal tracking of value errors by variance arithmetic.





\section{Conclusion and Discussion}
\label{sec: conclusion and discussion}

\subsection{Summary}

When uncorrelated uncertainty condition is met, statistical Taylor expansion outputs the mean, deviation, and reliability of an analytic expression.
It tracks the variable dependencies in intermediate steps, rejects invalid calculations, and explicitly incorporates the sample counts and the input uncertainty distributions into the statistical analysis.
While statistical Taylor expansion eliminates the dependency problem, it also removes most execution flexibility.

The presence of ideal coverage is a necessary condition for a numerical algorithm using statistical Taylor expansion to be considered correct. 
The ideal coverage defines the optimal applicable range for an algorithm. 

Variance arithmetic simplifies statistical Taylor expansion by introducing numerical rules to elimination invalid results, such as divergent, negative variance, unstable, infinite, or unreliable results.
It provides proper coverage for floating-point rounding errors.
The applicability of variance arithmetic has been demonstrated across a wide range of computational scenarios.

Library mathematical functions should be recalculated using variance arithmetic, so that each output value is accompanied by its corresponding uncertainty.
Without this refinement, the value errors in the library functions can produce unpredictable and potentially significant effects on numerical results.

The code and analysis framework for variance arithmetic are available as an open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}.
A more detailed description of this paper is presented at  \url{https://arxiv.org/abs/2410.01223}.



\subsection{Improvements Needed}

This paper presents statistical Taylor expansion and variance arithmetic, which is still in early stage of development, with several important questions remaining.

Bound momentum $\zeta(2n, \kappa)$ needs to be extended to all probability distributions.
Statistical bounding needs to be recalculated using $1 - \zeta(2, \kappa)$ instead of $1 - \zeta(0, \kappa)$
The statistical bounding to find the bounding range $\kappa$ for a sample count needs to be extended to discrete distributions.
The choice of ideal bounding range $\hat{\kappa}$ needs to extend to other distributions using the same value of $1 - \zeta(2, \kappa)$.
The statistical meaning for ideal ratio $\alpha$ in the context of its choice of ideal bounding range $\hat{\kappa}$ needs further investigation: Is $\alpha$ the possibility for the result to be bound by $\hat{\kappa}$?

The performance of variance arithmetic must be improved for broader practical adoption.
The fundamental formulas of statistical Taylor expansion, Formula \eqref{eqn: Taylor 1d mean}, \eqref{eqn: Taylor 1d variance}, \eqref{eqn: Taylor 2d mean}, and \eqref{eqn: Taylor 2d variance}, contain a large number of independent summations, making them excellent candidates for parallel processing.
Moreover, the inherently procedural nature of these formulas allows statistical Taylor expansion to be implemented efficiently at the hardware level.

A key open question is whether variance arithmetic can be adapted to achieve ideal coverage for floating-point rounding errors, because many theoretical calculations lack explicit input uncertainties.
Variance arithmetic does not adjust uncertainty variance when floating-point rounding error occurs during calculation.
This leads to error deviations larger than 1.
Detecting floating-point rounding error also needs hardware implementation for efficiency.

In variance arithmetic, deviations are comparable with values, but variances are used in calculation.
This effectively limits the range of deviations to the square root of that of values.
If the sign bit of the floating type can be re-purposed as an exponent bit in a new unsigned floating-point representation, the range of the deviation will be identical to that of values.

When an analytic expression undergoes statistical Taylor expansion, the resulting expression can become highly complex, as in the case of matrix inversion.
However, modern symbolic computation tools such as \textit{SymPy} and \text{Mathematica} can greatly facilitate these calculations.
This observation suggests that it may be time to shift from purely numerical programming toward analytic programming, particularly for problems that possess  inherently analytic formulations.

As an enhancement to dependency tracing, source tracing identifies each inputs contribution to the overall result uncertainty.
This capability enables engineers to pinpoint the primary sources of measurement inaccuracy and in turn guide targeted improvements in data acquisition and processing strategies.
For example, Formula \eqref{eqn: sum leakage} can guide how to improve the ideal ratio of $x \pm y$.

Because traditional numerical approaches are based on floating-point arithmetic, they must be reexamined or even reinvented within the framework of variance arithmetic.
For instance, most conventional numerical algorithms aim to identify optimal computational paths, whereas statistical Taylor expansion conceptually rejects all path-dependent calculations.
Reconciling these two paradigms may present a significant and ongoing challenge.

Establishing theoretical foundation for applying statistical Taylor expansion in the absence of a closed-form analytic solution, or when only limited low-order numerical derivatives are available, as in solving differential equations, remains an important  direction for future research.

\subsection{Possible Connections to Quantum Physics}

For input distribution without bounding, Formula \eqref{eqn: Taylor 1d variance} may not converge, for example, when the input uncertainty is Laplace \cite{Probability_Statistics} so that $\zeta(2n) = (2n)!$.
Formula \eqref{eqn: Taylor 1d variance} converges in all cases only when the input uncertainty is bounded due to sampling, which resembles the need for re-normalization in quantum field theory \cite{Quantum Field}.
The quantitative dependence of convergence on the choice of ideal bounding range $\hat{\kappa}$ is similar to the principle of quantum physics: what it is depends on how it is measured.
For example, because mathematical divergence seems to correspond to distributional zero, it is even tempting to postulate if a localized experiment changes $\hat{\kappa}$ to produce distributional zero and the quantum result as ``wave function collapse" \cite{Quantum Field}. 
Both resemblances are worth further investigation.
Could Feynman integration and summation \cite{Quantum Field} be a statistical Taylor expansion of an analytic expression?
Could Lamb shift \cite{Quantum Field} be an uncertainty offset?
These naive questions are entertaining to ponder.


\section{Statements and Declarations}

\subsection{Acknowledgments}

As an independent researcher without institutional affiliation, the author expresses sincere gratitude to Dr. Zhong Zhong (Brookhaven National Laboratory) and Prof Weigang Qiu (Hunter College) for their encouragement and valuable discussions.
Special thanks are extended to the organizers of \emph{AMCS 2005}, particularly Prof. Hamid R. Arabnia (University of Georgia), and to the organizers of the \emph{NKS Mathematica Forum 2007}. 
The author also gratefully acknowledges Prof Dongfeng Wu (Louisville University) for her insightful guidance on statistical topics.
Finally, heartfelt appreciation is extended to the editors and reviewers of \emph{Reliable Computing} for their substantial assistance in shaping and accepting an earlier version of this work, with special recognition to Managing Editor Prof. Rolph Baker Kearfott.


\subsection{Data  Availability Statement}

The data set used in this study are all generated in the open-source project at \url{https://github.com/Chengpu0707/VarianceArithmetic}. 
The execution assistance and explanation of the above code are available from the author upon request.


\subsection{Competing Interests}

The author has no competing interests to declare that are relevant to the content of this article.

\subsection{Founding}

No funding was received from any organization or agency in support of this research.


\ifdefined\ManualReference

\begin{thebibliography}{10}

\bibitem{Statistical_Methods}
Sylvain Ehrenfeld and Sebastian~B. Littauer.
\newblock {\em Introduction to Statistical Methods}.
\newblock McGraw-Hill, 1965.

\bibitem{Precisions_Physical_Measurements}
John~R. Taylor.
\newblock {\em Introduction to Error Analysis: The Study of Output Precisions
  in Physical Measurements}.
\newblock University Science Books, 1997.

\bibitem{Lower_Order_Variance_Expansion}
Fredrik Gustafsson and Gustaf Hendeby.
\newblock Some relations between extended and unscented kalman filters.
\newblock {\em IEEE Transactions on Signal Processing}, 60-2:545--555, 2012.

\bibitem{Probability_Statistics}
Michael~J. Evans and Jeffrey~S. Rosenthal.
\newblock {\em Probability and Statistics: The Science of Uncertainty}.
\newblock W. H. Freeman, 2003.

\bibitem{Numerical_Recipes}
William~H. Press, Saul~A Teukolsky, William~T. Vetterling, and Brian~P.
  Flannery.
\newblock {\em Numerical Recipes in C}.
\newblock Cambridge University Press, 1992.

\bibitem{Computer_Architecture}
John~P Hayes.
\newblock {\em Computer Architecture}.
\newblock McGraw-Hill, 1988.

\bibitem{Floating_Point_Arithmetic}
David Goldberg.
\newblock What every computer scientist should know about floating-point
  arithmetic.
\newblock {\em ACM Computing Surveys}, March 1991.

\bibitem{Floating_Point_Standard}
Institute of Electrical and Electronics Engineers.
\newblock {\em ANSI/IEEE 754-2008 Standard for Binary Floating-Point
  Arithmetic}, 2008.

\bibitem{Rounding_Error}
J.~H. Wilkinson.
\newblock {\em Rounding Errors in Algebraic Processes}.
\newblock SIAM, 1961.

\bibitem{Precise_Numerical_Methods}
Oliver Aberth.
\newblock {\em Precise Numerical Methods Using C++}.
\newblock Academic Press, 1998.

\bibitem{Algorithms_Accuracy}
Nicholas~J. Higham.
\newblock {\em Accuracy and Stability of Numerical Algorithms}.
\newblock SIAM, 2002.

\bibitem{Interval_Analysis}
R.E. Moore.
\newblock {\em Interval Analysis}.
\newblock Prentice Hall, 1966.

\bibitem{Worst_Case_Error_Bounds}
W.~Kramer.
\newblock A prior worst case error bounds for floating-point computations.
\newblock {\em IEEE Trans. Computers}, 47:750--756, 1998.

\bibitem{Interval_Analysis_Theory_Applications}
G.~Alefeld and G.~Mayer.
\newblock Interval analysis: Theory and applications.
\newblock {\em Journal of Computational and Applied Mathematics}, 121:421--464,
  2000.

\bibitem{Interval_Arithmetic}
W.~Kramer.
\newblock Generalized intervals and the dependency problem.
\newblock {\em Proceedings in Applied Mathematics and Mechanics}, 6:685--686,
  2006.

\bibitem{Interval_Analysis_Notations}
A.~Neumaier S.M. Rump S.P.~Shary B.~Kearfott, M. T.~Nakao and P.~Van
  Hentenryck.
\newblock Standardized notation in interval analysis.
\newblock {\em Computational Technologies}, 15:7--13, 2010.

\bibitem{Prev_Precision_Arithmetic}
C.~P. Wang.
\newblock A new uncertainty-bearing floating-point arithmetic.
\newblock {\em Reliable Computing}, 16:308--361, 2012.

\bibitem{Stochastic_Arithmetic}
J.~Vignes.
\newblock A stochastic arithmetic for reliable scientific computation.
\newblock {\em Mathematics and Computers in Simulation}, 35:233--261, 1993.

\bibitem{CADNA_library}
C.~Denis N.~S.~Scott, F.~Jezequel and J.~M. Chesneaux.
\newblock Numerical 'health' check for scientific codes: the cadna approach.
\newblock {\em Computer Physics Communications}, 176(8):501--527, 2007.

\bibitem{Linear_Algebra}
J.~Hefferon.
\newblock Linear algebra.
\newblock \url{http://joshua.smcvt.edu/linearalgebra/}, 2011.

\bibitem{Quantum Field}
Michel Le Bellac, G Barton.
\newblock Quantum and statistical Field Theory.
\newblock Oxford University Press, 1992, ISBN 9781383026535.


\end{thebibliography}

\else
\bibliographystyle{unsrt}
\bibliography{VarianceArithmetic}
\fi






\end{document}
